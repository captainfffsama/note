在大模型 PEFT 方法中，LoRA 比 P-tuning 更常用。两者的适用场景差异如下：

 1. LoRA（低秩适应）
- 优势：通过低秩矩阵更新参数，仅需调整 0.1%-1% 的参数即可接近全参数微调效果，且不增加推理延迟。
- 适用任务：
  - 复杂逻辑调整：如细粒度分类、医学影像分析等需要修改模型内部逻辑的任务
  - 多任务学习：不同任务可独立添加低秩矩阵，避免重复训练
  - 资源受限场景：边缘设备部署（如手机）或 GPU 显存不足时

 1. P-tuning（提示微调）
- 优势：仅优化输入提示的嵌入向量，无需修改模型参数，训练速度更快。
- 适用任务：
  - 文本生成类任务：如摘要、翻译、问答等自然语言处理场景
  - 小样本快速适配：通过修改输入提示快速适配新任务（如情感分类）
  - 避免知识遗忘：适合需要保留预训练通用知识的场景

 选择依据

- 数据量：P-tuning 在小样本下易过拟合，LoRA 对数据量要求更低
- 硬件资源：LoRA 显存占用更低（如千亿参数模型仅需增加 1% 参数）
- 任务复杂度：需调整模型内部逻辑时选 LoRA，仅需引导输出时选 P-tuning

总结：工业界更倾向 LoRA（如 HuggingFace 集成 PEFT 库），因其灵活性和高效性；P-tuning 则适合轻量级文本生成任务。