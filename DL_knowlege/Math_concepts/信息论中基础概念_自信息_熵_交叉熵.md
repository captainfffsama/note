#信息论

# 自信息

符合分布 P 的某一事件 x 出现，传达这条信息所需的最少信息长度为自信息，表达为:

$$
I(x)=log\frac{1}{P(x)}
$$

# 熵

从分布 P 中随机抽选一个事件，传达这条信息所需的最优平均信息长度为香农熵，表达为:

$$
H(P)=\sum_xP(x) log\frac{1}{P(x)}
$$

# 交叉熵

用分布 P 的最佳信息传递方式来传达分布 Q 中随机抽选的一个事件，所需的平均信息长度为交叉熵，表达为:

$$
H_P(Q)=\int_xQ(x)log\frac{1}{P(x)} {\rm d}x
$$

# KL 散度

用分布 P 的最佳信息传递方式来传达分布 Q，比用分布 Q 自己的最佳信息传递方式来传达分布 Q，平均多耗费的信息长度为 KL 散度，表达为 $D_p(Q)$ 或 $D_{KL}(Q||P)$ ，KL 散度衡量了两个分布之间的差异, 表达为:

$$
\begin{align}
D_p(Q)&=H_P(Q)-H(Q) \\
&=\sum_xQ(x)log\frac{1}{P(x)}-\sum_xQ(x)log\frac{1}{Q(x)} \\
&=\sum_xQ(x)log\frac{Q(x)}{P(x)}
\end{align}
$$

# 参考
- [如何理解KL散度的不对称性](https://www.jiqizhixin.com/articles/0224)