[原文](https://zhuanlan.zhihu.com/p/32230623)

[toc]

#  发展历程
SGD -> SGDM -> NAG ->AdaGrad -> AdaDelta -> Adam -> Nadam

# 一个框架
定义：

待优化参数： $w$
目标函数：$f(w)$
学习率： $\alpha$


计算流程：

在每个 epoch $t$ ：
1. 计算当前目标函数关于当前参数的梯度：$g_t=\nabla{f(w_t)}$
2. 根据历史梯度计算一阶动量：$m_t=\phi(g_1,g_2,...,g_t)$和二阶动量：$V_t=\psi(g_1,g_2,...,g_t)$
3. 计算当前时刻的下降梯度：$n_t=a\cdot{\frac{m_t}{\sqrt{V_t}}}$
4. 根据下降梯度进行更新：$w_{t+1}=w_t-n_t=w_t-a\cdot{\frac{m_t}{\sqrt{V_t}}}$
一般情况下各算法1，2两步不同，3，4两步基本一样

# 各算法分析
## SGD
### 计算方法
SGD 无动量的概念，因此 $m_t=g_t$ ; $V_t=I^2$
带入步骤3，4：
$$
n_t=a\cdot{g_t}
$$
$$
w_{t+1}=w_t-a\cdot{g_t}
$$
### SGD特点：
- 选择合适的learning rate比较困难 - 对所有的参数更新使用同样的learning rate。对于稀疏数据或者特征，有时我们可能想更新快一些对于不经常出现的特征，对于常出现的特征更新慢一些，这时候SGD就不太能满足要求了
- SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点

## SGD 带Momentum（SGDM）
### 计算方法
为了抑制SGD的震荡，SGDM认为梯度下降过程可以加入惯性。下坡的时候，如果发现是陡坡，那就利用惯性跑的快一些。SGDM全称是SGD with momentum，在SGD基础上引入了一阶动量：
$$
m_t=\beta_1\cdot{m_{t-1}}+(1-\beta_1)\cdot{g_t}
$$
<font color=red>**一阶动量是各个时刻梯度方向的指数移动平均值**，</font>约等于最近 $1/(1-\beta_1)$ 个时刻的梯度向量和的平均值。

也就是说，t 时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。 $\beta_1$ 的经验值为0.5、0.9和0.99，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。和学习率一样， $\beta_1$ 也会随着时间不断调整，一般初始值是一个较小的值，因为一开始没有积累什么学习经验，所以以当前找到的梯度方向为准；随后慢慢增大 $\beta_1$ 的值，下降方向主要是此前积累的下降方向，避免最后因为优化超平面的波动陷入持续震荡。动量法是令梯度直接指向最优解的策略之一.
**SGDM 仅使用了一阶动量，二阶动量仍然没变为 $V_t=I^2$ ，其改进主要在第2步的动量计算上**
### [SGDM的特点](https://zhuanlan.zhihu.com/p/22252270?utm_source=qq&utm_medium=social)
- 下降初期时，使用上一次参数更新，下降方向一致，乘上较大的， $\beta$ 能够进行很好的加速
- 下降中后期时，在局部最小值来回震荡的时候， ${{gradient}\to0  ,\beta}$ 使得更新幅度增大，跳出陷阱
- 在梯度改变方向的时候， $\beta$ 能够减少更新总而言之，momentum 项能够在相关方向加速 SGD，抑制振荡，从而加快收敛

## NAG(SGD with Nesterov Acceleration)
SGD 还有一个问题是困在局部最优的沟壑里面震荡。想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能待在这里了。可是如果你爬上高地，就会发现外面的世界还很广阔。因此，我们不能停留在当前位置去观察未来的方向，而要向前一步、多看一步、看远一些。



NAG全称Nesterov Accelerated Gradient，是在SGD、SGD-M的基础上的进一步改进，改进点在于步骤1。我们知道在时刻t的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算，那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步，那个时候再怎么走。因此，NAG在步骤1，不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向：
$$
g_t=\nabla{f(w_t-a\cdot\frac{m_{t-1}}{\sqrt{V_{t-1}}})}
$$
在NAG中没有用到二阶动量，因此此处：
$$
V_{t+1}=I
$$
也就是此处的梯度方向 $g_t$ 是使用下一时刻的梯度 $w_{t+1}$ 计算出来的，然后再根据 SGDM 中：
$$
m_t=\beta_1\cdot{m_{t-1}}+(1-\beta_1)\cdot{g_t}
$$
计算出当前时刻的累计动量。
**NAG 的改进点主要再第一步 $g_t$ 的计算上。**
## AdaGrad
之前都没有用过二阶动量，SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到（想想大规模的embedding）。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。怎么样去度量历史更新频率呢？那就是二阶动量——该维度上，迄今为止所有梯度值的平方和：
$$
V_t=\sum_{\tau=1}^{t}{g_{\tau}^{2}}
$$
从另一个角度理解，既对学习率进行了一个约束，将原始学习率 $a$ 变成了 $a/{\sqrt{V_t}}$ ，即此时：
$$
w_{t+1}=w_t-\frac{a}{\epsilon+\sqrt{\sum_{\tau=1}^{t}{g_{\tau}^{2}}}}\cdot{m_t}
$$
其中 $\epsilon$ 是一个很小的数用于防止分母为0，一般为 $10^{-7}$ ，由于 AdaGrad 中没有使用一阶动量，因此此处：
$$
m_t=g_t
$$
$$
w_{t+1}=w_t-\frac{a}{\epsilon+\sqrt{\sum_{\tau=1}^{t}{g_{\tau}^{2}}}}\cdot{g_t}
$$


**AdaGrad的改进点主要在第2步的二阶动量计算上**

### AdaGrad特点
- 前期 $g_t$ 较小的时候，约束较大，能够放大梯度
- 后期 $g_t$ 较大的时候，约束较小，能够约束梯度
- 适合处理稀疏梯度
### AdaGrad缺点
- 由公式可以看出，仍依赖于人工设置一个全局学习率
- $\alpha$ 设置过大的话，会使约束过于敏感，对梯度的调节太大
-中后期，分母上梯度平方的累加将会越来越大，使 $gradient\to0$ ，使得训练提前结束，即 $\sqrt{V_t}$ 是单调递增的，会使得学习率单调递减至0，可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识。
## AdaDelta&RMSprop
由于AdaGrad单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是AdaDelta名称中Delta的来历。

修改的思路很简单。前面我们讲到，指数移动平均值大约就是过去一段时间的平均值，因此我们用这一方法来计算二阶累积动量：
$$
V_t=\beta_1\cdot{V_{t-1}}+(1-\beta_1)\cdot{g_t}
$$
一般情况下建议是 $\beta_1=0.9$
标准的RMSprop就是AdaDelta的特例,即此时：
$$
\beta_1=0.5
$$
$$
V_t=\frac{1}{m}{\cdot{\sum_{t=0}^{m}g_t}}
$$
即 $V_t$ 就是 $g_t$ 的 RMS(均方根)
此时：
$$
w_{t+1}=w_t-\frac{a}{RMS\mid{g}\mid_t}{\cdot{g_t}}
$$
这里在使用中在分母中同样要添加 $\epsilon$ ，一般为 $10^{-6}$ 。
对于标准的AdaDelta：
$$
w_{t+1}=w_t-\frac{a}{\sqrt{V_t+\epsilon}}\cdot{g_t}
$$
为使Adadelta不依赖全局学习率（RMSprop还是依赖学习率的，并没有做这个处理），作者使用近似牛顿法的迭代，使：

$$
n_t=\frac{\sqrt{\frac{\sum_{i=0}^{t-1}{n_{t-1}^2}}{t-1}}}{\sqrt{\frac{\sum_{i=0}^{t-1}{g_{t-1}^2}}{t-1}}}\cdot{g_t}=\frac{RMS|n|_{t-1}}{RMS|g|_{t-1}}\cdot{g_t}=\frac{RMS|n|_{t-1}}{\sqrt{V_{t-1}+\epsilon}}\cdot{g_t}
$$
**AdaDelta的改进主要是在第二步的二阶动量计算上，在AdaGrad的基础上使用指数加权平均来计算二阶动量。RMSProp和AdaDelta其实是一个东西，不过是提出者不同。实现不用全局学习率的方法就是在学习率更新上使用指数加权平均。**
### AdaGrad&RMSprop的特点
- 训练初中期，加速效果不错，很快
- 训练后期，反复在局部最小值附近抖动

## Adam
Adam们是前述方法的集大成者。SGD-M在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加了二阶动量。把一阶动量和二阶动量都用起来，就是Adam了。
一阶动量：

$$
m_t=\beta_1\cdot{m_{t-1}}+(1-\beta_1)\cdot{g_t}
$$
二阶动量：
$$
V_t=\beta_2\cdot{V_{t-1}}+(1-\beta_2)\cdot{g_t^2}
$$
这里是一阶动量和二阶动量的计算实际上是对梯度的一阶矩估计和二阶矩估计，这种估计是有偏的。因此这里继续对偏差进行校正，来近似时间一阶矩和二阶矩的无偏估计：
$$
\hat{m_t}=\frac{m_t}{1-\beta_1}
$$
$$
\hat{V_t}=\frac{V_t}{1-\beta_2}
$$
计算方式就是：
$$
w_{t+1}=w_t-a\cdot\frac{\hat{m_t}}{\sqrt{\hat{V_t}}+\epsilon}
$$
这里推荐值是：
$$
\beta_1=0.9,\beta_2=0.999,\epsilon=10^{-8},a=0.0014
$$

在花书中， $a$不叫学习率，叫步长。
**Adam的改进点在第二步的一阶动量和二阶动量的计算上，都使用了指数加权平均,同时对偏差进行了修正**

### adam的特点
- 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点
- 对内存需求较小
- 为不同的参数计算不同的自适应学习率
- 也适用于大多非凸优化
- 适用于大数据集和高维空间

# Adam的一些变体
## NAdam
其实就是Adam结合NAG，在Adam基础上对第一步的计算进行了改进，使用Nesterov来计算。

## Adamax
由于Adam使用的固定时间窗口的累积的二阶动量，不是单调变化的，在训练后期容易引起震荡导致模型无法收敛。
而解决这个问题的一个修正方法Adamax，即对二阶动量的变化进行控制，避免上下波动。
在Adam中，二阶动量：
$$
V_t=\beta_2\cdot{V_{t-1}}+(1-\beta_2)\cdot{g_t^2}
$$
这里 $||V_t||$ 是有可能小于 $||V_{t-1}||$ 的。可能会引起震荡。
修正为Adamax，则：
$$
V_t=max(\beta_2\cdot{V_{t-1}}+(1-\beta_2)\cdot{g_t^2},V_{t-1})
$$
这里就保证了：
$$
||V_t||\geq||V_{t-1}||
$$
保证了二阶动量单调递增，实现了学习率的单调递减。

# 各个算法效果对比
![optima_1](../Attachments/optima_1.gif)]

![optima_2](../Attachments/optima_2.gif)]




# 参考资料
- https://zhuanlan.zhihu.com/p/32230623
- https://blog.csdn.net/wgb521cgl/article/details/80415990
- https://www.jiqizhixin.com/articles/2016-11-21-4
- https://zhuanlan.zhihu.com/p/22252270?utm_source=qq&utm_medium=social
- https://www.cnblogs.com/shixiangwan/p/7532864.html
- https://www.cnblogs.com/guoyaohua/p/8780548.html
- https://www.cnblogs.com/neopenx/p/4768388.html
- https://www.cnblogs.com/qniguoym/p/8058186.html
- https://blog.csdn.net/u012759136/article/details/52302426
- https://blog.csdn.net/Xyzx043874/article/details/79249907

# 各个算法伪代码（待补）


































