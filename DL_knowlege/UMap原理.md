#聚类 #降维 #可视化

以下内容来自豆包回答:

# UMAP 算法原理通俗解释 

UMAP（Uniform Manifold Approximation and Projection）是一种用于数据降维的算法，就像是把高维空间（很复杂的空间）里的“数据居民”搬到低维空间（简单的空间）里住，而且还要让这些“数据居民”在低维空间里的分布和在高维空间里差不多，尽量保持他们之间的邻里关系和整体社区结构。它主要基于两个关键的想法：一是数据在高维空间里其实是分布在一个隐藏的、简单的形状（流形）上的，就好像一群星星虽然在三维宇宙空间里，但其实它们大致是分布在一个二维的螺旋形状（这是一种简单的流形）上；二是通过衡量数据点之间的相似性来确定这个流形的结构。 

## 计算步骤通俗解释 
### 步骤一 ：寻找邻居（局部结构

对于高维空间中的每个数据点，算法会去寻找离它最近的一些数据点，这些最近的数据点就是它的邻居，就好像在一个小镇里，每个居民（数据点）都有自己最熟悉的一些邻居。通常会用一个参数\(k\) 来指定找多少个最近邻居，比如\(k = 10\)，就是找离这个数据点最近的 10 个数据点。 

计算这些邻居之间的距离，一般用欧几里得距离（简单理解就是两点之间直线的长度）来衡量。距离近的邻居之间的相似性就高，就像住在隔壁的邻居关系更亲密一样。

### 步骤二 ：构建模糊相似性（局部结构）

根据找到的邻居和它们之间的距离，构建一个模糊相似性矩阵。这个矩阵就像是一个表格，记录了每个数据点和它邻居之间的“亲密程度”。 

用一个特殊的函数（比如指数函数或者高斯函数）来把距离转化为相似性。如果两个数据点距离很近，它们的相似性就会很高，接近 1；如果距离很远，相似性就很低，接近 0。这就好比用温度计来衡量邻居之间的“热度”，距离近的“热度”高，距离远的“热度”低。 

### 步骤三 ：构建高维空间的图（全局结构）

把所有的数据点看成是一个图（网络）中的节点，节点之间的边的权重就是刚才计算出来的模糊相似性。这样就构建了一个表示高维空间数据结构的图，这个图既有每个数据点周围的局部信息（邻居关系），也有整体的连接信息，就像一个由很多小路（边）连接各个居民点（节点）的小镇地图。 

### 步骤四 ：优化低维空间布局（全局结构）

现在要把这些数据点搬到低维空间里。开始的时候，数据点在低维空间里是随机分布的。 

然后通过一个优化过程，不断调整这些数据点在低维空间里的位置，使得低维空间里的这个图和高维空间里的图尽可能相似。这个过程就像是在一个新的、小的街区里重新安排居民的住房，要让新街区里居民之间的邻里关系和原来小镇里的差不多。 

在优化过程中，会有一个损失函数来衡量高维空间和低维空间之间的差异。算法会不断尝试减小这个损失，就像调整住房位置的时候，不断检查和原来小镇布局的差异，尽量让差异变小。最终得到的低维空间里的数据点分布，就是 UMAP 算法的结果，也就是降维后的数据表示。

# 举例
## 数据准备 

假设我们有一个简单的数据集，是一个三维空间中的点集，代表一些动物的特征，比如体长、体重和奔跑速度。数据集有 5 个数据点： 

$$
A=(1, 2, 3)，B=(2, 3, 4)，C=(4, 6, 8)，D=(5, 7, 9)，E=(10, 15, 20) 
$$

我们的目标是将这个三维数据集通过 UMAP 算法降到二维空间。 

## 寻找邻居 （以 k = 2 为例）

计算数据点之间的欧几里得距离。欧几里得距离公式为

$$
d=\sqrt{(x_2 - x_1)^2+(y_2 - y_1)^2+(z_2 - z_1)^2}
$$

对于点 $A=(1, 2, 3)$ ： 

计算 A 到 B 的距离：$d_{AB}=\sqrt{(2 - 1)^2+(3 - 2)^2+(4 - 3)^2}=\sqrt{1 + 1+ 1}=\sqrt{3}\approx1.73$

计算 A 到 C 的距离： $d_{AC}=\sqrt{(4 - 1)^2+(6 - 2)^2+(8 - 3)^2}=\sqrt{9 + 16+ 25}=\sqrt{50}\approx7.07$

以此类推，计算 A 到 D 和 E 的距离。 

根据距离找到每个点的 k = 2 个最近邻居。对于点 A，最近的两个邻居可能是 B 和（假设）D。 

## 构建模糊相似性 

假设我们使用高斯核函数来构建模糊相似性，公式为

$$
s(x,y)=\exp\left(-\frac{d(x,y)^2}{2\sigma^2}\right)
$$

其中 $d(x,y)$ 是两点之间的距离， $\sigma$ 是一个控制相似性分布的参数，假设 $\sigma = 1$ 。 

对于点 A 和它的邻居 B： 

$$
s_{AB}=\exp\left(-\frac{d_{AB}^2}{2\times1^2}\right)=\exp\left(-\frac{(\sqrt{3})^2}{2}\right)=\exp\left(-\frac{3}{2}\right)\approx0.22
$$

同样计算 A 和它另一个邻居 D 之间的模糊相似性。 

## 构建高维空间的图 

把每个数据点看作一个节点，节点之间的边的权重就是刚才计算的模糊相似性。 

例如，在这个图中，节点 A 和 B 之间有一条边，权重是 0.22，A 和 D 之间也有一条边，权重是（计算出来的值）。这样就构建了一个表示高维（三维）空间数据结构的图。 

## 优化低维空间布局 

开始时，将这些数据点随机放置在二维空间中。

定义一个损失函数，例如，它可以衡量高维空间中边的权重和低维空间中相应点之间距离的差异。

假设在低维空间中，点 A 的坐标初始为 $(x_1,y_1)$ ，点 B 的坐标为 $(x_2,y_2)$ ，可以用一个函数（如交叉熵损失函数）来衡量高维空间中 A 和 B 的模糊相似性与低维空间中两点距离之间的差异。

通过优化算法（如随机梯度下降）不断调整这些点在二维空间中的位置，以减小这个损失函数的值。

经过多次迭代后，最终确定数据点在二维空间中的位置，完成降维。例如，经过优化后，点 A 可能被放置在 (0.1,0.2)，点 B 可能被放置在 (0.3,0.4) 等，这些新的二维坐标就是 UMAP 算法对原始三维数据点降维后的结果。

# UMap 优化低维空间布局详细举例步骤
## UMAP 优化低维空间布局的基本原理 

UMAP 通过最小化一个特定的损失函数来优化低维空间布局，这个损失函数主要衡量高维空间和低维空间中数据点之间相似性的差异。它基于梯度下降的思想，在每次迭代中，根据损失函数对低维空间中数据点坐标的梯度来更新这些坐标，使得低维空间中的数据点布局逐渐趋近于能够更好地反映高维空间数据相似性结构的状态。 

## 举例说明优化过程 
### 步骤一 ：初始化低维空间数据点位置

假设我们有一个简单的高维数据集，包含三个数据点 A、B、C，通过 UMAP 算法要将其从三维空间降到二维空间。首先，在二维空间中随机初始化这三个点的位置，例如 $A=(0.1,0.2)，B=(0.3,0.4)，C=(0.5,0.6)$ 。 

### 步骤二 ：定义损失函数

UMAP 使用的损失函数比较复杂，这里简化理解。假设损失函数 $L$ 主要由两部分组成：一部分是基于数据点之间的吸引力，另一部分是基于数据点之间的排斥力。 

- 吸引力部分：对于高维空间中相似的两个数据点（例如 A 和 B），在低维空间中希望它们也比较靠近。可以用一个函数来衡量这种吸引力损失，比如 $L_{attraction}= \sum_{i,j \in \text{similar pairs}} (d_{ij}^{low} - d_{ij}^{high})^2$ ，其中 $d_{ij}^{low}$ 是低维空间中数据点 $i$ 和 $j$ 之间的距离， $d_{ij}^{high}$ 是高维空间中它们之间的距离，这个求和是对所有高维空间中相似的数据点对进行的。 
- 排斥力部分：对于低维空间中所有的数据点，不希望它们过于靠近而导致重叠，所以有一个排斥力的损失函数部分。例如， $L_{repulsion}= \sum_{i \neq j} \max(0, \epsilon - d_{ij}^{low})^2$ ，其中 $\epsilon$ 是一个小的正数，表示数据点之间的最小期望距离。 
- 总的损失函数 $L = L_{attraction}+L_{repulsion}$ 。

### 步骤三 ：计算梯度并更新位置计算损失函数 L

对每个数据点坐标的梯度。例如，对于数据点 $A=(x_A,y_A)$ ，计算 $\frac{\partial L}{\partial x_A}$ 和 $\frac{\partial L}{\partial y_A}$ 。 

假设经过计算， $\frac{\partial L}{\partial x_A}=0.2$ ， $\frac{\partial L}{\partial y_A}= - 0.1$ ，并且我们选择学习率 $\eta = 0.01$ 。 

那么更新 A 的位置： $x_A^{new}=x_A - \eta\times\frac{\partial L}{\partial x_A}=0.1 - 0.01\times0.2 = 0.098$ ， $y_A^{new}=y_A - \eta\times\frac{\partial L}{\partial y_A}=0.2 - 0.01\times(-0.1)=0.201$ 。 

同样地，对 B 和 C 的数据点坐标进行计算和更新。 

### 步骤四 ：迭代优化

重复步骤三，经过多次迭代（例如 100 次），每次迭代都会使得损失函数的值逐渐减小，低维空间中的数据点位置不断调整。随着迭代的进行，低维空间中的数据点布局会越来越符合高维空间中的数据相似性结构。例如，经过 100 次迭代后， $A=(0.15,0.22)$ ， $B=(0.32,0.41)$ ， $C=(0.53,0.60)$ ，此时损失函数的值达到了一个相对较小的值，认为低维空间布局已经得到了较好的优化。
