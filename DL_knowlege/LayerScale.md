来自论文 [Cait](https://readpaper.com/paper/3146097248),LayerScale 在每个残差块的输出上添加一个可学习的对角矩阵，该矩阵被初始化为接近 0。在每个残差块之后添加这个简单的层可以提高训练的动态性，使我们能够训练更深层次的大容量.

在 18 层之前，它们初始化为 0.1。若网络更深，则在 24 层之前初始化为 $10^{-5}$ 。若网络更深，则在之后更深的网络中初始化为 $10^{-6}$ 。这样做的目的是希望使得每个 block 在一开始的时候更接近 Identity mapping，在训练的过程中逐渐地学习到模块自身的 function

# 参考

- <https://zhuanlan.zhihu.com/p/363370678>