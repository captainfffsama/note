#自监督 

[toc]

- 论文: <https://readpaper.com/pdf-annotate/note?pdfId=4745545008782786561&noteId=1816025687761446400>
- 代码: <https://github.com/facebookresearch/dinov2>
- 作者团队: META

# 全文关键点翻译
## 摘要

近年来, 自然语言处理 (NLP) 领域上使用大规模数据来对模型进行预训练的突破, 为视觉领域基础模型的工作开辟了道路. 这些模型可以产生通用的视觉特征, 可以极大的简化在各个系统中图像的处理. 即这些通用特征可以在不同的图片分布和任务上使用而无需微调. 现有的预训练方法, 特别是自监督方法, 是可以通过在足量的不同来源且精心筛选的数据上训练来得到这样的通用特征的. 我们回顾了现有方法, 并结合不同的技术来调整在不同的模型和数据规模上进行预训练的方法. 现有多数方法主要旨在加速和稳定大规模训练. 而本文在数据方面, 我们构建了一套自动化流水线来构建一个专用的, 多样的精心筛选的数据集, 而不是像其他自监督方法那样使用杂乱没有筛选的数据训练. 模型方面, 我们训练了一个 1 B 参数的 ViT, 并将其蒸馏成了一堆小模型. 这些模型在多数图像和像素级基准上都超越了现有的最好的通用特征模型, OpenCLIP.

## 1. 引言

学习一个任务无关的预训练表征是 NLP 中的基础工作. 人们可以无需微调的 " 按原样 " 使用这些特征, 就可以在下游任务上取得比特定任务模型还要好的效果. 而实现这种效果的方式通常是使用大规模的原始文本来对语言模型或词向量进行无监督预训练.

参照 NLP, 我们期望视觉邻域也可以有这样一个模型. 即这些模型无论是在图像级任务 (比如图像分类) 还是像素级任务 (比如分割), 都可以产生一个开箱即用的特征. 实现这个目标最有希望的方向是文本指导预训练, 即使用文本监督指导特征的训练. 但是这种文本指导预训练的方法限制了模型保留图像信息的丰富性, 毕竟文本只是图像信息的一种精简, 而一些复杂的像素级信息不会在此类监督训练中保留. 另外, 这些图像编码器需要图文对齐的语料库来训练, 不能直接使用原始数据训练.

而另一种有希望的方向是自监督学习, 即仅使用图片学习特征. 这类方法从概念上来说更接近于语言建模任务, 他们可以获取图片或者像素级的信息. 尽管这个方向看上去可以学到一个通用特征, 但是目前大部分的工作还是在一个相对较小的数据集 (ImageNet-1 k) 上进行的. 虽然也有一些工作尝试在更大数据规模上进行, 但是他们主要使用的是一些很粗糙的数据集, 这通常会导致特征的质量显著下降. 因为这类数据确实对数据质量和多样性的控制, 而这两个要素对产生好特征是至关重要的.

本文, 我们将探索自监督学习是否可以在一个大规模且高质量的数据集上学到一个通用视觉特征. 我们回顾了现有的可以学习图片和像素级特征的判别式自监督方法, 比如 iBOT, 并在更大数据集下重新思考了它们的设计. 我们大部分的技术都旨在在不同规模的模型和数据规模上实现稳定且高效的判别式自监督学习. 这些改进使得我们方法比一般判别式自监督方法快 2 倍, 内存上省 3 倍.

对于预训练数据, 我们则建立了一个自动流水线来过滤和平衡那些未整理的图片. 这是启发自 [Wenzek 等人](https://readpaper.com/paper/2989539713) 的 NLP 工作, 即主要利用数据间的相似性而且外部元数据, 且不需要人工标注. 处理这些图片的一大难点是如何平衡多样性, 避免在某几个域 (dominant) 上过拟合. 使用基本的聚类方法就可以较好的解决这个问题. 我们收集来一个小而多样, 大约 142 M 张图片的数据集来验证我们的方法.

最后, 我们提供了各种预训练模型 (搞笑泥浆去尿), 即 DINOv 2, 它是使用我们数据训练的各种 ViT 模型. 我们还开源了所有模型和代码, DINOv 2 可以在任意数据上复现. 如图 2 所示, 我们在不同视觉任务基准上测试了 DINOv 2. 结果表明, 自监督预训练是可以学到一个较通用的冻结特征参数, 其效果可以和目前公开的最佳弱监督模型相媲美.

![dinov2_meta_fig1](../../Attachments/dinov2_meta_fig1.png)

![dinov2_meta_fig2](../../Attachments/dinov2_meta_fig2.png)

## 2. 相关工作
### 图像内部自监督训练

自监督方法的第一大类是专注于从图像构建角度来进行建模, 比如从图片的剩余部分中获得监督信号 **(已知一部分预测剩下一部分)** .这个思路来源于 Doersch 等人 2015 年的工作, 他们通过预测给定图像块的上下文来进行训练. 还有很多工作是基于图像重新着色, 预测图像转换, 图像修复或者图像块重排序. 而最近, 随着基于 patch 的架构, 比如 ViT 的出现, 学界开始回顾图片修复做预训练, 包括在特征空间中做图像修复. 值得一提的是, He 等人的工作, MAE 效果不错, 其学到的特征在下游任务微调时可以提点. 然后其他人借鉴 MAE, 将其应用在来视频等其他模式上. 然而他们的特征依然需要监督微调, 而**我们的特征是开箱即用的**.

### 判别式自监督训练

第二大类的工作思想和我们类似, 使用一组图片中的判别信号来学习特征. 这类方法可以追溯到早期深度学习工作, 在实例分类方法出现之后开始变得流行. 有一些工作基于实例级目标, 或是聚类提出来一些改进方法. 这些方法在 ImageNet 等一些标准数据集上可以提供较好的冻结特征, 但是难以扩展到更大的模型上. 我们在大型预训练数据集和模型的背景下重新审视这些方法的训练. 我们在 Zhou 等人的工作上进行进一步实验, 因为我们发现这个工作很适合模型数据缩放.

### 自监督预训练的缩放

目前越来越多的工作着眼于自监督学习在数据和模型大小上缩放的能力. 多数工作都是使用大规模的未经清理的数据来做无监督训练. 这些工作证明了判别自监督式方法能力随着数据规模提升, 但是由于预训练数据质量不高, 多数工作还是需要微调他们的特征. 值得一提的是, Goyal 等人还发现这些方法在预训练数据足够的情况下, 主要收益于模型大小的缩放. 这一系列的工作质疑自监督方法处理任何数据的能力, 而我们专注于生产最好的预训练编码器.

### 数据自动管理

我们数据集构建借鉴自数据检索社区. 而在半监督领域中, 使用数据检索来增强训练集也很常见. 一些工作使用哈希标签或者其他元数据来过滤未清洗的数据. 不同于这类工作, 我们不使用元数据作为监督来过滤图片, 而是利用图片之间的视觉相似性. 我们的方法主要是受文本数据处理流程启发, 即在 wikipedia 上训练一个语言模型, 然后从未清洗数据源中提取文本进行评分.

## 3. 数据处理

我们从大量未经清洗的数据中检索和几个精选数据集类似的图片来构建我们的 LVD-142 M 数据集. 下面, 我们将展示我们数据处理流程中的一些主要环节, 包括在精选和未清洗数据集上进行的处理, 图像去重和检索系统. 如图 3 所示, 我们的处理流程不需要任何的元数据或是文本, 仅在图像上工作. 附录 A 中包含了我们方法上的更多细节.

![dinov2_meta_fig3](../../Attachments/dinov2_meta_fig3.png)

### 数据源

附录表格 15 详细列出了我们使用的精选数据集, 包括 ImageNet-22 k, ImageNet-1 k 训练部分, Google Landmarks 等. 对于未清洗的粗略数据, 我们是直接从网上爬的原始图像. 对于存储库中每个网页, 我们使用 `<img>` 标签来提取图片的 URL. 然后丢弃不安全和受域限制的 URL, 然后对下载下来的图片进行后处理 (包括 PCA 哈希去重, NSFW 过滤, 模糊可识别面孔). 最后得到 1.2 B 的无重复图像.

### 去重

我们对未清洗的数据应用了 [Pizzi 等人 2022 年工作](https://readpaper.com/paper/4594878964267229185) 中的复制检测流程, 去除了相似图片. 这将减少数据集中的冗余并增加了图片的多样性. 我们还将任何可能和我们工作中使用到的基准数据集中验证和测试部分相似的图片都移除了.

### 自监督图像检索

通过从未清洗的数据中检索和精选数据集中类似的图片, 我们构建了清洗之后的预训练数据集. 我们使用在 ImageNet-22 k 上预训练的 ViT-H/16 作为图像编码器,使用 k-means 聚类未清洗的数据, 距离计算使用余弦相似度. 对于待查数据集 (应该是 benchmark),给定一个待查图片, 若其足够大, 我们就对每个 query 图片检索 N (通常为 4) 个最近邻居. 若很小, 我们就从其对应的聚类中抽取 M 张图片 (未清洗). 我们将对检索结果进行确认, 然后再调整 M 和 N.

### 实现细节

在去重和检索阶段, 我们都使用 [Faiss](https://github.com/facebookresearch/faiss) 计算查找最近邻居的嵌入. 我们还使用了 GPU 加速索引, 带有乘积量化编码的文件倒排索引. 整个处理流程使用了 20 个节点, 每个节点 8 块 V 100, 时间两天.

## 4. 自监督式判别预训练

本文方法可以视为是在 SwAV 框架下 DINO 和 iBOT 损失的组合. 我们还添加了一个正则化来扩展特征, 并为高分辨率图片添加了一个短期的训练. 接下来, 我们将快速过一下这些方法, 更多细节可以在相关论文或我们的开源代码里找到.

### 图像级目标

主要考虑学生模型特征和老师模型特征之间的交叉熵损失. 这里的特征都是 vit 的 CLS token, 从同一图像的不同剪裁中获得. 我们学习学生模型的参数, 老师模型参数是学生模型参数的指数移动平均法.

### Patch 级目标

随机遮盖一些学生网络上的输入 patch，然后对学生网络和教师网络在遮盖的 patch 上的特征进行交叉熵损失函数的比较。这种损失函数与图像级别的损失函数相结合。

### 将以上两个目标的头权重解绑

作我们发现，将两种目标的权重绑定在一起会导致模型在 Patch-level 欠拟合，在 Image-level 过拟合。分离这些权重可以解决这个问题，并提高两种尺度上的性能。

### Sinkhorn-Knopp 中心化

这个方法是对 DINO 和 iBOT 两种方法中的一些步骤进行改进，具体可见这篇论文：Unsupervised Learning of Visual Features by Contrasting Cluster Assignments (arxiv.org)

### KoLeo 正则化

KoLeo regularizer 是一种正则化方法，它通过计算特征向量之间的差异来确保它们在批次内均匀分布，其源自于 Kozachenko-Leonenko 微分熵估计器，并鼓励批次内特征的均匀跨度。具体见：Spreading vectors for similarity search (arxiv.org)

### 自适应分辨率

这一步主要是涉及在预训练的最后一段时间内，将图像的分辨率提高到 518×518 ，便在下游任务中更好地处理像素级别的信息，例如分割或检测任务。高分辨率的图像通常需要更多的计算资源和存储空间，因此只在预训练的最后阶段使用这种方法，以减少时间和资源成本。

## 5. 高效实现细节

我们使用 A 100 和 PyTorch 2.0 训练模型. 代码也可以仅用来做特征提取. 模型细节见附录表 17. 相同硬件条件下, 对比 iBOT 实现, DINOv 2 实现要快两倍, 而内存消耗仅其 1/3.

### Fast and memory-efficient attention

我们自己实现了 FlashAttention 来提高自监督层内存利用率和速度. 在所有情况下, 我们的实现都不弱于原版, 且支持更多硬件. 由于 GPU 的特性, 每个头的嵌入维度是 64 的整数倍时, 效率最高, 而整个嵌入维度是 256 的整数倍时, 矩阵操作更加高效. 为了最大化计算效率, 我们的 ViT-g 结果和 [Zhai 等人](https://readpaper.com/paper/3172942063) 提出的略有不同. 我们的嵌入维度是 1536, 有 24 个头 (一个头 64 维), 而原版是 1408 维, 16 个头 (一个头 88 维). 我们的实验没有发现两者最终精度上有大的差别, 我们的 Vit-g 参数量是 1.1 B.

### Nested tensors in self-attention
### Efficient stochastic depth
### Fully-Sharded Data Parallel (FSDP)
### Model distillation

## 6. 消融实验
### 6.1 训练策略优化
### 6.2 预训练数据源
### 6.3 模型和数据规模
### 6.4 损失的组成
### 6.5 知识蒸馏影响
### 6.6 分辨率影响

## 7. 结果
### 7.1 ImageNet 分类
### 7.2 其他图像和视频分类基准
### 7.3 实例识别
### 7.4 密集识别任务
### 7.5 定性结果

## 8. 公平性和偏见分析
### 8.1 地理公平性
### 8.2 性别, 肤色, 年龄公平性

## 9. 模型训练时的环境影响

## 10. 展望


# 相关解析参考
- <https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/>
- <https://levelup.gitconnected.com/meta-dino-how-self-supervised-learning-is-changing-computer-vision-1666a5e43dbb>
- <https://zhuanlan.zhihu.com/p/623274167>
- <https://mp.weixin.qq.com/s/fjsmYZ6DK-uRTdTnHEIaEw>
