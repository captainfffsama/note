#å›¾åƒåˆ†å‰²

# Per-Pixel Classification is Not All You Need for Semantic Segmentation

- ä½œè€…:
	1. Bowen Chengâ€ƒ
	2. Alexander G. Schwing â€ƒ
	3. Alexander Kirillov
- æœºæ„:
	1. Facebook AI Research (FAIR) â€ƒâ€ƒ
	2. University of Illinois at Urbana-Champaign (UIUC)
- ä»£ç : <https://github.com/facebookresearch/MaskFormer>
- æ–‡ç« :<https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2107.06278>
- ç›¸å…³è§£è¯»:
	- <https://zhuanlan.zhihu.com/p/389457610>
	- <https://www.zhihu.com/question/472122951>
	- <https://medium.com/@HannaMergui/maskformer-per-pixel-classification-is-not-all-you-need-for-semantic-segmentation-1e2fe3bf31cb>

## æ‘˜è¦

Modern approaches typically formulate semantic segmentation as a _per-pixel classification_ task, while instance-level segmentation is handled with an alternative _mask classification_. Our key insight: mask classification is sufficiently general to solve both semantic- and instance-level segmentation tasks in a unified manner using the exact same model, loss, and training procedure. Following this observation, we propose MaskFormer, a simple mask classification model which predicts a set of binary masks, each associated with a _single_ global class label prediction. Overall, the proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results. In particular, we observe that MaskFormer outperforms per-pixel classification baselines when the number of classes is large. Our mask classification-based method outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K) and panoptic segmentation (52.7 PQ on COCO) models.

Project page: [https://bowenc0221.github.io/maskformer](https://bowenc0221.github.io/maskformer)  
ç°ä»£æ–¹æ³•é€šå¸¸å°†è¯­ä¹‰åˆ†å‰²è¡¨è¿°ä¸ºæ¯åƒç´ åˆ†ç±»ä»»åŠ¡ï¼Œè€Œå®ä¾‹çº§åˆ†å‰²åˆ™ä½¿ç”¨æ›¿ä»£æ©ç åˆ†ç±»è¿›è¡Œå¤„ç†ã€‚æˆ‘ä»¬çš„ä¸»è¦è§è§£æ˜¯ï¼šæ©ç åˆ†ç±»è¶³å¤Ÿé€šç”¨ï¼Œå¯ä»¥ä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ¨¡å‹ã€æŸå¤±å’Œè®­ç»ƒè¿‡ç¨‹ä»¥ç»Ÿä¸€çš„æ–¹å¼è§£å†³è¯­ä¹‰çº§å’Œå®ä¾‹çº§åˆ†å‰²ä»»åŠ¡ã€‚æ ¹æ®è¿™ä¸€è§‚å¯Ÿç»“æœï¼Œæˆ‘ä»¬æå‡ºäº† MaskFormerï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„æ©ç åˆ†ç±»æ¨¡å‹ï¼Œå®ƒé¢„æµ‹ä¸€ç»„äºŒè¿›åˆ¶æ©ç ï¼Œæ¯ä¸ªæ©ç éƒ½ä¸å•ä¸ªå…¨å±€ç±»æ ‡ç­¾é¢„æµ‹ç›¸å…³è”ã€‚æ€»ä½“è€Œè¨€ï¼Œæ‰€æå‡ºçš„åŸºäºæ©æ¨¡åˆ†ç±»çš„æ–¹æ³•ç®€åŒ–äº†è¯­ä¹‰å’Œå…¨æ™¯åˆ†å‰²ä»»åŠ¡çš„æœ‰æ•ˆæ–¹æ³•çš„æ ¼å±€ï¼Œå¹¶æ˜¾ç¤ºå‡ºä¼˜å¼‚çš„å®è¯ç»“æœã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå½“ç±»æ•°é‡è¾ƒå¤§æ—¶ï¼ŒMaskFormer çš„æ€§èƒ½ä¼˜äºæ¯åƒç´ åˆ†ç±»åŸºçº¿ã€‚æˆ‘ä»¬åŸºäºæ©æ¨¡åˆ†ç±»çš„æ–¹æ³•ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„è¯­ä¹‰ï¼ˆADE20K ä¸Šçš„ 55.6 mIoUï¼‰å’Œå…¨æ™¯åˆ†å‰²ï¼ˆCOCO ä¸Šçš„ 52.7 PQï¼‰æ¨¡å‹ã€‚ 

## 1 Introduction1 å¼•è¨€

The goal of semantic segmentation is to partition an image into regions with different semantic categories. Starting from Fully Convolutional Networks (FCNs) work of LongÂ _etÂ al_.Â \[[30](#bib.bib30)\], most _deep learning-based_ semantic segmentation approaches formulate semantic segmentation as _per-pixel classification_ (FigureÂ [1](#S1.F1 "Figure 1 â€£ 1 Introduction â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation") left), applying a classification loss to each output pixelÂ \[[9](#bib.bib9), [52](#bib.bib52)\]. Per-pixel predictions in this formulation naturally partition an image into regions of different classes.  
è¯­ä¹‰åˆ†å‰²çš„ç›®æ ‡æ˜¯å°†å›¾åƒåˆ’åˆ†ä¸ºå…·æœ‰ä¸åŒè¯­ä¹‰ç±»åˆ«çš„åŒºåŸŸã€‚ä» Long ç­‰äºº\[30\] çš„å…¨å·ç§¯ç½‘ç»œï¼ˆFCNï¼‰å·¥ä½œå¼€å§‹ï¼Œå¤§å¤šæ•°åŸºäºæ·±åº¦å­¦ä¹ çš„è¯­ä¹‰åˆ†å‰²æ–¹æ³•å°†è¯­ä¹‰åˆ†å‰²è¡¨è¿°ä¸ºæ¯åƒç´ åˆ†ç±»ï¼ˆå·¦å›¾ 1ï¼‰ï¼Œå¯¹æ¯ä¸ªè¾“å‡ºåƒç´ åº”ç”¨åˆ†ç±»æŸå¤±\[9,52\]ã€‚æ­¤å…¬å¼ä¸­çš„æ¯åƒç´ é¢„æµ‹è‡ªç„¶åœ°å°†å›¾åƒåˆ’åˆ†ä¸ºä¸åŒç±»åˆ«çš„åŒºåŸŸã€‚

Mask classification is an alternative paradigm that disentangles the image partitioning and classification aspects of segmentation. Instead of classifying each pixel, mask classification-based methods predict a set of binary masks, each associated with a _single_ class prediction (FigureÂ [1](#S1.F1 "Figure 1 â€£ 1 Introduction â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation") right). The more flexible mask classification dominates the field of instance-level segmentation. Both Mask R-CNNÂ \[[21](#bib.bib21)\] and DETRÂ \[[4](#bib.bib4)\] yield a single class prediction per segment for instance and panoptic segmentation. In contrast, per-pixel classification assumes a static number of outputs and cannot return a variable number of predicted regions/segments, which is required for instance-level tasks.  
æ©ç åˆ†ç±»æ˜¯ä¸€ç§æ›¿ä»£èŒƒå¼ï¼Œå®ƒè§£å¼€äº†åˆ†å‰²çš„å›¾åƒåˆ†åŒºå’Œåˆ†ç±»æ–¹é¢ã€‚åŸºäºæ©ç åˆ†ç±»çš„æ–¹æ³•ä¸æ˜¯å¯¹æ¯ä¸ªåƒç´ è¿›è¡Œåˆ†ç±»ï¼Œè€Œæ˜¯é¢„æµ‹ä¸€ç»„äºŒè¿›åˆ¶æ©ç ï¼Œæ¯ä¸ªæ©ç éƒ½ä¸å•ä¸ªç±»é¢„æµ‹ç›¸å…³è”ï¼ˆå³å›¾ 1ï¼‰ã€‚æ›´çµæ´»çš„æ©ç åˆ†ç±»åœ¨å®ä¾‹çº§åˆ†å‰²é¢†åŸŸå ä¸»å¯¼åœ°ä½ã€‚Mask R-CNN \[21\] å’Œ DETR \[4\] éƒ½å¯¹æ¯ä¸ªæ®µäº§ç”Ÿä¸€ä¸ªç±»åˆ«é¢„æµ‹ï¼Œä¾‹å¦‚å…¨æ™¯åˆ†å‰²ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ¯åƒç´ åˆ†ç±»å‡å®šè¾“å‡ºæ•°é‡ä¸ºé™æ€æ•°ï¼Œå¹¶ä¸”æ— æ³•è¿”å›å¯å˜æ•°é‡çš„é¢„æµ‹åŒºåŸŸ/æ®µï¼Œè€Œè¿™æ˜¯å®ä¾‹çº§ä»»åŠ¡æ‰€å¿…éœ€çš„ã€‚

![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/x1.png)

Figure 1: Per-pixel classification _vs_. mask classification. (left) Semantic segmentation with per-pixel classification applies the same classification loss to each location. (right) Mask classification predicts a set of binary masks and assigns a single class to each mask. Each prediction is supervised with a per-pixel binary mask loss and a classification loss. Matching between the set of predictions and ground truth segments can be done either via _bipartite matching_ similarly to DETRÂ \[[4](#bib.bib4)\] or by _fixed matching_ via direct indexing if the number of predictions and classes match, _i.e_., if N=Kğ‘ğ¾N=K.  
å›¾ 1ï¼šæ¯åƒç´ åˆ†ç±»ä¸è’™ç‰ˆåˆ†ç±»ã€‚ï¼ˆå·¦ï¼‰ä½¿ç”¨æ¯åƒç´ åˆ†ç±»çš„è¯­ä¹‰åˆ†å‰²å°†ç›¸åŒçš„åˆ†ç±»æŸå¤±åº”ç”¨äºæ¯ä¸ªä½ç½®ã€‚ï¼ˆå³ï¼‰æ©ç åˆ†ç±»é¢„æµ‹ä¸€ç»„äºŒè¿›åˆ¶æ©ç ï¼Œå¹¶ä¸ºæ¯ä¸ªæ©ç åˆ†é…ä¸€ä¸ªç±»ã€‚æ¯ä¸ªé¢„æµ‹éƒ½é€šè¿‡æ¯åƒç´ äºŒè¿›åˆ¶æ©ç æŸå¤±å’Œåˆ†ç±»æŸå¤±è¿›è¡Œç›‘ç£ã€‚é¢„æµ‹é›†å’ŒçœŸå®çº¿æ®µä¹‹é—´çš„åŒ¹é…å¯ä»¥é€šè¿‡ç±»ä¼¼äº DETR \[4\] çš„äºŒåˆ†åŒ¹é…æ¥å®Œæˆï¼Œæˆ–è€…å¦‚æœé¢„æµ‹å’Œç±»çš„æ•°é‡åŒ¹é…ï¼Œå³å¦‚æœ N=Kğ‘ğ¾N=K .

Our key observation: mask classification is sufficiently general to solve both semantic- and instance-level segmentation tasks. In fact, before FCNÂ \[[30](#bib.bib30)\], the best performing semantic segmentation methods like O2PÂ \[[5](#bib.bib5)\] and SDSÂ \[[20](#bib.bib20)\] used a mask classification formulation. Given this perspective, a natural question emerges: _can a single mask classification model simplify the landscape of effective approaches to semantic- and instance-level segmentation tasks? And can such a mask classification model outperform existing per-pixel classification methods for semantic segmentation?_  
æˆ‘ä»¬çš„ä¸»è¦è§‚å¯Ÿç»“æœæ˜¯ï¼šæ©ç åˆ†ç±»è¶³å¤Ÿé€šç”¨ï¼Œå¯ä»¥è§£å†³è¯­ä¹‰çº§å’Œå®ä¾‹çº§çš„åˆ†å‰²ä»»åŠ¡ã€‚äº‹å®ä¸Šï¼Œåœ¨ FCN \[30\] ä¹‹å‰ï¼Œæ€§èƒ½æœ€å¥½çš„è¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œå¦‚ O2P \[5\] å’Œ SDS \[20\]ï¼Œéƒ½ä½¿ç”¨äº†æ©ç åˆ†ç±»å…¬å¼ã€‚ä»è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼Œä¸€ä¸ªè‡ªç„¶è€Œç„¶çš„é—®é¢˜å‡ºç°äº†ï¼šå•ä¸ªæ©ç åˆ†ç±»æ¨¡å‹èƒ½å¦ç®€åŒ–è¯­ä¹‰çº§å’Œå®ä¾‹çº§åˆ†å‰²ä»»åŠ¡çš„æœ‰æ•ˆæ–¹æ³•çš„æ ¼å±€ï¼Ÿè¿™æ ·çš„æ©ç åˆ†ç±»æ¨¡å‹èƒ½å¦ä¼˜äºç°æœ‰çš„æ¯åƒç´ åˆ†ç±»æ–¹æ³•è¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Ÿ

To address both questions we propose a simple MaskFormer approach that seamlessly converts any existing per-pixel classification model into a mask classification. Using the set prediction mechanism proposed in DETRÂ \[[4](#bib.bib4)\], MaskFormer employs a Transformer decoderÂ \[[41](#bib.bib41)\] to compute a set of pairs, each consisting of a class prediction and a mask embedding vector. The mask embedding vector is used to get the binary mask prediction via a dot product with the per-pixel embedding obtained from an underlying fully-convolutional network. The new model solves both semantic- and instance-level segmentation tasks in a unified manner: no changes to the model, losses, and training procedure are required. Specifically, for semantic and panoptic segmentation tasks alike, MaskFormer is supervised with the same per-pixel binary mask loss and a single classification loss per mask. Finally, we design a simple inference strategy to blend MaskFormer outputs into a task-dependent prediction format.  
ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„ MaskFormer æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ— ç¼åœ°å°†ä»»ä½•ç°æœ‰çš„æ¯åƒç´ åˆ†ç±»æ¨¡å‹è½¬æ¢ä¸ºæ©ç åˆ†ç±»ã€‚ä½¿ç”¨ DETR \[4\] ä¸­æå‡ºçš„é›†åˆé¢„æµ‹æœºåˆ¶ï¼ŒMaskFormer ä½¿ç”¨ Transformer è§£ç å™¨\[41\] æ¥è®¡ç®—ä¸€ç»„å¯¹ï¼Œæ¯ä¸ªå¯¹ç”±ä¸€ä¸ªç±»é¢„æµ‹å’Œä¸€ä¸ªæ©ç åµŒå…¥å‘é‡ç»„æˆã€‚æ©æ¨¡åµŒå…¥å‘é‡ç”¨äºé€šè¿‡ç‚¹ç§¯è·å¾—äºŒè¿›åˆ¶æ©ç é¢„æµ‹ï¼Œæ¯åƒç´ åµŒå…¥ä»åº•å±‚å…¨å·ç§¯ç½‘ç»œè·å¾—ã€‚æ–°æ¨¡å‹ä»¥ç»Ÿä¸€çš„æ–¹å¼è§£å†³äº†è¯­ä¹‰çº§å’Œå®ä¾‹çº§åˆ†å‰²ä»»åŠ¡ï¼šæ— éœ€æ›´æ”¹æ¨¡å‹ã€æŸå¤±å’Œè®­ç»ƒè¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºè¯­ä¹‰å’Œå…¨æ™¯åˆ†å‰²ä»»åŠ¡ï¼ŒMaskFormer çš„ç›‘ç£ä½œç”¨ä¸æ¯ä¸ªåƒç´ çš„äºŒè¿›åˆ¶äºŒè¿›åˆ¶æ©ç æŸå¤±å’Œæ¯ä¸ªæ©ç çš„å•ä¸ªåˆ†ç±»æŸå¤±ç›¸åŒã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç®€å•çš„æ¨ç†ç­–ç•¥ï¼Œå°† MaskFormer è¾“å‡ºæ··åˆåˆ°ä¸ä»»åŠ¡ç›¸å…³çš„é¢„æµ‹æ ¼å¼ä¸­ã€‚

We evaluate MaskFormer on five semantic segmentation datasets with various numbers of categories: CityscapesÂ \[[15](#bib.bib15)\] (19 classes), Mapillary VistasÂ \[[34](#bib.bib34)\] (65 classes), ADE20KÂ \[[55](#bib.bib55)\] (150 classes), COCO-Stuff-10KÂ \[[3](#bib.bib3)\] (171 classes), and ADE20K-FullÂ \[[55](#bib.bib55)\] (847 classes). While MaskFormer performs on par with per-pixel classification models for Cityscapes, which has a few diverse classes, the new model demonstrates superior performance for datasets with larger vocabulary. We hypothesize that a single class prediction per mask models fine-grained recognition better than per-pixel class predictions. MaskFormer achieves the new state-of-the-art on ADE20K (55.6 mIoU) with Swin-TransformerÂ \[[29](#bib.bib29)\] backbone, outperforming a per-pixel classification modelÂ \[[29](#bib.bib29)\] with the same backbone by 2.1 mIoU, while being more efficient (10% reduction in parameters and 40% reduction in FLOPs).  
æˆ‘ä»¬åœ¨äº”ä¸ªå…·æœ‰ä¸åŒç±»åˆ«çš„è¯­ä¹‰åˆ†å‰²æ•°æ®é›†ä¸Šè¯„ä¼°äº† MaskFormerï¼šCityscapes \[15\]ï¼ˆ19 ä¸ªç±»ï¼‰ã€Mapillary Vistas \[34\]ï¼ˆ65 ä¸ªç±»ï¼‰ã€ADE20K \[55\]ï¼ˆ150 ä¸ªç±»ï¼‰ã€COCO-Stuff-10K \[3\]ï¼ˆ171 ä¸ªç±»ï¼‰å’Œ ADE20K-Full \[55\]ï¼ˆ847 ä¸ªç±»ï¼‰ã€‚è™½ç„¶ MaskFormer çš„æ€§èƒ½ä¸å…·æœ‰å‡ ä¸ªä¸åŒç±»åˆ«çš„ Cityscapes çš„æ¯åƒç´ åˆ†ç±»æ¨¡å‹ç›¸å½“ï¼Œä½†æ–°æ¨¡å‹åœ¨å…·æœ‰è¾ƒå¤§è¯æ±‡é‡çš„æ•°æ®é›†ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬å‡è®¾æ¯ä¸ªæ©ç çš„å•ä¸ªç±»é¢„æµ‹æ¯”æ¯ä¸ªåƒç´ ç±»é¢„æµ‹æ›´å¥½åœ°æ¨¡æ‹Ÿç»†ç²’åº¦è¯†åˆ«ã€‚MaskFormer åœ¨é‡‡ç”¨ Swin-Transformer \[29\] ä¸»å¹²çš„ ADE20K ï¼ˆ55.6 mIoUï¼‰ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯ï¼Œæ¯”å…·æœ‰ç›¸åŒä¸»å¹²çš„æ¯åƒç´ åˆ†ç±»æ¨¡å‹\[29\] é«˜å‡º 2.1 mIoUï¼ŒåŒæ—¶æ•ˆç‡æ›´é«˜ï¼ˆå‚æ•°å‡å°‘ 10%ï¼ŒFLOP å‡å°‘ 40%ï¼‰ã€‚

Finally, we study MaskFormerâ€™s ability to solve instance-level tasks using two panoptic segmentation datasets: COCOÂ \[[28](#bib.bib28), [24](#bib.bib24)\] and ADE20KÂ \[[55](#bib.bib55)\]. MaskFormer outperforms a more complex DETR modelÂ \[[4](#bib.bib4)\] with the same backbone and the same post-processing. Moreover, MaskFormer achieves the new state-of-the-art on COCO (52.7 PQ), outperforming prior state-of-the-artÂ \[[42](#bib.bib42)\] by 1.6 PQ. Our experiments highlight MaskFormerâ€™s ability to unify instance- and semantic-level segmentation.  
æœ€åï¼Œæˆ‘ä»¬ç ”ç©¶äº† MaskFormer ä½¿ç”¨ä¸¤ä¸ªå…¨æ™¯åˆ†å‰²æ•°æ®é›†ï¼ˆCOCO \[28ï¼Œ 24\] å’Œ ADE20K \[55\] è§£å†³å®ä¾‹çº§ä»»åŠ¡çš„èƒ½åŠ›ã€‚MaskFormer ä¼˜äºå…·æœ‰ç›¸åŒä¸»å¹²å’Œç›¸åŒåå¤„ç†çš„æ›´å¤æ‚çš„ DETR æ¨¡å‹\[4\]ã€‚æ­¤å¤–ï¼ŒMaskFormer åœ¨ COCOï¼ˆ52.7 PQï¼‰ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œæ¯”ä¹‹å‰æœ€å…ˆè¿›çš„\[42\] é«˜å‡º 1.6 PQã€‚æˆ‘ä»¬çš„å®éªŒçªå‡ºäº† MaskFormer ç»Ÿä¸€å®ä¾‹çº§å’Œè¯­ä¹‰çº§åˆ†å‰²çš„èƒ½åŠ›ã€‚

## 2 Related Works ç›¸å…³è‘—ä½œ

Both per-pixel classification and mask classification have been extensively studied for semantic segmentation. In early work, Konishi and YuilleÂ \[[25](#bib.bib25)\] apply per-pixel Bayesian classifiers based on local image statistics. Then, inspired by early works on non-semantic groupingsÂ \[[13](#bib.bib13), [36](#bib.bib36)\], mask classification-based methods became popular demonstrating the best performance in PASCAL VOC challengesÂ \[[18](#bib.bib18)\]. Methods like O2PÂ \[[5](#bib.bib5)\] and CFMÂ \[[16](#bib.bib16)\] have achieved state-of-the-art results by classifying mask proposalsÂ \[[6](#bib.bib6), [40](#bib.bib40), [2](#bib.bib2)\]. In 2015, FCNÂ \[[30](#bib.bib30)\] extended the idea of per-pixel classification to deep nets, significantly outperforming all prior methods on mIoU (a per-pixel evaluation metric which particularly suits the per-pixel classification formulation of segmentation).  
å¯¹äºè¯­ä¹‰åˆ†å‰²ï¼Œæ¯åƒç´ åˆ†ç±»å’Œæ©ç åˆ†ç±»éƒ½è¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ã€‚åœ¨æ—©æœŸçš„å·¥ä½œä¸­ï¼ŒKonishi å’Œ Yuille\[25\] åº”ç”¨äº†åŸºäºå±€éƒ¨å›¾åƒç»Ÿè®¡çš„æ¯åƒç´ è´å¶æ–¯åˆ†ç±»å™¨ã€‚ç„¶åï¼Œå—åˆ°æ—©æœŸéè¯­ä¹‰åˆ†ç»„å·¥ä½œ\[13,36\] çš„å¯å‘ï¼ŒåŸºäºæ©ç åˆ†ç±»çš„æ–¹æ³•å¼€å§‹æµè¡Œï¼Œåœ¨ PASCAL VOC æŒ‘æˆ˜ä¸­è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½\[18\]ã€‚O2P \[5\] å’Œ CFM \[16\] ç­‰æ–¹æ³•é€šè¿‡å¯¹æ©æ¨¡å»ºè®®è¿›è¡Œåˆ†ç±» \[6ï¼Œ 40ï¼Œ 2\] å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚2015 å¹´ï¼ŒFCN \[30\] å°†æ¯åƒç´ åˆ†ç±»çš„æ€æƒ³æ‰©å±•åˆ°æ·±åº¦ç½‘ç»œï¼Œæ˜¾è‘—ä¼˜äº mIoUï¼ˆä¸€ç§ç‰¹åˆ«é€‚åˆåˆ†å‰²çš„æ¯åƒç´ åˆ†ç±»å…¬å¼ï¼‰çš„æ‰€æœ‰å…ˆå‰æ–¹æ³•ã€‚

Per-pixel classification became the dominant way for _deep-net-based_ semantic segmentation since the seminal work of Fully Convolutional Networks (FCNs)Â \[[30](#bib.bib30)\]. Modern semantic segmentation models focus on aggregating long-range context in the final feature map: ASPPÂ \[[7](#bib.bib7), [8](#bib.bib8)\] uses atrous convolutions with different atrous rates; PPMÂ \[[52](#bib.bib52)\] uses pooling operators with different kernel sizes; DANetÂ \[[19](#bib.bib19)\], OCNetÂ \[[51](#bib.bib51)\], and CCNetÂ \[[23](#bib.bib23)\] use different variants of non-local blocksÂ \[[43](#bib.bib43)\]. Recently, SETRÂ \[[53](#bib.bib53)\] and SegmenterÂ \[[37](#bib.bib37)\] replace traditional convolutional backbones with Vision Transformers (ViT)Â \[[17](#bib.bib17)\] that capture long-range context starting from the very first layer. However, these concurrent Transformer-basedÂ \[[41](#bib.bib41)\] semantic segmentation approaches still use a per-pixel classification formulation. Note, that our MaskFormer module can convert any per-pixel classification model to the mask classification setting, allowing seamless adoption of advances in per-pixel classification.  
è‡ªå…¨å·ç§¯ç½‘ç»œï¼ˆFCNï¼‰çš„å¼€åˆ›æ€§å·¥ä½œä»¥æ¥ï¼Œæ¯åƒç´ åˆ†ç±»æˆä¸ºåŸºäºæ·±åº¦ç½‘ç»œçš„è¯­ä¹‰åˆ†å‰²çš„ä¸»è¦æ–¹å¼\[30\]ã€‚ç°ä»£è¯­ä¹‰åˆ†å‰²æ¨¡å‹ä¾§é‡äºåœ¨æœ€ç»ˆç‰¹å¾å›¾ä¸­èšåˆé•¿ç¨‹ä¸Šä¸‹æ–‡ï¼šASPP \[7ï¼Œ 8\] ä½¿ç”¨å…·æœ‰ä¸åŒç‰¹å¾ç‡çš„å¼¹æ€§å·ç§¯;PPM \[52\] ä½¿ç”¨å…·æœ‰ä¸åŒå†…æ ¸å¤§å°çš„æ± åŒ–è¿ç®—ç¬¦;DANet \[19\]ã€OCNet \[51\] å’Œ CCNet \[23\] ä½¿ç”¨éæœ¬åœ°å—çš„ä¸åŒå˜ä½“ \[43\]ã€‚æœ€è¿‘ï¼ŒSETR \[53\] å’Œ Segmenter \[37\] ç”¨ Vision Transformer ï¼ˆViTï¼‰ \[17\] å–ä»£äº†ä¼ ç»Ÿçš„å·ç§¯ä¸»å¹²ï¼Œä»ç¬¬ä¸€å±‚å¼€å§‹æ•è·è¿œç¨‹ä¸Šä¸‹æ–‡ã€‚ç„¶è€Œï¼Œè¿™äº›åŸºäº Transformer çš„å¹¶å‘\[41\] è¯­ä¹‰åˆ†å‰²æ–¹æ³•ä»ç„¶ä½¿ç”¨æ¯åƒç´ åˆ†ç±»å…¬å¼ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬çš„ MaskFormer æ¨¡å—å¯ä»¥å°†ä»»ä½•æ¯åƒç´ åˆ†ç±»æ¨¡å‹è½¬æ¢ä¸ºæ©ç åˆ†ç±»è®¾ç½®ï¼Œä»è€Œæ— ç¼é‡‡ç”¨æ¯åƒç´ åˆ†ç±»çš„è¿›æ­¥ã€‚

Mask classification is commonly used for instance-level segmentation tasksÂ \[[20](#bib.bib20), [24](#bib.bib24)\]. These tasks require a dynamic number of predictions, making application of per-pixel classification challenging as it assumes a static number of outputs. Omnipresent Mask R-CNNÂ \[[21](#bib.bib21)\] uses a global classifier to classify mask proposals for instance segmentation. DETRÂ \[[4](#bib.bib4)\] further incorporates a TransformerÂ \[[41](#bib.bib41)\] design to handle thing and stuff segmentation simultaneously for panoptic segmentationÂ \[[24](#bib.bib24)\]. However, these mask classification methods require predictions of bounding boxes, which may limit their usage in semantic segmentation. The recently proposed Max-DeepLabÂ \[[42](#bib.bib42)\] removes the dependence on box predictions for panoptic segmentation with conditional convolutionsÂ \[[39](#bib.bib39), [44](#bib.bib44)\]. However, in addition to the main mask classification losses it requires multiple auxiliary losses (_i.e_., instance discrimination loss, mask-ID cross entropy loss, and the standard per-pixel classification loss).  
æ©ç åˆ†ç±»é€šå¸¸ç”¨äºå®ä¾‹çº§åˆ†å‰²ä»»åŠ¡\[20,24\]ã€‚è¿™äº›ä»»åŠ¡éœ€è¦åŠ¨æ€æ•°é‡çš„é¢„æµ‹ï¼Œè¿™ä½¿å¾—æ¯åƒç´ åˆ†ç±»çš„åº”ç”¨å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒå‡å®šäº†é™æ€æ•°é‡çš„è¾“å‡ºã€‚Omnipresent Mask R-CNN \[21\] ä½¿ç”¨å…¨å±€åˆ†ç±»å™¨å¯¹æ©ç å»ºè®®è¿›è¡Œåˆ†ç±»ï¼Œä»¥ä¾¿è¿›è¡Œå®ä¾‹åˆ†å‰²ã€‚DETR \[4\] è¿›ä¸€æ­¥é‡‡ç”¨äº† Transformer \[41\] è®¾è®¡ï¼Œä»¥åŒæ—¶å¤„ç†ç‰©å’Œç‰©çš„åˆ†å‰²ï¼Œä»¥å®ç°å…¨æ™¯åˆ†å‰² \[24\]ã€‚ä½†æ˜¯ï¼Œè¿™äº›æ©ç åˆ†ç±»æ–¹æ³•éœ€è¦å¯¹è¾¹ç•Œæ¡†è¿›è¡Œé¢„æµ‹ï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶å®ƒä»¬åœ¨è¯­ä¹‰åˆ†å‰²ä¸­çš„ä½¿ç”¨ã€‚æœ€è¿‘æå‡ºçš„ Max-DeepLab\[42\] æ¶ˆé™¤äº†å¯¹æ¡ä»¶å·ç§¯å…¨æ™¯åˆ†å‰²çš„ç®±å¼é¢„æµ‹çš„ä¾èµ–\[39,44\]ã€‚ä½†æ˜¯ï¼Œé™¤äº†ä¸»è¦çš„æ©ç åˆ†ç±»æŸå¤±å¤–ï¼Œå®ƒè¿˜éœ€è¦å¤šä¸ªè¾…åŠ©æŸå¤±ï¼ˆå³å®ä¾‹è¾¨åˆ«æŸå¤±ã€æ©ç  ID äº¤å‰ç†µæŸå¤±å’Œæ ‡å‡†æ¯åƒç´ åˆ†ç±»æŸå¤±ï¼‰ã€‚

## 3 From Per-Pixel to Mask Classification  

In this section, we first describe how semantic segmentation can be formulated as either a per-pixel classification or a mask classification problem. Then, we introduce our instantiation of the mask classification model with the help of a Transformer decoderÂ \[[41](#bib.bib41)\]. Finally, we describe simple inference strategies to transform mask classification outputs into task-dependent prediction formats.  
åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»‹ç»å¦‚ä½•å°†è¯­ä¹‰åˆ†å‰²è¡¨è¿°ä¸ºæ¯åƒç´ åˆ†ç±»æˆ–æ©ç åˆ†ç±»é—®é¢˜ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»‹ç»äº†åœ¨ Transformer è§£ç å™¨\[41\] çš„å¸®åŠ©ä¸‹å¯¹æ©ç åˆ†ç±»æ¨¡å‹çš„å®ä¾‹åŒ–ã€‚æœ€åï¼Œæˆ‘ä»¬æè¿°äº†å°†æ©ç åˆ†ç±»è¾“å‡ºè½¬æ¢ä¸ºä¸ä»»åŠ¡ç›¸å…³çš„é¢„æµ‹æ ¼å¼çš„ç®€å•æ¨ç†ç­–ç•¥ã€‚

### 3.1 Per-pixel classification formulation  

For per-pixel classification, a segmentation model aims to predict the probability distribution over all possible Kğ¾K categories for every pixel of an HÃ—Wğ»ğ‘ŠH\\times W image: y={pi|piâˆˆÎ”K}i=1Hâ‹…Wğ‘¦superscriptsubscriptconditional-setsubscriptğ‘ğ‘–subscriptğ‘ğ‘–superscriptÎ”ğ¾ğ‘–1â‹…ğ»ğ‘Šy=\\{p_{i}|p_{i}\\in\\Delta^{K}\\}_{i=1}^{H\\cdot W}. Here Î”KsuperscriptÎ”ğ¾\\Delta^{K} is the Kğ¾K-dimensional probability simplex. Training a per-pixel classification model is straight-forward: given ground truth category labels ygt={yigt|yigtâˆˆ{1,â€¦,K}}i=1Hâ‹…Wsuperscriptğ‘¦gtsuperscriptsubscriptconditional-setsuperscriptsubscriptğ‘¦ğ‘–gtsuperscriptsubscriptğ‘¦ğ‘–gt1â€¦ğ¾ğ‘–1â‹…ğ»ğ‘Šy^{\\text{gt}}=\\{y_{i}^{\\text{gt}}|y_{i}^{\\text{gt}}\\in\\{1,\\dots,K\\}\\}_{i=1}^{H\\cdot W} for every pixel, a per-pixel cross-entropy (negative log-likelihood) loss is usually applied, _i.e_., â„’pixel-clsâ€‹(y,ygt)=âˆ‘i=1Hâ‹…Wâˆ’logâ¡piâ€‹(yigt)subscriptâ„’pixel-clsğ‘¦superscriptğ‘¦gtsuperscriptsubscriptğ‘–1â‹…ğ»ğ‘Šsubscriptğ‘ğ‘–superscriptsubscriptğ‘¦ğ‘–gt\\mathcal{L}_{\\text{pixel-cls}}(y,y^{\\text{gt}})=\\sum\\nolimits_{i=1}^{H\\cdot W}-\\log p_{i}(y_{i}^{\\text{gt}}).  
å¯¹äºæ¯åƒç´ åˆ†ç±»ï¼Œåˆ†å‰²æ¨¡å‹æ—¨åœ¨é¢„æµ‹ HÃ—Wğ»ğ‘ŠH\\times W å›¾åƒä¸­æ¯ä¸ªåƒç´ åœ¨æ‰€æœ‰å¯èƒ½ Kğ¾K ç±»åˆ«ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒï¼š y={pi|piâˆˆÎ”K}i=1Hâ‹…Wğ‘¦superscriptsubscriptconditional-setsubscriptğ‘ğ‘–subscriptğ‘ğ‘–superscriptÎ”ğ¾ğ‘–1â‹…ğ»ğ‘Šy=\\{p_{i}|p_{i}\\in\\Delta^{K}\\}_{i=1}^{H\\cdot W} ã€‚è¿™æ˜¯ Î”KsuperscriptÎ”ğ¾\\Delta^{K} Kğ¾K - ç»´æ¦‚ç‡å•çº¯å½¢ã€‚è®­ç»ƒæ¯åƒç´ åˆ†ç±»æ¨¡å‹å¾ˆç®€å•ï¼šç»™å®šæ¯ä¸ªåƒç´ çš„ ygt={yigt|yigtâˆˆ{1,â€¦,K}}i=1Hâ‹…Wsuperscriptğ‘¦gtsuperscriptsubscriptconditional-setsuperscriptsubscriptğ‘¦ğ‘–gtsuperscriptsubscriptğ‘¦ğ‘–gt1â€¦ğ¾ğ‘–1â‹…ğ»ğ‘Šy^{\\text{gt}}=\\{y_{i}^{\\text{gt}}|y_{i}^{\\text{gt}}\\in\\{1,\\dots,K\\}\\}_{i=1}^{H\\cdot W} çœŸå€¼ç±»åˆ«æ ‡ç­¾ï¼Œé€šå¸¸ä¼šåº”ç”¨æ¯åƒç´ äº¤å‰ç†µï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰æŸå¤±ï¼Œå³ â„’pixel-clsâ€‹(y,ygt)=âˆ‘i=1Hâ‹…Wâˆ’logâ¡piâ€‹(yigt)subscriptâ„’pixel-clsğ‘¦superscriptğ‘¦gtsuperscriptsubscriptğ‘–1â‹…ğ»ğ‘Šsubscriptğ‘ğ‘–superscriptsubscriptğ‘¦ğ‘–gt\\mathcal{L}_{\\text{pixel-cls}}(y,y^{\\text{gt}})=\\sum\\nolimits_{i=1}^{H\\cdot W}-\\log p_{i}(y_{i}^{\\text{gt}}) ã€‚

### 3.2 Mask classification formulation  

Mask classification splits the segmentation task into 1) partitioning/grouping the image into Nğ‘N regions (Nğ‘N does not need to equal Kğ¾K), represented with binary masks {mi|miâˆˆ\[0,1\]HÃ—W}i=1Nsuperscriptsubscriptconditional-setsubscriptğ‘šğ‘–subscriptğ‘šğ‘–superscript01ğ»ğ‘Šğ‘–1ğ‘\\{m_{i}|m_{i}\\in\[0,1\]^{H\\times W}\\}_{i=1}^{N}; and 2) associating each region as a whole with some distribution over Kğ¾K categories. To jointly group and classify a segment, _i.e_., to perform mask classification, we define the desired output zğ‘§z as a set of Nğ‘N probability-mask pairs, _i.e_., z={(pi,mi)}i=1N.ğ‘§superscriptsubscriptsubscriptğ‘ğ‘–subscriptğ‘šğ‘–ğ‘–1ğ‘z=\\{(p_{i},m_{i})\\}_{i=1}^{N}. In contrast to per-pixel class probability prediction, for mask classification the probability distribution piâˆˆÎ”K+1subscriptğ‘ğ‘–superscriptÎ”ğ¾1p_{i}\\in\\Delta^{K+1} contains an auxiliary â€œno objectâ€ label (âˆ…\\varnothing) in addition to the Kğ¾K category labels. The âˆ…\\varnothing label is predicted for masks that do not correspond to any of the Kğ¾K categories. Note, mask classification allows multiple mask predictions with the same associated class, making it applicable to both semantic- and instance-level segmentation tasks.  
æ©ç åˆ†ç±»å°†åˆ†å‰²ä»»åŠ¡æ‹†åˆ†ä¸º 1ï¼‰ å°†å›¾åƒåˆ’åˆ†/åˆ†ç»„ä¸º Nğ‘N åŒºåŸŸï¼ˆ Nğ‘N ä¸éœ€è¦ç›¸ç­‰ Kğ¾K ï¼‰ï¼Œç”¨äºŒè¿›åˆ¶æ©ç  {mi|miâˆˆ\[0,1\]HÃ—W}i=1Nsuperscriptsubscriptconditional-setsubscriptğ‘šğ‘–subscriptğ‘šğ‘–superscript01ğ»ğ‘Šğ‘–1ğ‘\\{m_{i}|m_{i}\\in\[0,1\]^{H\\times W}\\}_{i=1}^{N} è¡¨ç¤º;2ï¼‰å°†æ¯ä¸ªåŒºåŸŸä½œä¸ºä¸€ä¸ªæ•´ä½“ä¸ç±»åˆ«çš„ Kğ¾K æŸç§åˆ†å¸ƒç›¸å…³è”ã€‚ä¸ºäº†å¯¹ä¸€ä¸ªç‰‡æ®µè¿›è¡Œè”åˆåˆ†ç»„å’Œåˆ†ç±»ï¼Œå³æ‰§è¡Œæ©ç åˆ†ç±»ï¼Œæˆ‘ä»¬å°†æ‰€éœ€çš„è¾“å‡º zğ‘§z å®šä¹‰ä¸ºä¸€ç»„ Nğ‘N æ¦‚ç‡æ©ç å¯¹ï¼Œå³ï¼Œ z={(pi,mi)}i=1N.ğ‘§superscriptsubscriptsubscriptğ‘ğ‘–subscriptğ‘šğ‘–ğ‘–1ğ‘z=\\{(p_{i},m_{i})\\}_{i=1}^{N}. ä¸æ¯åƒç´ ç±»æ¦‚ç‡é¢„æµ‹ç›¸åï¼Œå¯¹äºæ©ç åˆ†ç±»ï¼Œæ¦‚ç‡åˆ†å¸ƒ piâˆˆÎ”K+1subscriptğ‘ğ‘–superscriptÎ”ğ¾1p_{i}\\in\\Delta^{K+1} é™¤äº† Kğ¾K ç±»åˆ«æ ‡ç­¾å¤–ï¼Œè¿˜åŒ…å«ä¸€ä¸ªè¾…åŠ©çš„â€œæ— å¯¹è±¡â€æ ‡ç­¾ï¼ˆ âˆ…\\varnothing ï¼‰ã€‚å¯¹äºä¸ä»»ä½• Kğ¾K ç±»åˆ«ä¸å¯¹åº”çš„å£ç½©ï¼Œé¢„æµ‹æ ‡ç­¾ âˆ…\\varnothing ã€‚è¯·æ³¨æ„ï¼Œæ©ç åˆ†ç±»å…è®¸ä½¿ç”¨ç›¸åŒçš„å…³è”ç±»è¿›è¡Œå¤šä¸ªæ©ç é¢„æµ‹ï¼Œä½¿å…¶é€‚ç”¨äºè¯­ä¹‰çº§å’Œå®ä¾‹çº§åˆ†æ®µä»»åŠ¡ã€‚

To train a mask classification model, a matching Ïƒğœ\\sigma between the set of predictions zğ‘§z and the set of Ngtsuperscriptğ‘gtN^{\\text{gt}} ground truth segments zgt={(cigt,migt)|cigtâˆˆ{1,â€¦,K},migtâˆˆ{0,1}HÃ—W}i=1Ngtsuperscriptğ‘§gtsuperscriptsubscriptconditional-setsuperscriptsubscriptğ‘ğ‘–gtsuperscriptsubscriptğ‘šğ‘–gtformulae-sequencesuperscriptsubscriptğ‘ğ‘–gt1â€¦ğ¾superscriptsubscriptğ‘šğ‘–gtsuperscript01ğ»ğ‘Šğ‘–1superscriptğ‘gtz^{\\text{gt}}=\\{(c_{i}^{\\text{gt}},m_{i}^{\\text{gt}})|c_{i}^{\\text{gt}}\\in\\{1,\\dots,K\\},m_{i}^{\\text{gt}}\\in\\{0,1\\}^{H\\times W}\\}_{i=1}^{N^{\\text{gt}}} is required.222Different mask classification methods utilize various matching rules. For instance, Mask R-CNNÂ \[[21](#bib.bib21)\] uses a heuristic procedure based on anchor boxes and DETRÂ \[[4](#bib.bib4)\] optimizes a bipartite matching between zğ‘§z and zgtsuperscriptğ‘§gtz^{\\text{gt}}.Â  Here cigtsuperscriptsubscriptğ‘ğ‘–gtc_{i}^{\\text{gt}} is the ground truth class of the ithsuperscriptğ‘–thi^{\\text{th}} ground truth segment. Since the size of prediction set |z|=Nğ‘§ğ‘|z|=N and ground truth set |zgt|=Ngtsuperscriptğ‘§gtsuperscriptğ‘gt|z^{\\text{gt}}|=N^{\\text{gt}} generally differ, we assume Nâ‰¥Ngtğ‘superscriptğ‘gtN\\geq N^{\\text{gt}} and pad the set of ground truth labels with â€œno objectâ€ tokens âˆ…\\varnothing to allow one-to-one matching.  
è‹¥è¦è®­ç»ƒæ©ç åˆ†ç±»æ¨¡å‹ï¼Œéœ€è¦åœ¨é¢„æµ‹é›† zğ‘§z å’Œ Ngtsuperscriptğ‘gtN^{\\text{gt}} çœŸå€¼æ®µ zgt={(cigt,migt)|cigtâˆˆ{1,â€¦,K},migtâˆˆ{0,1}HÃ—W}i=1Ngtsuperscriptğ‘§gtsuperscriptsubscriptconditional-setsuperscriptsubscriptğ‘ğ‘–gtsuperscriptsubscriptğ‘šğ‘–gtformulae-sequencesuperscriptsubscriptğ‘ğ‘–gt1â€¦ğ¾superscriptsubscriptğ‘šğ‘–gtsuperscript01ğ»ğ‘Šğ‘–1superscriptğ‘gtz^{\\text{gt}}=\\{(c_{i}^{\\text{gt}},m_{i}^{\\text{gt}})|c_{i}^{\\text{gt}}\\in\\{1,\\dots,K\\},m_{i}^{\\text{gt}}\\in\\{0,1\\}^{H\\times W}\\}_{i=1}^{N^{\\text{gt}}} é›†ä¹‹é—´è¿›è¡ŒåŒ¹é… Ïƒğœ\\sigma ã€‚ 2 è¿™æ˜¯ cigtsuperscriptsubscriptğ‘ğ‘–gtc_{i}^{\\text{gt}} ithsuperscriptğ‘–thi^{\\text{th}} çœŸå€¼æ®µçš„çœŸå€¼ç±»ã€‚ç”±äºé¢„æµ‹é›† |z|=Nğ‘§ğ‘|z|=N å’ŒçœŸå€¼é›† |zgt|=Ngtsuperscriptğ‘§gtsuperscriptğ‘gt|z^{\\text{gt}}|=N^{\\text{gt}} çš„å¤§å°é€šå¸¸ä¸åŒï¼Œå› æ­¤æˆ‘ä»¬å‡è®¾ Nâ‰¥Ngtğ‘superscriptğ‘gtN\\geq N^{\\text{gt}} å¹¶ç”¨â€œæ— å¯¹è±¡â€æ ‡è®°å¡«å……çœŸå€¼æ ‡ç­¾é›† âˆ…\\varnothing ï¼Œä»¥å…è®¸ä¸€å¯¹ä¸€åŒ¹é…ã€‚

For semantic segmentation, a trivial _fixed matching_ is possible if the number of predictions Nğ‘N matches the number of category labels Kğ¾K. In this case, the ithsuperscriptğ‘–thi^{\\text{th}} prediction is matched to a ground truth region with class label iğ‘–i and to âˆ…\\varnothing if a region with class label iğ‘–i is not present in the ground truth. In our experiments, we found that a _bipartite matching_-based assignment demonstrates better results than the fixed matching. Unlike DETRÂ \[[4](#bib.bib4)\] that uses bounding boxes to compute the assignment costs between prediction zisubscriptğ‘§ğ‘–z_{i} and ground truth zjgtsuperscriptsubscriptğ‘§ğ‘—gtz_{j}^{\\text{gt}} for the matching problem, we directly use class and mask predictions, _i.e_., âˆ’piâ€‹(cjgt)+â„’maskâ€‹(mi,mjgt)subscriptğ‘ğ‘–superscriptsubscriptğ‘ğ‘—gtsubscriptâ„’masksubscriptğ‘šğ‘–superscriptsubscriptğ‘šğ‘—gt-p_{i}(c_{j}^{\\text{gt}})+\\mathcal{L}_{\\text{mask}}(m_{i},m_{j}^{\\text{gt}}), where â„’masksubscriptâ„’mask\\mathcal{L}_{\\text{mask}} is a binary mask loss.  
å¯¹äºè¯­ä¹‰åˆ†å‰²ï¼Œå¦‚æœé¢„æµ‹çš„æ•°é‡ä¸ç±»åˆ«æ ‡ç­¾çš„æ•°é‡ Nğ‘N åŒ¹é…ï¼Œåˆ™å¯ä»¥è¿›è¡Œå¾®ä¸è¶³é“çš„ Kğ¾K å›ºå®šåŒ¹é…ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ ithsuperscriptğ‘–thi^{\\text{th}} é¢„æµ‹å°†ä¸å…·æœ‰ç±»æ ‡ç­¾ iğ‘–i çš„çœŸå€¼åŒºåŸŸåŒ¹é…ï¼Œ âˆ…\\varnothing å¹¶ä¸”å¦‚æœçœŸå€¼ä¸­ä¸å­˜åœ¨å…·æœ‰ç±»æ ‡ç­¾ iğ‘–i çš„åŒºåŸŸï¼Œåˆ™ä¸é¢„æµ‹åŒ¹é…ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°åŸºäºäºŒåˆ†åŒ¹é…çš„èµ‹å€¼æ¯”å›ºå®šåŒ¹é…æ˜¾ç¤ºå‡ºæ›´å¥½çš„ç»“æœã€‚ä¸ DETR \[4\] ä¸åŒï¼ŒDETR \[4\] ä½¿ç”¨è¾¹ç•Œæ¡†æ¥è®¡ç®—åŒ¹é…é—®é¢˜çš„é¢„æµ‹ zisubscriptğ‘§ğ‘–z_{i} å’Œåœ°é¢å®å†µ zjgtsuperscriptsubscriptğ‘§ğ‘—gtz_{j}^{\\text{gt}} ä¹‹é—´çš„åˆ†é…æˆæœ¬ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨ç±»å’Œæ©ç é¢„æµ‹ï¼Œå³ âˆ’piâ€‹(cjgt)+â„’maskâ€‹(mi,mjgt)subscriptğ‘ğ‘–superscriptsubscriptğ‘ğ‘—gtsubscriptâ„’masksubscriptğ‘šğ‘–superscriptsubscriptğ‘šğ‘—gt-p_{i}(c_{j}^{\\text{gt}})+\\mathcal{L}_{\\text{mask}}(m_{i},m_{j}^{\\text{gt}}) ï¼Œå…¶ä¸­ â„’masksubscriptâ„’mask\\mathcal{L}_{\\text{mask}} æ˜¯äºŒè¿›åˆ¶æ©ç æŸå¤±ã€‚

To train model parameters, given a matching, the main mask classification loss â„’mask-clssubscriptâ„’mask-cls\\mathcal{L}_{\\text{mask-cls}} is composed of a cross-entropy classification loss and a binary mask loss â„’masksubscriptâ„’mask\\mathcal{L}_{\\text{mask}} for each predicted segment:  
ä¸ºäº†è®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œç»™å®šåŒ¹é…ï¼Œä¸»æ©ç åˆ†ç±»æŸå¤± â„’mask-clssubscriptâ„’mask-cls\\mathcal{L}_{\\text{mask-cls}} ç”±æ¯ä¸ªé¢„æµ‹æ®µçš„äº¤å‰ç†µåˆ†ç±»æŸå¤±å’ŒäºŒå…ƒæ©ç æŸå¤± â„’masksubscriptâ„’mask\\mathcal{L}_{\\text{mask}} ç»„æˆï¼š

|  | â„’mask-clsâ€‹(z,zgt)=âˆ‘j=1N\[âˆ’logâ¡pÏƒâ€‹(j)â€‹(cjgt)+ğŸ™cjgtâ‰ âˆ…â€‹â„’maskâ€‹(mÏƒâ€‹(j),mjgt)\].subscriptâ„’mask-clsğ‘§superscriptğ‘§gtsuperscriptsubscriptğ‘—1ğ‘delimited-\[\]subscriptğ‘ğœğ‘—superscriptsubscriptğ‘ğ‘—gtsubscript1superscriptsubscriptğ‘ğ‘—gtsubscriptâ„’masksubscriptğ‘šğœğ‘—superscriptsubscriptğ‘šğ‘—gt\\mathcal{L}_{\\text{mask-cls}}(z,z^{\\text{gt}})=\\sum\\nolimits_{j=1}^{N}\\left\[-\\log p_{\\sigma(j)}(c_{j}^{\\text{gt}})+\\mathds{1}_{c_{j}^{\\text{gt}}\\neq\\varnothing}\\mathcal{L}_{\\text{mask}}(m_{\\sigma(j)},m_{j}^{\\text{gt}})\\right\]. |  | (1) |

Note, that most existing mask classification models use auxiliary losses (_e.g_., a bounding box lossÂ \[[21](#bib.bib21), [4](#bib.bib4)\] or an instance discrimination lossÂ \[[42](#bib.bib42)\]) in addition to â„’mask-clssubscriptâ„’mask-cls\\mathcal{L}_{\\text{mask-cls}}. In the next section we present a simple mask classification model that allows end-to-end training with â„’mask-clssubscriptâ„’mask-cls\\mathcal{L}_{\\text{mask-cls}} alone.  
è¯·æ³¨æ„ï¼Œå¤§å¤šæ•°ç°æœ‰çš„æ©ç åˆ†ç±»æ¨¡å‹é™¤äº†ä½¿ç”¨è¾…åŠ©æŸå¤±ï¼ˆä¾‹å¦‚ï¼Œè¾¹ç•Œæ¡†æŸå¤± \[21ï¼Œ 4\] æˆ–å®ä¾‹åˆ¤åˆ«æŸå¤± \[42\]ï¼‰ä¹‹å¤– â„’mask-clssubscriptâ„’mask-cls\\mathcal{L}_{\\text{mask-cls}} ï¼Œè¿˜ä½¿ç”¨è¾…åŠ©æŸå¤±ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ä¸€ä¸ªç®€å•çš„æ©ç åˆ†ç±»æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…è®¸ â„’mask-clssubscriptâ„’mask-cls\\mathcal{L}_{\\text{mask-cls}} å•ç‹¬è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚

### 3.3 MaskFormer3.3 è’™ç‰ˆæˆå‹

![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/x2.png)

Figure 2: MaskFormer overview. We use a backbone to extract image features â„±â„±\\mathcal{F}. A pixel decoder gradually upsamples image features to extract per-pixel embeddings â„°pixelsubscriptâ„°pixel\\mathcal{E}_{\\text{pixel}}. A transformer decoder attends to image features and produces Nğ‘N per-segment embeddings ğ’¬ğ’¬\\mathcal{Q}. The embeddings independently generate Nğ‘N class predictions with Nğ‘N corresponding mask embeddings â„°masksubscriptâ„°mask\\mathcal{E}_{\\text{mask}}. Then, the model predicts Nğ‘N possibly overlapping binary mask predictions via a dot product between pixel embeddings â„°pixelsubscriptâ„°pixel\\mathcal{E}_{\\text{pixel}} and mask embeddings â„°masksubscriptâ„°mask\\mathcal{E}_{\\text{mask}} followed by a sigmoid activation. For semantic segmentation task we can get the final prediction by combining Nğ‘N binary masks with their class predictions using a simple matrix multiplication (see SectionÂ [3.4](#S3.SS4 "3.4 Mask-classification inference â€£ 3 From Per-Pixel to Mask Classification â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation")). Note, the dimensions for multiplication â¨‚tensor-product\\bigotimes are shown in gray.  
å›¾ 2ï¼šMaskFormer æ¦‚è¿°ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸»å¹²æ¥æå–å›¾åƒç‰¹å¾ â„±â„±\\mathcal{F} ã€‚åƒç´ è§£ç å™¨é€æ¸å¯¹å›¾åƒç‰¹å¾è¿›è¡Œä¸Šé‡‡æ ·ï¼Œä»¥æå–æ¯ä¸ªåƒç´ çš„åµŒå…¥ â„°pixelsubscriptâ„°pixel\\mathcal{E}_{\\text{pixel}} ã€‚Transformer è§£ç å™¨å¤„ç†å›¾åƒç‰¹å¾å¹¶ç”Ÿæˆ Nğ‘N æ¯ä¸ªæ®µçš„åµŒå…¥ ğ’¬ğ’¬\\mathcal{Q} ã€‚åµŒå…¥ç‹¬ç«‹ç”Ÿæˆ Nğ‘N å…·æœ‰ Nğ‘N ç›¸åº”æ©ç åµŒå…¥çš„ç±»é¢„æµ‹ â„°masksubscriptâ„°mask\\mathcal{E}_{\\text{mask}} ã€‚ç„¶åï¼Œè¯¥æ¨¡å‹é€šè¿‡åƒç´ åµŒå…¥ â„°pixelsubscriptâ„°pixel\\mathcal{E}_{\\text{pixel}} å’Œæ©ç åµŒå…¥ä¹‹é—´çš„ç‚¹ç§¯ â„°masksubscriptâ„°mask\\mathcal{E}_{\\text{mask}} ï¼Œç„¶åè¿›è¡Œ S å½¢æ¿€æ´»ï¼Œé¢„æµ‹ Nğ‘N å¯èƒ½é‡å çš„äºŒå…ƒæ©æ¨¡é¢„æµ‹ã€‚å¯¹äºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ç®€å•çš„çŸ©é˜µä¹˜æ³•å°†äºŒè¿›åˆ¶æ©ç ä¸å…¶ç±»é¢„æµ‹ç›¸ç»“åˆ Nğ‘N æ¥è·å¾—æœ€ç»ˆé¢„æµ‹ï¼ˆå‚è§ç¬¬ 3.4 èŠ‚ï¼‰ã€‚è¯·æ³¨æ„ï¼Œä¹˜æ³• â¨‚tensor-product\\bigotimes çš„ç»´åº¦ä»¥ç°è‰²æ˜¾ç¤ºã€‚

We now introduce MaskFormer, the new mask classification model, which computes Nğ‘N probability-mask pairs z={(pi,mi)}i=1Nğ‘§superscriptsubscriptsubscriptğ‘ğ‘–subscriptğ‘šğ‘–ğ‘–1ğ‘z=\\{(p_{i},m_{i})\\}_{i=1}^{N}. The model contains three modules (see Fig.Â [2](#S3.F2 "Figure 2 â€£ 3.3 MaskFormer â€£ 3 From Per-Pixel to Mask Classification â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation")): 1) a pixel-level module that extracts per-pixel embeddings used to generate binary mask predictions; 2) a transformer module, where a stack of Transformer decoder layersÂ \[[41](#bib.bib41)\] computes Nğ‘N per-segment embeddings; and 3) a segmentation module, which generates predictions {(pi,mi)}i=1Nsuperscriptsubscriptsubscriptğ‘ğ‘–subscriptğ‘šğ‘–ğ‘–1ğ‘\\{(p_{i},m_{i})\\}_{i=1}^{N} from these embeddings. During inference, discussed in Sec.Â [3.4](#S3.SS4 "3.4 Mask-classification inference â€£ 3 From Per-Pixel to Mask Classification â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"), pisubscriptğ‘ğ‘–p_{i} and misubscriptğ‘šğ‘–m_{i} are assembled into the final prediction.

Pixel-level module takes an image of size HÃ—Wğ»ğ‘ŠH\\times W as input. A backbone generates a (typically) low-resolution image feature map â„±âˆˆâ„Câ„±Ã—HSÃ—WSâ„±superscriptâ„subscriptğ¶â„±ğ»ğ‘†ğ‘Šğ‘†\\mathcal{F}\\in\\mathbb{R}^{C_{\\mathcal{F}}\\times\\frac{H}{S}\\times\\frac{W}{S}}, where Câ„±subscriptğ¶â„±C_{\\mathcal{F}} is the number of channels and Sğ‘†S is the stride of the feature map (Câ„±subscriptğ¶â„±C_{\\mathcal{F}} depends on the specific backbone and we use S=32ğ‘†32S=32 in this work). Then, a pixel decoder gradually upsamples the features to generate per-pixel embeddings â„°pixelâˆˆâ„Câ„°Ã—HÃ—Wsubscriptâ„°pixelsuperscriptâ„subscriptğ¶â„°ğ»ğ‘Š\\mathcal{E}_{\\text{pixel}}\\in\\mathbb{R}^{C_{\\mathcal{E}}\\times H\\times W}, where Câ„°subscriptğ¶â„°C_{\\mathcal{E}} is the embedding dimension. Note, that any per-pixel classification-based segmentation model fits the pixel-level module design including recent Transformer-based modelsÂ \[[37](#bib.bib37), [53](#bib.bib53), [29](#bib.bib29)\]. MaskFormer seamlessly converts such a model to mask classification.

Transformer module uses the standard Transformer decoderÂ \[[41](#bib.bib41)\] to compute from image features â„±â„±\\mathcal{F} and Nğ‘N learnable positional embeddings (_i.e_., queries) its output, _i.e_., Nğ‘N per-segment embeddings ğ’¬âˆˆâ„Cğ’¬Ã—Nğ’¬superscriptâ„subscriptğ¶ğ’¬ğ‘\\mathcal{Q}\\in\\mathbb{R}^{C_{\\mathcal{Q}}\\times N} of dimension Cğ’¬subscriptğ¶ğ’¬C_{\\mathcal{Q}} that encode global information about each segment MaskFormer predicts. Similarly toÂ \[[4](#bib.bib4)\], the decoder yields all predictions in parallel.

Segmentation module applies a linear classifier, followed by a softmax activation, on top of the per-segment embeddings ğ’¬ğ’¬\\mathcal{Q} to yield class probability predictions {piâˆˆÎ”K+1}i=1Nsuperscriptsubscriptsubscriptğ‘ğ‘–superscriptÎ”ğ¾1ğ‘–1ğ‘\\{p_{i}\\in\\Delta^{K+1}\\}_{i=1}^{N} for each segment. Note, that the classifier predicts an additional â€œno objectâ€ category (âˆ…\\varnothing) in case the embedding does not correspond to any region. For mask prediction, a Multi-Layer Perceptron (MLP) with 2 hidden layers converts the per-segment embeddings ğ’¬ğ’¬\\mathcal{Q} to Nğ‘N mask embeddings â„°maskâˆˆâ„Câ„°Ã—Nsubscriptâ„°masksuperscriptâ„subscriptğ¶â„°ğ‘\\mathcal{E}_{\\text{mask}}\\in\\mathbb{R}^{C_{\\mathcal{E}}\\times N} of dimension Câ„°subscriptğ¶â„°C_{\\mathcal{E}}. Finally, we obtain each binary mask prediction miâˆˆ\[0,1\]HÃ—Wsubscriptğ‘šğ‘–superscript01ğ»ğ‘Šm_{i}\\in\[0,1\]^{H\\times W} via a dot product between the ithsuperscriptğ‘–thi^{\\text{th}} mask embedding and per-pixel embeddings â„°pixelsubscriptâ„°pixel\\mathcal{E}_{\\text{pixel}} computed by the pixel-level module. The dot product is followed by a sigmoid activation, _i.e_., miâ€‹\[h,w\]=sigmoidâ€‹(â„°maskâ€‹\[:,i\]Tâ‹…â„°pixelâ€‹\[:,h,w\])subscriptğ‘šğ‘–â„ğ‘¤sigmoidâ‹…subscriptâ„°masksuperscript:ğ‘–Tsubscriptâ„°pixel:â„ğ‘¤m_{i}\[h,w\]=\\text{sigmoid}(\\mathcal{E}_{\\text{mask}}\[:,i\]^{\\text{T}}\\cdot\\mathcal{E}_{\\text{pixel}}\[:,h,w\]).  
åˆ†å‰²æ¨¡å—åœ¨æ¯æ®µåµŒå…¥çš„åŸºç¡€ä¸Šåº”ç”¨çº¿æ€§åˆ†ç±»å™¨ï¼Œç„¶åæ¿€æ´» softmaxï¼Œ ğ’¬ğ’¬\\mathcal{Q} ä»¥ç”Ÿæˆæ¯ä¸ªæ®µ {piâˆˆÎ”K+1}i=1Nsuperscriptsubscriptsubscriptğ‘ğ‘–superscriptÎ”ğ¾1ğ‘–1ğ‘\\{p_{i}\\in\\Delta^{K+1}\\}_{i=1}^{N} çš„ç±»æ¦‚ç‡é¢„æµ‹ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœåµŒå…¥ä¸å¯¹åº”äºä»»ä½•åŒºåŸŸï¼Œåˆ†ç±»å™¨ä¼šé¢„æµ‹é¢å¤–çš„â€œæ— å¯¹è±¡â€ç±»åˆ« ï¼ˆ âˆ…\\varnothing ï¼‰ã€‚å¯¹äºæ©ç é¢„æµ‹ï¼Œå…·æœ‰ 2 ä¸ªéšè—å±‚çš„å¤šå±‚æ„ŸçŸ¥å™¨ ï¼ˆMLPï¼‰ å°†æ¯ä¸ªæ®µçš„åµŒå…¥ ğ’¬ğ’¬\\mathcal{Q} è½¬æ¢ä¸ºç»´åº¦ â„°maskâˆˆâ„Câ„°Ã—Nsubscriptâ„°masksuperscriptâ„subscriptğ¶â„°ğ‘\\mathcal{E}_{\\text{mask}}\\in\\mathbb{R}^{C_{\\mathcal{E}}\\times N} çš„ Nğ‘N æ©ç åµŒå…¥ Câ„°subscriptğ¶â„°C_{\\mathcal{E}} ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ ithsuperscriptğ‘–thi^{\\text{th}} æ©ç åµŒå…¥å’Œåƒç´ çº§æ¨¡å— â„°pixelsubscriptâ„°pixel\\mathcal{E}_{\\text{pixel}} è®¡ç®—çš„æ¯åƒç´ åµŒå…¥ä¹‹é—´çš„ç‚¹ç§¯è·å¾—æ¯ä¸ªäºŒè¿›åˆ¶æ©ç é¢„æµ‹ miâˆˆ\[0,1\]HÃ—Wsubscriptğ‘šğ‘–superscript01ğ»ğ‘Šm_{i}\\in\[0,1\]^{H\\times W} ã€‚ç‚¹ç§¯ä¹‹åæ˜¯ S å½¢æ¿€æ´»ï¼Œå³ miâ€‹\[h,w\]=sigmoidâ€‹(â„°maskâ€‹\[:,i\]Tâ‹…â„°pixelâ€‹\[:,h,w\])subscriptğ‘šğ‘–â„ğ‘¤sigmoidâ‹…subscriptâ„°masksuperscript:ğ‘–Tsubscriptâ„°pixel:â„ğ‘¤m_{i}\[h,w\]=\\text{sigmoid}(\\mathcal{E}_{\\text{mask}}\[:,i\]^{\\text{T}}\\cdot\\mathcal{E}_{\\text{pixel}}\[:,h,w\]) .

Note, we empirically find it is beneficial to _not_ enforce mask predictions to be mutually exclusive to each other by using a softmax activation. During training, the â„’mask-clssubscriptâ„’mask-cls\\mathcal{L}_{\\text{mask-cls}} loss combines a cross entropy classification loss and a binary mask loss â„’masksubscriptâ„’mask\\mathcal{L}_{\\text{mask}} for each predicted segment. For simplicity we use the same â„’masksubscriptâ„’mask\\mathcal{L}_{\\text{mask}} as DETRÂ \[[4](#bib.bib4)\], _i.e_., a linear combination of a focal lossÂ \[[27](#bib.bib27)\] and a dice lossÂ \[[33](#bib.bib33)\] multiplied by hyper-parameters Î»focalsubscriptğœ†focal\\lambda_{\\text{focal}} and Î»dicesubscriptğœ†dice\\lambda_{\\text{dice}} respectively.  
è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ ¹æ®ç»éªŒå‘ç°ï¼Œé€šè¿‡ä½¿ç”¨ softmax æ¿€æ´»ï¼Œä¸å¼ºåˆ¶å°†æ©ç é¢„æµ‹å¼ºåˆ¶ä¸ºç›¸äº’æ’æ–¥æ˜¯æœ‰ç›Šçš„ã€‚åœ¨è®­ç»ƒæœŸé—´ï¼Œ â„’mask-clssubscriptâ„’mask-cls\\mathcal{L}_{\\text{mask-cls}} æŸå¤±ç»“åˆäº†æ¯ä¸ªé¢„æµ‹æ®µçš„äº¤å‰ç†µåˆ†ç±»æŸå¤±å’ŒäºŒå…ƒæ©ç æŸå¤± â„’masksubscriptâ„’mask\\mathcal{L}_{\\text{mask}} ã€‚ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸ DETR \[4\] ç›¸åŒçš„ â„’masksubscriptâ„’mask\\mathcal{L}_{\\text{mask}} æ–¹æ³•ï¼Œå³ç„¦ç‚¹æŸå¤± \[27\] å’Œéª°å­æŸå¤± \[33\] åˆ†åˆ«ä¹˜ä»¥è¶…å‚æ•° Î»focalsubscriptğœ†focal\\lambda_{\\text{focal}} å’Œ Î»dicesubscriptğœ†dice\\lambda_{\\text{dice}} çš„çº¿æ€§ç»„åˆã€‚

### 3.4 Mask-classification inference  
3.4 

First, we present a simple _general inference_ procedure that converts mask classification outputs {(pi,mi)}i=1Nsuperscriptsubscriptsubscriptğ‘ğ‘–subscriptğ‘šğ‘–ğ‘–1ğ‘\\{(p_{i},m_{i})\\}_{i=1}^{N} to either panoptic or semantic segmentation output formats. Then, we describe a _semantic inference_ procedure specifically designed for semantic segmentation. We note, that the specific choice of inference strategy largely depends on the evaluation metric rather than the task.  
é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•çš„é€šç”¨æ¨ç†è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹å°†æ©ç åˆ†ç±»è¾“å‡º {(pi,mi)}i=1Nsuperscriptsubscriptsubscriptğ‘ğ‘–subscriptğ‘šğ‘–ğ‘–1ğ‘\\{(p_{i},m_{i})\\}_{i=1}^{N} è½¬æ¢ä¸ºå…¨æ™¯æˆ–è¯­ä¹‰åˆ†å‰²è¾“å‡ºæ ¼å¼ã€‚ç„¶åï¼Œæˆ‘ä»¬æè¿°äº†ä¸€ä¸ªä¸“é—¨ä¸ºè¯­ä¹‰åˆ†å‰²è®¾è®¡çš„è¯­ä¹‰æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œæ¨ç†ç­–ç•¥çš„å…·ä½“é€‰æ‹©å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºè¯„ä¼°æŒ‡æ ‡è€Œä¸æ˜¯ä»»åŠ¡ã€‚

General inference partitions an image into segments by assigning each pixel \[h,w\]â„ğ‘¤\[h,w\] to one of the Nğ‘N predicted probability-mask pairs via argâ€‹maxi:ciâ‰ âˆ…â¡piâ€‹(ci)â‹…miâ€‹\[h,w\]â‹…subscriptargmax:ğ‘–subscriptğ‘ğ‘–subscriptğ‘ğ‘–subscriptğ‘ğ‘–subscriptğ‘šğ‘–â„ğ‘¤\\operatorname*{arg\\,max}_{i:c_{i}\\neq\\varnothing}p_{i}(c_{i})\\cdot m_{i}\[h,w\]. Here cisubscriptğ‘ğ‘–c_{i} is the most likely class label ci=argâ€‹maxcâˆˆ{1,â€¦,K,âˆ…}â¡piâ€‹(c)subscriptğ‘ğ‘–subscriptargmaxğ‘1â€¦ğ¾subscriptğ‘ğ‘–ğ‘c_{i}=\\operatorname*{arg\\,max}_{c\\in\\{1,\\dots,K,\\varnothing\\}}p_{i}(c) for each probability-mask pair iğ‘–i. Intuitively, this procedure assigns a pixel at location \[h,w\]â„ğ‘¤\[h,w\] to probability-mask pair iğ‘–i only if both the _most likely_ class probability piâ€‹(ci)subscriptğ‘ğ‘–subscriptğ‘ğ‘–p_{i}(c_{i}) and the mask prediction probability miâ€‹\[h,w\]subscriptğ‘šğ‘–â„ğ‘¤m_{i}\[h,w\] are high. Pixels assigned to the same probability-mask pair iğ‘–i form a segment where each pixel is labelled with cisubscriptğ‘ğ‘–c_{i}. For semantic segmentation, segments sharing the same category label are merged; whereas for instance-level segmentation tasks, the index iğ‘–i of the probability-mask pair helps to distinguish different instances of the same class. Finally, to reduce false positive rates in panoptic segmentation we follow previous inference strategiesÂ \[[4](#bib.bib4), [24](#bib.bib24)\]. Specifically, we filter out low-confidence predictions prior to inference and remove predicted segments that have large parts of their binary masks (mi>0.5subscriptğ‘šğ‘–0.5m_{i}>0.5) occluded by other predictions.  
ä¸€èˆ¬æ¨ç†é€šè¿‡ å°†æ¯ä¸ªåƒç´  \[h,w\]â„ğ‘¤\[h,w\] åˆ†é…ç»™é¢„æµ‹ argâ€‹maxi:ciâ‰ âˆ…â¡piâ€‹(ci)â‹…miâ€‹\[h,w\]â‹…subscriptargmax:ğ‘–subscriptğ‘ğ‘–subscriptğ‘ğ‘–subscriptğ‘ğ‘–subscriptğ‘šğ‘–â„ğ‘¤\\operatorname*{arg\\,max}_{i:c_{i}\\neq\\varnothing}p_{i}(c_{i})\\cdot m_{i}\[h,w\] çš„æ¦‚ç‡æ©ç å¯¹ä¹‹ä¸€ Nğ‘N ï¼Œå°†å›¾åƒåˆ’åˆ†ä¸ºå¤šä¸ªæ®µã€‚ä»¥ä¸‹æ˜¯ cisubscriptğ‘ğ‘–c_{i} æ¯ä¸ªæ¦‚ç‡æ©ç å¯¹ iğ‘–i æœ€æœ‰å¯èƒ½çš„ç±»æ ‡ç­¾ ci=argâ€‹maxcâˆˆ{1,â€¦,K,âˆ…}â¡piâ€‹(c)subscriptğ‘ğ‘–subscriptargmaxğ‘1â€¦ğ¾subscriptğ‘ğ‘–ğ‘c_{i}=\\operatorname*{arg\\,max}_{c\\in\\{1,\\dots,K,\\varnothing\\}}p_{i}(c) ã€‚ç›´è§‚åœ°è¯´ï¼Œä»…å½“æœ€å¯èƒ½çš„ç±»æ¦‚ç‡ piâ€‹(ci)subscriptğ‘ğ‘–subscriptğ‘ğ‘–p_{i}(c_{i}) å’Œæ©ç é¢„æµ‹æ¦‚ç‡ miâ€‹\[h,w\]subscriptğ‘šğ‘–â„ğ‘¤m_{i}\[h,w\] éƒ½å¾ˆé«˜æ—¶ï¼Œæ­¤è¿‡ç¨‹æ‰ä¼šå°†ä½ç½® \[h,w\]â„ğ‘¤\[h,w\] çš„åƒç´ åˆ†é…ç»™æ¦‚ç‡æ©ç å¯¹ iğ‘–i ã€‚åˆ†é…ç»™åŒä¸€æ¦‚ç‡æ©ç å¯¹ iğ‘–i çš„åƒç´ å½¢æˆä¸€ä¸ªçº¿æ®µï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ éƒ½ç”¨ cisubscriptğ‘ğ‘–c_{i} æ ‡è®°ã€‚å¯¹äºè¯­ä¹‰åˆ†å‰²ï¼Œå°†åˆå¹¶å…±äº«åŒä¸€ç±»åˆ«æ ‡ç­¾çš„åŒºæ®µ; è€Œå¯¹äºå®ä¾‹çº§åˆ†æ®µä»»åŠ¡ï¼Œæ¦‚ç‡æ©ç å¯¹çš„ç´¢å¼• iğ‘–i æœ‰åŠ©äºåŒºåˆ†åŒä¸€ç±»çš„ä¸åŒå®ä¾‹ã€‚æœ€åï¼Œä¸ºäº†é™ä½å…¨æ™¯åˆ†å‰²ä¸­çš„è¯¯æŠ¥ç‡ï¼Œæˆ‘ä»¬éµå¾ªå…ˆå‰çš„æ¨ç†ç­–ç•¥\[4,24\]ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æ¨ç†ä¹‹å‰è¿‡æ»¤æ‰ä½ç½®ä¿¡åº¦çš„é¢„æµ‹ï¼Œå¹¶åˆ é™¤å…¶äºŒè¿›åˆ¶æ©ç  ï¼ˆ mi>0.5subscriptğ‘šğ‘–0.5m_{i}>0.5 ï¼‰ çš„å¤§éƒ¨åˆ†è¢«å…¶ä»–é¢„æµ‹é®æŒ¡çš„é¢„æµ‹æ®µã€‚

Semantic inference is designed specifically for semantic segmentation and is done via a simple matrix multiplication. We empirically find that marginalization over probability-mask pairs, _i.e_., argâ€‹maxcâˆˆ{1,â€¦,K}â€‹âˆ‘i=1Npiâ€‹(c)â‹…miâ€‹\[h,w\]subscriptargmaxğ‘1â€¦ğ¾superscriptsubscriptğ‘–1ğ‘â‹…subscriptğ‘ğ‘–ğ‘subscriptğ‘šğ‘–â„ğ‘¤\\operatorname*{arg\\,max}_{c\\in\\{1,\\dots,K\\}}\\sum_{i=1}^{N}p_{i}(c)\\cdot m_{i}\[h,w\], yields better results than the hard assignment of each pixel to a probability-mask pair iğ‘–i used in the general inference strategy. The argmax does not include the â€œno objectâ€ category (âˆ…\\varnothing) as standard semantic segmentation requires each output pixel to take a label. Note, this strategy returns a per-pixel class probability âˆ‘i=1Npiâ€‹(c)â‹…miâ€‹\[h,w\]superscriptsubscriptğ‘–1ğ‘â‹…subscriptğ‘ğ‘–ğ‘subscriptğ‘šğ‘–â„ğ‘¤\\sum_{i=1}^{N}p_{i}(c)\\cdot m_{i}\[h,w\]. However, we observe that directly maximizing per-pixel class likelihood leads to poor performance. We hypothesize, that gradients are evenly distributed to every query, which complicates training.  
è¯­ä¹‰æ¨ç†æ˜¯ä¸“é—¨ä¸ºè¯­ä¹‰åˆ†å‰²è€Œè®¾è®¡çš„ï¼Œé€šè¿‡ç®€å•çš„çŸ©é˜µä¹˜æ³•å®Œæˆã€‚æˆ‘ä»¬æ ¹æ®ç»éªŒå‘ç°ï¼Œå¯¹æ¦‚ç‡æ©ç å¯¹çš„è¾¹ç¼˜åŒ–ï¼Œå³ argâ€‹maxcâˆˆ{1,â€¦,K}â€‹âˆ‘i=1Npiâ€‹(c)â‹…miâ€‹\[h,w\]subscriptargmaxğ‘1â€¦ğ¾superscriptsubscriptğ‘–1ğ‘â‹…subscriptğ‘ğ‘–ğ‘subscriptğ‘šğ‘–â„ğ‘¤\\operatorname*{arg\\,max}_{c\\in\\{1,\\dots,K\\}}\\sum_{i=1}^{N}p_{i}(c)\\cdot m_{i}\[h,w\] ï¼Œæ¯”å°†æ¯ä¸ªåƒç´ ç¡¬åˆ†é…ç»™ä¸€èˆ¬æ¨ç†ç­–ç•¥ä¸­ä½¿ç”¨çš„æ¦‚ç‡æ©ç å¯¹ iğ‘–i äº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚argmax ä¸åŒ…æ‹¬â€œæ— å¯¹è±¡â€ç±»åˆ« ï¼ˆ âˆ…\\varnothing ï¼‰ï¼Œå› ä¸ºæ ‡å‡†è¯­ä¹‰åˆ†å‰²è¦æ±‚æ¯ä¸ªè¾“å‡ºåƒç´ éƒ½é‡‡ç”¨æ ‡ç­¾ã€‚è¯·æ³¨æ„ï¼Œæ­¤ç­–ç•¥è¿”å›æ¯ä¸ªåƒç´ ç±»çš„æ¦‚ç‡ âˆ‘i=1Npiâ€‹(c)â‹…miâ€‹\[h,w\]superscriptsubscriptğ‘–1ğ‘â‹…subscriptğ‘ğ‘–ğ‘subscriptğ‘šğ‘–â„ğ‘¤\\sum_{i=1}^{N}p_{i}(c)\\cdot m_{i}\[h,w\] ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œç›´æ¥æœ€å¤§åŒ–æ¯ä¸ªåƒç´ ç±»çš„å¯èƒ½æ€§ä¼šå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚æˆ‘ä»¬å‡è®¾ï¼Œæ¢¯åº¦å‡åŒ€åœ°åˆ†å¸ƒåˆ°æ¯ä¸ªæŸ¥è¯¢ï¼Œè¿™ä½¿å¾—è®­ç»ƒå˜å¾—å¤æ‚ã€‚

## 4 Experiments å®éªŒ

We demonstrate that MaskFormer seamlessly unifies semantic- and instance-level segmentation tasks by showing state-of-the-art results on both semantic segmentation and panoptic segmentation datasets. Then, we ablate the MaskFormer design confirming that observed improvements in semantic segmentation indeed stem from the shift from per-pixel classification to mask classification.  
æˆ‘ä»¬è¯æ˜äº† MaskFormer é€šè¿‡åœ¨è¯­ä¹‰åˆ†å‰²å’Œå…¨æ™¯åˆ†å‰²æ•°æ®é›†ä¸Šæ˜¾ç¤ºæœ€å…ˆè¿›çš„ç»“æœï¼Œæ— ç¼ç»Ÿä¸€äº†è¯­ä¹‰çº§å’Œå®ä¾‹çº§åˆ†å‰²ä»»åŠ¡ã€‚ç„¶åï¼Œæˆ‘ä»¬æ¶ˆèäº† MaskFormer è®¾è®¡ï¼Œç¡®è®¤è§‚å¯Ÿåˆ°çš„è¯­ä¹‰åˆ†å‰²æ”¹è¿›ç¡®å®æºäºä»æ¯åƒç´ åˆ†ç±»åˆ°æ©ç åˆ†ç±»çš„è½¬å˜ã€‚

Datasets. We study MaskFormer using four widely used semantic segmentation datasets: ADE20KÂ \[[55](#bib.bib55)\] (150 classes) from the SceneParse150 challengeÂ \[[54](#bib.bib54)\], COCO-Stuff-10KÂ \[[3](#bib.bib3)\] (171 classes), CityscapesÂ \[[15](#bib.bib15)\] (19 classes), and Mapillary VistasÂ \[[34](#bib.bib34)\] (65 classes). In addition, we use the ADE20K-FullÂ \[[55](#bib.bib55)\] dataset annotated in an open vocabulary setting (we keep 874 classes that are present in both train and validation sets). For panotic segmenation evaluation we use COCOÂ \[[28](#bib.bib28), [3](#bib.bib3), [24](#bib.bib24)\] (80 â€œthingsâ€ and 53 â€œstuffâ€ categories) and ADE20K-PanopticÂ \[[55](#bib.bib55), [24](#bib.bib24)\] (100 â€œthingsâ€ and 50 â€œstuffâ€ categories). Please see the appendix for detailed descriptions of all used datasets.  
æ•°æ®ã€‚æˆ‘ä»¬ä½¿ç”¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„è¯­ä¹‰åˆ†å‰²æ•°æ®é›†æ¥ç ”ç©¶ MaskFormerï¼šæ¥è‡ª SceneParse150 æŒ‘æˆ˜ \[54\] çš„ ADE20K \[55\]ï¼ˆ150 ä¸ªç±»ï¼‰ã€COCO-Stuff-10K \[3\]ï¼ˆ171 ä¸ªç±»ï¼‰ã€Cityscapes \[15\]ï¼ˆ19 ä¸ªç±»ï¼‰å’Œ Mapillary Vistas \[34\]ï¼ˆ65 ä¸ªç±»ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨åœ¨å¼€æ”¾è¯æ±‡è¡¨è®¾ç½®ä¸­æ³¨é‡Šçš„ ADE20K-Full \[55\] æ•°æ®é›†ï¼ˆæˆ‘ä»¬ä¿ç•™äº†è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­éƒ½å­˜åœ¨çš„ 874 ä¸ªç±»ï¼‰ã€‚å¯¹äºå…¨æ™¯åˆ†ç±»è¯„ä¼°ï¼Œæˆ‘ä»¬ä½¿ç”¨ COCO \[28ï¼Œ 3ï¼Œ 24\]ï¼ˆ80 ä¸ªâ€œäº‹ç‰©â€å’Œ 53 ä¸ªâ€œä¸œè¥¿â€ç±»åˆ«ï¼‰å’Œ ADE20K-Panoptic \[55ï¼Œ 24\]ï¼ˆ100 ä¸ªâ€œäº‹ç‰©â€å’Œ 50 ä¸ªâ€œä¸œè¥¿â€ç±»åˆ«ï¼‰ã€‚æœ‰å…³æ‰€æœ‰å·²ç”¨æ•°æ®é›†çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚é˜…é™„å½•ã€‚

Evaluation metrics. For _semantic segmentation_ the standard metric is mIoU (mean Intersection-over-Union)Â \[[18](#bib.bib18)\], a per-pixel metric that directly corresponds to the per-pixel classification formulation. To better illustrate the difference between segmentation approaches, in our ablations we supplement mIoU with PQStSt{}^{\\text{St}} (PQ stuff)Â \[[24](#bib.bib24)\], a per-region metric that treats all classes as â€œstuffâ€ and evaluates each segment equally, irrespective of its size. We report the median of 3 runs for all datasets, except for Cityscapes where we report the median of 5 runs. For _panoptic segmentation_, we use the standard PQ (panoptic quality) metricÂ \[[24](#bib.bib24)\] and report single run results due to prohibitive training costs.  
è¯„ä¼°æŒ‡æ ‡ã€‚å¯¹äºè¯­ä¹‰åˆ†å‰²ï¼Œæ ‡å‡†æŒ‡æ ‡æ˜¯ mIoUï¼ˆå¹³å‡äº¤é›†å¹¶é›†ï¼‰\[18\]ï¼Œè¿™æ˜¯ä¸€ä¸ªç›´æ¥å¯¹åº”äºæ¯åƒç´ åˆ†ç±»å…¬å¼çš„æ¯åƒç´ æŒ‡æ ‡ã€‚ä¸ºäº†æ›´å¥½åœ°è¯´æ˜åˆ†å‰²æ–¹æ³•ä¹‹é—´çš„å·®å¼‚ï¼Œåœ¨æˆ‘ä»¬çš„æ¶ˆèä¸­ï¼Œæˆ‘ä»¬ç”¨ PQ StSt{}^{\\text{St}} ï¼ˆPQ å†…å®¹ï¼‰\[24\] è¡¥å……äº† mIoUï¼Œè¿™æ˜¯ä¸€ä¸ªæ¯ä¸ªåŒºåŸŸçš„æŒ‡æ ‡ï¼Œå°†æ‰€æœ‰ç±»åˆ«è§†ä¸ºâ€œå†…å®¹â€ï¼Œå¹¶å¹³ç­‰åœ°è¯„ä¼°æ¯ä¸ªç‰‡æ®µï¼Œæ— è®ºå…¶å¤§å°å¦‚ä½•ã€‚æˆ‘ä»¬æŠ¥å‘Šæ‰€æœ‰æ•°æ®é›†çš„ 3 æ¬¡è¿è¡Œä¸­ä½æ•°ï¼Œä½† Cityscapes é™¤å¤–ï¼Œå…¶ä¸­æˆ‘ä»¬æŠ¥å‘Šçš„ä¸­ä½æ•°ä¸º 5 æ¬¡è¿è¡Œã€‚å¯¹äºå…¨æ™¯åˆ†å‰²ï¼Œæˆ‘ä»¬ä½¿ç”¨æ ‡å‡†çš„ PQï¼ˆå…¨æ™¯è´¨é‡ï¼‰æŒ‡æ ‡\[24\]ï¼Œå¹¶æŠ¥å‘Šå•æ¬¡è¿è¡Œç»“æœï¼Œå› ä¸ºè®­ç»ƒæˆæœ¬è¿‡é«˜ã€‚

![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/x3.png)

Baseline models. On the right we sketch the used per-pixel classification baselines. The PerPixelBaseline uses the pixel-level module of MaskFormer and directly outputs per-pixel class scores. For a fair comparison, we design PerPixelBaseline+ which adds the transformer module and mask embedding MLP to the PerPixelBaseline. Thus, PerPixelBaseline+ and MaskFormer differ only in the formulation: per-pixel _vs_. mask classification. Note that these baselines are for ablation and we compare MaskFormer with state-of-the-art per-pixel classification models as well.  
åŸºçº¿æ¨¡å‹ã€‚åœ¨å³è¾¹ï¼Œæˆ‘ä»¬å‹¾å‹’äº†ä½¿ç”¨çš„æ¯åƒç´ åˆ†ç±»åŸºçº¿ã€‚PerPixelBaseline ä½¿ç”¨ MaskFormer çš„åƒç´ çº§æ¨¡å—ï¼Œç›´æ¥è¾“å‡ºæ¯ä¸ªåƒç´ çš„ç±»åˆ†æ•°ã€‚ä¸ºäº†å…¬å¹³åœ°è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è®¾è®¡äº† PerPixelBaseline+ï¼Œå®ƒå°†è½¬æ¢å™¨æ¨¡å—å’Œæ©æ¨¡åµŒå…¥ MLP æ·»åŠ åˆ° PerPixelBaseline ä¸­ã€‚å› æ­¤ï¼ŒPerPixelBaseline+ å’Œ MaskFormer ä»…åœ¨å…¬å¼ä¸Šæœ‰æ‰€ä¸åŒï¼šæ¯åƒç´ ä¸è’™ç‰ˆåˆ†ç±»ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›åŸºçº¿æ˜¯ç”¨äºæ¶ˆèçš„ï¼Œæˆ‘ä»¬å°† MaskFormer ä¸æœ€å…ˆè¿›çš„æ¯åƒç´ åˆ†ç±»æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚

### 4.1 Implementation details  
4.1 å®ç°ç»†èŠ‚

Backbone. MaskFormer is compatible with any backbone architecture. In our work we use the standard convolution-based ResNetÂ \[[22](#bib.bib22)\] backbones (R50 and R101 with 50 and 101 layers respectively) and recently proposed Transformer-based Swin-TransformerÂ \[[29](#bib.bib29)\] backbones. In addition, we use the R101c modelÂ \[[7](#bib.bib7)\] which replaces the first 7Ã—7777\\times 7 convolution layer of R101 with 3 consecutive 3Ã—3333\\times 3 convolutions and which is popular in the semantic segmentation communityÂ \[[52](#bib.bib52), [8](#bib.bib8), [9](#bib.bib9), [23](#bib.bib23), [50](#bib.bib50), [11](#bib.bib11)\].  
éª¨å¹²ã€‚MaskFormer ä¸ä»»ä½•éª¨å¹²æ¶æ„å…¼å®¹ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†åŸºäºå·ç§¯çš„æ ‡å‡† ResNet \[22\] ä¸»å¹²ï¼ˆR50 å’Œ R101 åˆ†åˆ«æœ‰ 50 å±‚å’Œ 101 å±‚ï¼‰å’Œæœ€è¿‘æå‡ºçš„åŸºäº Transformer çš„ Swin-Transformer \[29\] ä¸»å¹²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ R101c æ¨¡å‹ \[7\]ï¼Œè¯¥æ¨¡å‹å°† R101 çš„ç¬¬ä¸€ä¸ª 7Ã—7777\\times 7 å·ç§¯å±‚æ›¿æ¢ä¸º 3 ä¸ªè¿ç»­ 3Ã—3333\\times 3 å·ç§¯ï¼Œè¯¥æ¨¡å‹åœ¨è¯­ä¹‰åˆ†å‰²ç¤¾åŒºä¸­å¾ˆæµè¡Œ \[52ï¼Œ 8ï¼Œ 9ï¼Œ 23ï¼Œ 50ï¼Œ 11\]ã€‚

Pixel decoder. The pixel decoder in FigureÂ [2](#S3.F2 "Figure 2 â€£ 3.3 MaskFormer â€£ 3 From Per-Pixel to Mask Classification â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation") can be implemented using any semantic segmentation decoder (_e.g_., \[[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)\]). Many per-pixel classification methods use modules like ASPPÂ \[[7](#bib.bib7)\] or PSPÂ \[[52](#bib.bib52)\] to collect and distribute context across locations. The Transformer module attends to all image features, collecting global information to generate class predictions. This setup reduces the need of the per-pixel module for heavy context aggregation. Therefore, for MaskFormer, we design a light-weight pixel decoder based on the popular FPNÂ \[[26](#bib.bib26)\] architecture.  
åƒç´ è§£ç å™¨ã€‚å›¾ 2 ä¸­çš„åƒç´ è§£ç å™¨å¯ä»¥ä½¿ç”¨ä»»ä½•è¯­ä¹‰åˆ†å‰²è§£ç å™¨ï¼ˆä¾‹å¦‚ï¼Œ\[9,10,11\]ï¼‰æ¥å®ç°ã€‚è®¸å¤šæ¯åƒç´ åˆ†ç±»æ–¹æ³•ä½¿ç”¨ ASPP \[7\] æˆ– PSP \[52\] ç­‰æ¨¡å—æ¥æ”¶é›†å’Œåˆ†å‘è·¨ä½ç½®çš„ä¸Šä¸‹æ–‡ã€‚Transformer æ¨¡å—å…³æ³¨æ‰€æœ‰å›¾åƒç‰¹å¾ï¼Œæ”¶é›†å…¨å±€ä¿¡æ¯ä»¥ç”Ÿæˆç±»é¢„æµ‹ã€‚æ­¤è®¾ç½®å‡å°‘äº†å¯¹æ¯åƒç´ æ¨¡å—è¿›è¡Œå¤§é‡ä¸Šä¸‹æ–‡èšåˆçš„éœ€æ±‚ã€‚å› æ­¤ï¼Œå¯¹äº MaskFormerï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºæµè¡Œçš„ FPN \[26\] æ¶æ„çš„è½»é‡çº§åƒç´ è§£ç å™¨ã€‚

Following FPN, we 2Ã—2\\times upsample the low-resolution feature map in the decoder and sum it with the projected feature map of corresponding resolution from the backbone; Projection is done to match channel dimensions of the feature maps with a 1Ã—1111\\times 1 convolution layer followed by GroupNorm (GN)Â \[[45](#bib.bib45)\]. Next, we fuse the summed features with an additional 3Ã—3333\\times 3 convolution layer followed by GN and ReLU activation. We repeat this process starting with the stride 32 feature map until we obtain a final feature map of stride 4. Finally, we apply a single 1Ã—1111\\times 1 convolution layer to get the per-pixel embeddings. All feature maps in the pixel decoder have a dimension of 256 channels.  
åœ¨ FPN ä¹‹åï¼Œæˆ‘ä»¬å¯¹ 2Ã—2\\times è§£ç å™¨ä¸­çš„ä½åˆ†è¾¨ç‡ç‰¹å¾å›¾è¿›è¡Œä¸Šé‡‡æ ·ï¼Œå¹¶å°†å…¶ä¸æ¥è‡ªä¸»å¹²çš„ç›¸åº”åˆ†è¾¨ç‡çš„æŠ•å½±ç‰¹å¾å›¾æ±‚å’Œ; è¿›è¡ŒæŠ•å½±æ˜¯ä¸ºäº†å°†ç‰¹å¾å›¾çš„é€šé“ç»´åº¦ä¸ 1Ã—1111\\times 1 å·ç§¯å±‚åŒ¹é…ï¼Œç„¶åæ˜¯ GroupNormï¼ˆGNï¼‰\[45\]ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ±‚å’Œç‰¹å¾ä¸é¢å¤–çš„ 3Ã—3333\\times 3 å·ç§¯å±‚èåˆï¼Œç„¶åè¿›è¡Œ GN å’Œ ReLU æ¿€æ´»ã€‚æˆ‘ä»¬ä»æ­¥å¹… 32 ç‰¹å¾å›¾å¼€å§‹é‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°è·å¾—æ­¥å¹… 4 çš„æœ€ç»ˆç‰¹å¾å›¾ã€‚æœ€åï¼Œæˆ‘ä»¬åº”ç”¨å•ä¸ª 1Ã—1111\\times 1 å·ç§¯å±‚æ¥è·å¾—æ¯ä¸ªåƒç´ çš„åµŒå…¥ã€‚åƒç´ è§£ç å™¨ä¸­çš„æ‰€æœ‰ç‰¹å¾å›¾çš„å°ºå¯¸ä¸º 256 ä¸ªé€šé“ã€‚

Transformer decoder. We use the same Transformer decoder design as DETRÂ \[[4](#bib.bib4)\]. The Nğ‘N query embeddings are initialized as zero vectors, and we associate each query with a learnable positional encoding. We use 6 Transformer decoder layers with 100 queries by default, and, following DETR, we apply the same loss after each decoder. In our experiments we observe that MaskFormer is competitive for semantic segmentation with a single decoder layer too, whereas for instance-level segmentation multiple layers are necessary to remove duplicates from the final predictions.  
å˜å‹å™¨è§£ç å™¨ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸ DETR \[4\] ç›¸åŒçš„ Transformer è§£ç å™¨è®¾è®¡ã€‚ Nğ‘N æŸ¥è¯¢åµŒå…¥åˆå§‹åŒ–ä¸ºé›¶å‘é‡ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªæŸ¥è¯¢ä¸å¯å­¦ä¹ çš„ä½ç½®ç¼–ç ç›¸å…³è”ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ 6 ä¸ª Transformer è§£ç å™¨å±‚å’Œ 100 ä¸ªæŸ¥è¯¢ï¼Œå¹¶ä¸”åœ¨ DETR ä¹‹åï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªè§£ç å™¨ååº”ç”¨ç›¸åŒçš„æŸå¤±ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ° MaskFormer åœ¨ä½¿ç”¨å•ä¸ªè§£ç å™¨å±‚è¿›è¡Œè¯­ä¹‰åˆ†å‰²æ–¹é¢ä¹Ÿå…·æœ‰ç«äº‰åŠ›ï¼Œè€Œå¯¹äºå®ä¾‹çº§åˆ†å‰²ï¼Œéœ€è¦å¤šä¸ªå±‚æ‰èƒ½ä»æœ€ç»ˆé¢„æµ‹ä¸­åˆ é™¤é‡å¤é¡¹ã€‚

Segmentation module. The multi-layer perceptron (MLP) in FigureÂ [2](#S3.F2 "Figure 2 â€£ 3.3 MaskFormer â€£ 3 From Per-Pixel to Mask Classification â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation") has 2 hidden layers of 256 channels to predict the mask embeddings â„°masksubscriptâ„°mask\\mathcal{E}_{\\text{mask}}, analogously to the box head in DETR. Both per-pixel â„°pixelsubscriptâ„°pixel\\mathcal{E}_{\\text{pixel}} and mask â„°masksubscriptâ„°mask\\mathcal{E}_{\\text{mask}} embeddings have 256 channels.  
åˆ†æ®µæ¨¡å—ã€‚å›¾ 2 ä¸­çš„å¤šå±‚æ„ŸçŸ¥å™¨ ï¼ˆMLPï¼‰ æœ‰ 2 ä¸ªéšè—å±‚ï¼Œæ¯å±‚ 256 ä¸ªé€šé“ç”¨äºé¢„æµ‹æ©æ¨¡åµŒå…¥ â„°masksubscriptâ„°mask\\mathcal{E}_{\\text{mask}} ï¼Œç±»ä¼¼äº DETR ä¸­çš„ç›’å¤´ã€‚æ¯åƒç´  â„°pixelsubscriptâ„°pixel\\mathcal{E}_{\\text{pixel}} å’Œè’™ç‰ˆ â„°masksubscriptâ„°mask\\mathcal{E}_{\\text{mask}} åµŒå…¥éƒ½æœ‰ 256 ä¸ªé€šé“ã€‚

Loss weights. We use focal lossÂ \[[27](#bib.bib27)\] and dice lossÂ \[[33](#bib.bib33)\] for our mask loss: â„’maskâ€‹(m,mgt)=Î»focalâ€‹â„’focalâ€‹(m,mgt)+Î»diceâ€‹â„’diceâ€‹(m,mgt)subscriptâ„’maskğ‘šsuperscriptğ‘šgtsubscriptğœ†focalsubscriptâ„’focalğ‘šsuperscriptğ‘šgtsubscriptğœ†dicesubscriptâ„’diceğ‘šsuperscriptğ‘šgt\\mathcal{L}_{\\text{mask}}(m,m^{\\text{gt}})=\\lambda_{\\text{focal}}\\mathcal{L}_{\\text{focal}}(m,m^{\\text{gt}})+\\lambda_{\\text{dice}}\\mathcal{L}_{\\text{dice}}(m,m^{\\text{gt}}), and set the hyper-parameters to Î»focal=20.0subscriptğœ†focal20.0\\lambda_{\\text{focal}}=20.0 and Î»dice=1.0subscriptğœ†dice1.0\\lambda_{\\text{dice}}=1.0. Following DETRÂ \[[4](#bib.bib4)\], the weight for the â€œno objectâ€ (âˆ…\\varnothing) in the classification loss is set to 0.1.  
å‡è‚¥é‡é‡ã€‚æˆ‘ä»¬ä½¿ç”¨ç„¦ç‚¹æŸå¤± \[27\] å’Œéª°å­æŸå¤± \[33\] æ¥è¡¨ç¤ºæ©æ¨¡æŸå¤±ï¼š â„’maskâ€‹(m,mgt)=Î»focalâ€‹â„’focalâ€‹(m,mgt)+Î»diceâ€‹â„’diceâ€‹(m,mgt)subscriptâ„’maskğ‘šsuperscriptğ‘šgtsubscriptğœ†focalsubscriptâ„’focalğ‘šsuperscriptğ‘šgtsubscriptğœ†dicesubscriptâ„’diceğ‘šsuperscriptğ‘šgt\\mathcal{L}_{\\text{mask}}(m,m^{\\text{gt}})=\\lambda_{\\text{focal}}\\mathcal{L}_{\\text{focal}}(m,m^{\\text{gt}})+\\lambda_{\\text{dice}}\\mathcal{L}_{\\text{dice}}(m,m^{\\text{gt}}) ï¼Œå¹¶å°†è¶…å‚æ•°è®¾ç½®ä¸º Î»focal=20.0subscriptğœ†focal20.0\\lambda_{\\text{focal}}=20.0 å’Œ Î»dice=1.0subscriptğœ†dice1.0\\lambda_{\\text{dice}}=1.0 ã€‚æ ¹æ® DETR \[4\]ï¼Œåˆ†ç±»æŸå¤±ä¸­â€œæ— å¯¹è±¡â€ï¼ˆ âˆ…\\varnothing ï¼‰çš„æƒé‡è®¾ç½®ä¸º 0.1ã€‚

### 4.2 Training settings4.2 åŸ¹è®­è®¾ç½®

Semantic segmentation. We use Detectron2Â \[[46](#bib.bib46)\] and follow the commonly used training settings for each dataset. More specifically, we use AdamWÂ \[[31](#bib.bib31)\] and the _poly_Â \[[7](#bib.bib7)\] learning rate schedule with an initial learning rate of 10âˆ’4superscript10410^{-4} and a weight decay of 10âˆ’4superscript10410^{-4} for ResNetÂ \[[22](#bib.bib22)\] backbones, and an initial learning rate of 6â‹…10âˆ’5â‹…6superscript1056\\cdot 10^{-5} and a weight decay of 10âˆ’2superscript10210^{-2} for Swin-TransformerÂ \[[29](#bib.bib29)\] backbones. Backbones are pre-trained on ImageNet-1KÂ \[[35](#bib.bib35)\] if not stated otherwise. A learning rate multiplier of 0.10.10.1 is applied to CNN backbones and 1.01.01.0 is applied to Transformer backbones. The standard random scale jittering between 0.50.50.5 and 2.02.02.0, random horizontal flipping, random cropping as well as random color jittering are used as data augmentationÂ \[[14](#bib.bib14)\]. For the ADE20K dataset, if not stated otherwise, we use a crop size of 512Ã—512512512512\\times 512, a batch size of 161616 and train all models for 160k iterations. For the ADE20K-Full dataset, we use the same setting as ADE20K except that we train all models for 200k iterations. For the COCO-Stuff-10k dataset, we use a crop size of 640Ã—640640640640\\times 640, a batch size of 32 and train all models for 60k iterations. All models are trained with 8 V100 GPUs. We report both performance of single scale (s.s.) inference and multi-scale (m.s.) inference with horizontal flip and scales of 0.50.50.5, 0.750.750.75, 1.01.01.0, 1.251.251.25, 1.51.51.5, 1.751.751.75. See appendix for Cityscapes and Mapillary Vistas settings.  
è¯­ä¹‰åˆ†å‰²ã€‚æˆ‘ä»¬ä½¿ç”¨ Detectron2 \[46\] å¹¶éµå¾ªæ¯ä¸ªæ•°æ®é›†çš„å¸¸ç”¨è®­ç»ƒè®¾ç½®ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨ AdamW \[31\] å’Œ poly \[7\] å­¦ä¹ ç‡è¡¨ï¼ŒResNet \[22\] ä¸»å¹²çš„åˆå§‹å­¦ä¹ ç‡ 10âˆ’4superscript10410^{-4} å’Œæƒé‡è¡°å‡ä¸º Swin-Transformer 10âˆ’4superscript10410^{-4} \[29\] ä¸»å¹²çš„åˆå§‹å­¦ä¹ ç‡ 6â‹…10âˆ’5â‹…6superscript1056\\cdot 10^{-5} å’Œæƒé‡è¡°å‡ 10âˆ’2superscript10210^{-2} ã€‚å¦‚æœæ²¡æœ‰ç‰¹åˆ«è¯´æ˜ï¼Œä¸»å¹²ç½‘ä¼šåœ¨ ImageNet-1K \[35\] ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚å­¦ä¹ ç‡ä¹˜æ•°åº”ç”¨äº 0.10.10.1 CNN ä¸»å¹²ç½‘ï¼Œå¹¶ 1.01.01.0 åº”ç”¨äº Transformer ä¸»å¹²ç½‘ã€‚ä½¿ç”¨æ ‡å‡†éšæœºå°ºåº¦åœ¨å’Œ 2.02.02.0 ä¹‹é—´ 0.50.50.5 æŠ–åŠ¨ã€éšæœºæ°´å¹³ç¿»è½¬ã€éšæœºè£å‰ªä»¥åŠéšæœºé¢œè‰²æŠ–åŠ¨ä½œä¸ºæ•°æ®å¢å¼º\[14\]ã€‚å¯¹äº ADE20K æ•°æ®é›†ï¼Œå¦‚æœæ²¡æœ‰å¦è¡Œè¯´æ˜ï¼Œæˆ‘ä»¬ä½¿ç”¨ çš„è£å‰ªå¤§å° 512Ã—512512512512\\times 512 ï¼Œæ‰¹é‡å¤§å° ï¼Œ 161616 å¹¶è®­ç»ƒæ‰€æœ‰æ¨¡å‹è¿›è¡Œ 160k è¿­ä»£ã€‚å¯¹äº ADE20K-Full æ•°æ®é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸ ADE20K ç›¸åŒçš„è®¾ç½®ï¼Œåªæ˜¯æˆ‘ä»¬è®­ç»ƒæ‰€æœ‰æ¨¡å‹è¿›è¡Œ 200k è¿­ä»£ã€‚å¯¹äº COCO-Stuff-10k æ•°æ®é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨ 640Ã—640640640640\\times 640 è£å‰ªå¤§å° ï¼Œæ‰¹å¤„ç†å¤§å°ä¸º 32ï¼Œå¹¶è®­ç»ƒæ‰€æœ‰æ¨¡å‹è¿›è¡Œ 60k è¿­ä»£ã€‚æ‰€æœ‰å‹å·å‡ä½¿ç”¨ 8 ä¸ª V100 GPU è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬æŠ¥å‘Šäº†å•å°ºåº¦ ï¼ˆs.s.ï¼‰ æ¨ç†å’Œå¤šå°ºåº¦ ï¼ˆm.s.ï¼‰ æ¨ç†çš„æ€§èƒ½ï¼Œæ°´å¹³ç¿»è½¬å’Œå°ºåº¦ä¸º 0.50.50.5 ã€ 0.750.750.75 ã€ 1.01.01.0 ã€ 1.251.251.25 1.51.51.5 1.751.751.75 ã€ ã€‚æœ‰å…³ Cityscapes å’Œ Mapillary Vistas è®¾ç½®ï¼Œè¯·å‚é˜…é™„å½•ã€‚

Panoptic segmentation. We follow exactly the same architecture, loss, and training procedure as we use for semantic segmentation. The only difference is supervision: _i.e_., category region masks in semantic segmentation _vs_. object instance masks in panoptic segmentation. We strictly follow the DETRÂ \[[4](#bib.bib4)\] setting to train our model on the COCO panoptic segmentation datasetÂ \[[24](#bib.bib24)\] for a fair comparison. On the ADE20K panoptic segmentation dataset, we follow the semantic segmentation setting but train for longer (720k iterations) and use a larger crop size (640Ã—640640640640\\times 640). COCO models are trained using 64 V100 GPUs and ADE20K experiments are trained with 8 V100 GPUs. We use the general inference (SectionÂ [3.4](#S3.SS4 "3.4 Mask-classification inference â€£ 3 From Per-Pixel to Mask Classification â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation")) with the following parameters: we filter out masks with class confidence below 0.8 and set masks whose contribution to the final panoptic segmentation is less than 80% of its mask area to VOID. We report performance of single scale inference.  
å…¨æ™¯åˆ†å‰²ã€‚æˆ‘ä»¬éµå¾ªä¸è¯­ä¹‰åˆ†å‰²å®Œå…¨ç›¸åŒçš„æ¶æ„ã€æŸå¤±å’Œè®­ç»ƒè¿‡ç¨‹ã€‚å”¯ä¸€çš„åŒºåˆ«æ˜¯ç›‘ç£ï¼šå³è¯­ä¹‰åˆ†å‰²ä¸­çš„ç±»åˆ«åŒºåŸŸæ©ç ä¸å…¨æ™¯åˆ†å‰²ä¸­çš„å¯¹è±¡å®ä¾‹æ©ç ã€‚æˆ‘ä»¬ä¸¥æ ¼éµå¾ª DETR \[4\] è®¾ç½®ï¼Œåœ¨ COCO å…¨æ™¯åˆ†å‰²æ•°æ®é›† \[24\] ä¸Šè®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œä»¥ä¾¿è¿›è¡Œå…¬å¹³çš„æ¯”è¾ƒã€‚åœ¨ ADE20K å…¨æ™¯åˆ†å‰²æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬éµå¾ªè¯­ä¹‰åˆ†å‰²è®¾ç½®ï¼Œä½†è®­ç»ƒæ—¶é—´æ›´é•¿ï¼ˆ720k è¿­ä»£ï¼‰å¹¶ä½¿ç”¨æ›´å¤§çš„è£å‰ªå¤§å° ï¼ˆ 640Ã—640640640640\\times 640 ï¼‰ã€‚COCO æ¨¡å‹ä½¿ç”¨ 64 ä¸ª V100 GPU è¿›è¡Œè®­ç»ƒï¼ŒADE20K å®éªŒä½¿ç”¨ 8 ä¸ª V100 GPU è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬ä½¿ç”¨å…·æœ‰ä»¥ä¸‹å‚æ•°çš„ä¸€èˆ¬æ¨ç†ï¼ˆç¬¬ 3.4 èŠ‚ï¼‰ï¼šæˆ‘ä»¬è¿‡æ»¤æ‰ç±»ç½®ä¿¡åº¦ä½äº 0.8 çš„æ©æ¨¡ï¼Œå¹¶å°†å¯¹æœ€ç»ˆå…¨æ™¯åˆ†å‰²çš„è´¡çŒ®å°äºå…¶é®ç½©é¢ç§¯ 80% çš„é®ç½©è®¾ç½®ä¸º VOIDã€‚æˆ‘ä»¬æŠ¥å‘Šäº†å•å°ºåº¦æ¨ç†çš„æ€§èƒ½ã€‚

Table 1: Semantic segmentation on ADE20K val with 150 categories. Mask classification-based MaskFormer outperforms the best per-pixel classification approaches while using fewer parameters and less computation. We report both single-scale (s.s.) and multi-scale (m.s.) inference results with Â±plus-or-minus\\pmstd. FLOPs are computed for the given crop size. Frames-per-second (fps) is measured on a V100 GPU with a batch size of 1.444It isnâ€™t recommended to compare fps from different papers: speed is measured in different environments. DeepLabV3+ fps are from MMSegmentationÂ \[[14](#bib.bib14)\], and Swin-UperNet fps are from the original paperÂ \[[29](#bib.bib29)\].Â Backbones pre-trained on ImageNet-22K are marked with â€ â€ {}^{\\text{\\textdagger}}.  
è¡¨ 1ï¼šADE20K val çš„è¯­ä¹‰åˆ†å‰²ï¼ŒåŒ…å« 150 ä¸ªç±»åˆ«ã€‚åŸºäºæ©æ¨¡åˆ†ç±»çš„ MaskFormer ä¼˜äºæœ€ä½³çš„æ¯åƒç´ åˆ†ç±»æ–¹æ³•ï¼ŒåŒæ—¶ä½¿ç”¨æ›´å°‘çš„å‚æ•°å’Œæ›´å°‘çš„è®¡ç®—ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†å•å°ºåº¦ ï¼ˆs.s.ï¼‰ å’Œå¤šå°ºåº¦ ï¼ˆm.s.ï¼‰ æ¨ç†ç»“æœï¼Œå¹¶é’ˆå¯¹ Â±plus-or-minus\\pm ç»™å®šçš„ä½œç‰©å¤§å°è®¡ç®—äº† FLOPã€‚æ¯ç§’å¸§æ•° ï¼ˆfpsï¼‰ æ˜¯åœ¨æ‰¹å¤„ç†å¤§å°ä¸º 1 çš„ V100 GPU ä¸Šæµ‹é‡çš„ã€‚ 4 åœ¨ ImageNet-22K ä¸Šé¢„è®­ç»ƒçš„ä¸»å¹²ç½‘æ ‡æœ‰ â€ â€ {}^{\\text{\\textdagger}} ã€‚

|  | method | backbone | crop size ä½œç‰©å¤§å° | mIoU (s.s.)mIoU ï¼ˆs.s.ï¼‰ | mIoU (m.s.)mIoU ï¼ˆç¡•å£«ï¼‰ | #params. | FLOPs | fps |

| CNN backbonesCNN éª¨å¹²ç½‘ | OCRNetÂ \[[50](#bib.bib50)\] | R101c | 520Ã—520520520520\\times 520 | \- Â±plus-or-minus\\pm0.5 | 45.3 Â±plus-or-minus\\pm0.5 | - | - | - |

| DeepLabV3+Â \[[9](#bib.bib9)\]DeepLabV3+ï¼ˆæ·±å®éªŒå®¤ V3+ï¼‰ \[9\] | 0R50c | 512Ã—512512512512\\times 512 | 44.0 Â±plus-or-minus\\pm0.5 | 44.9 Â±plus-or-minus\\pm0.5 | 044M | 177G | 21.0 |

| R101c | 512Ã—512512512512\\times 512 | 45.5 Â±plus-or-minus\\pm0.5 | 46.4 Â±plus-or-minus\\pm0.5 | 063M | 255G | 14.2 |

| MaskFormer (ours)MaskFormerï¼ˆæˆ‘ä»¬çš„ï¼‰ | 0R50c | 512Ã—512512512512\\times 512 | 44.5 Â±plus-or-minus\\pm0.5 | 46.7 Â±plus-or-minus\\pm0.6 | 041M | 053G | 24.5 |

| R101c | 512Ã—512512512512\\times 512 | 45.5 Â±plus-or-minus\\pm0.5 | 47.2 Â±plus-or-minus\\pm0.2 | 060M | 073G | 19.5 |

| R101c | 512Ã—512512512512\\times 512 | 46.0  Â±plus-or-minus\\pm0.1 | 48.1  Â±plus-or-minus\\pm0.2 | 060M | 080G | 19.0 |

| Transformer backbones å˜å‹å™¨ä¸»å¹²ç½‘ | SETRÂ \[[53](#bib.bib53)\] å¡ç‰¹ 53 | ViT-Lâ€ â€ {}^{\\text{\\textdagger}}ViT-L â€ â€ {}^{\\text{\\textdagger}} å‹ | 512Ã—512512512512\\times 512 | \- Â±plus-or-minus\\pm0.5 | 50.3 Â±plus-or-minus\\pm0.5 | 308M | - | - |

| Swin-UperNetÂ \[[29](#bib.bib29), [49](#bib.bib49)\] æ–¯æ¸© - ä¹Œç€ç½‘ \[29ï¼Œ 49\] | Swin-Tâ€ â€ {}^{\\text{\\textdagger}}Swin-T â€ â€ {}^{\\text{\\textdagger}} å‹ | 512Ã—512512512512\\times 512 | \- Â±plus-or-minus\\pm0.5 | 46.1 Â±plus-or-minus\\pm0.5 | 060M | 236G | 18.5 |

| Swin-Sâ€ â€ {}^{\\text{\\textdagger}}æ–¯æ¸© -S â€ â€ {}^{\\text{\\textdagger}} | 512Ã—512512512512\\times 512 | \- Â±plus-or-minus\\pm0.5 | 49.3 Â±plus-or-minus\\pm0.5 | 081M | 259G | 15.2 |

| Swin-Bâ€ â€ {}^{\\text{\\textdagger}}Swin-B â€ â€ {}^{\\text{\\textdagger}} å‹ | 640Ã—640640640640\\times 640 | \- Â±plus-or-minus\\pm0.5 | 51.6 Â±plus-or-minus\\pm0.5 | 121M | 471G | 08.7 |

| Swin-Lâ€ â€ {}^{\\text{\\textdagger}}æ–¯æ¸© -L â€ â€ {}^{\\text{\\textdagger}} | 640Ã—640640640640\\times 640 | \- Â±plus-or-minus\\pm0.5 | 53.5 Â±plus-or-minus\\pm0.5 | 234M | 647G | 06.2 |

| MaskFormer (ours)MaskFormerï¼ˆæˆ‘ä»¬çš„ï¼‰ | Swin-Tâ€ â€ {}^{\\text{\\textdagger}}Swin-T â€ â€ {}^{\\text{\\textdagger}} å‹ | 512Ã—512512512512\\times 512 | 46.7 Â±plus-or-minus\\pm0.7 | 48.8 Â±plus-or-minus\\pm0.6 | 042M | 055G | 22.1 |

| Swin-Sâ€ â€ {}^{\\text{\\textdagger}}æ–¯æ¸© -S â€ â€ {}^{\\text{\\textdagger}} | 512Ã—512512512512\\times 512 | 49.8 Â±plus-or-minus\\pm0.4 | 51.0 Â±plus-or-minus\\pm0.4 | 063M | 079G | 19.6 |

| Swin-Bâ€ â€ {}^{\\text{\\textdagger}}Swin-B â€ â€ {}^{\\text{\\textdagger}} å‹ | 640Ã—640640640640\\times 640 | 51.1 Â±plus-or-minus\\pm0.2 | 52.3 Â±plus-or-minus\\pm0.4 | 102M | 195G | 12.6 |

| Swin-Bâ€ â€ {}^{\\text{\\textdagger}}Swin-B â€ â€ {}^{\\text{\\textdagger}} å‹ | 640Ã—640640640640\\times 640 | 52.7 Â±plus-or-minus\\pm0.4 | 53.9 Â±plus-or-minus\\pm0.2 | 102M | 195G | 12.6 |

| Swin-Lâ€ â€ {}^{\\text{\\textdagger}}æ–¯æ¸© -L â€ â€ {}^{\\text{\\textdagger}} | 640Ã—640640640640\\times 640 | 54.1  Â±plus-or-minus\\pm0.2 | 55.6  Â±plus-or-minus\\pm0.1 | 212M | 375G | 07.9 |

Table 2: MaskFormer _vs_. per-pixel classification baselines on 4 semantic segmentation datasets. MaskFormer improvement is larger when the number of classes is larger. We use a ResNet-50 backbone and report single scale mIoU and PQStSt{}^{\\text{St}} for ADE20K, COCO-Stuff and ADE20K-Full, whereas for higher-resolution Cityscapes we use a deeper ResNet-101 backbone followingÂ \[[8](#bib.bib8), [9](#bib.bib9)\].

|  | Cityscapes (19 classes) | ADE20K (150 classes) | COCO-Stuff (171 classes) | ADE20K-Full (847 classes) |

|  | mIoU | PQStSt{}^{\\text{St}} | mIoU | PQStSt{}^{\\text{St}} | mIoU | PQStSt{}^{\\text{St}} | mIoU | PQStSt{}^{\\text{St}} |

| PerPixelBaseline | 77.4  (+0.0) | 58.9  (+0.0) | 39.2  (+0.0) | 21.6  (+0.0) | 32.4  (+0.0) | 15.5  (+0.0) | 12.4  (+0.0) | 05.8  (+0.0) |

| PerPixelBaseline+ | 78.5  (+0.0) | 60.2 (+0.0) | 41.9 (+0.0) | 28.3 (+0.0) | 34.2 (+0.0) | 24.6 (+0.0) | 13.9 (+0.0) | 09.0 (+0.0) |

| MaskFormer (ours) | 78.5  (+0.0) | 63.1  (+2.9) | 44.5  (+2.6) | 33.4  (+5.1) | 37.1  (+2.9) | 28.9  (+4.3) | 17.4  (+3.5) | 11.9  (+2.9) |

### 4.3 Main results

Semantic segmentation. In TableÂ [1](#S4.T1 "Table 1 â€£ 4.2 Training settings â€£ 4 Experiments â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"), we compare MaskFormer with state-of-the-art per-pixel classification models for semantic segmentation on the ADE20K val set. With the same standard CNN backbones (_e.g_., ResNetÂ \[[22](#bib.bib22)\]), MaskFormer outperforms DeepLabV3+Â \[[9](#bib.bib9)\] by 1.7 mIoU. MaskFormer is also compatible with recent Vision TransformerÂ \[[17](#bib.bib17)\] backbones (_e.g_., the Swin TransformerÂ \[[29](#bib.bib29)\]), achieving a new state-of-the-art of 55.6 mIoU, which is 2.1 mIoU better than the prior state-of-the-artÂ \[[29](#bib.bib29)\]. Observe that MaskFormer outperforms the best per-pixel classification-based models while having fewer parameters and faster inference time. This result suggests that the mask classification formulation has significant potential for semantic segmentation. See appendix for results on test set.

Beyond ADE20K, we further compare MaskFormer with our baselines on COCO-Stuff-10K, ADE20K-Full as well as Cityscapes in TableÂ [2](#S4.T2 "Table 2 â€£ 4.2 Training settings â€£ 4 Experiments â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation") and we refer to the appendix for comparison with state-of-the-art methods on these datasets. The improvement of MaskFormer over PerPixelBaseline+ is larger when the number of classes is larger: For Cityscapes, which has only 19 categories, MaskFormer performs similarly well as PerPixelBaseline+; While for ADE20K-Full, which has 847 classes, MaskFormer outperforms PerPixelBaseline+ by 3.5 mIoU.  
é™¤äº† ADE20K ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥å°† MaskFormer ä¸è¡¨ 2 ä¸­çš„ COCO-Stuff-10Kã€ADE20K-Full ä»¥åŠ Cityscapes çš„åŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶å‚è€ƒé™„å½•ä¸è¿™äº›æ•°æ®é›†ä¸Šçš„æœ€æ–°æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚å½“ç±»æ•°é‡è¾ƒå¤šæ—¶ï¼ŒMaskFormer ç›¸å¯¹äº PerPixelBaseline+ çš„æ”¹è¿›æ›´å¤§ï¼šå¯¹äºåªæœ‰ 19 ä¸ªç±»åˆ«çš„ Cityscapesï¼ŒMaskFormer çš„è¡¨ç°ä¸ PerPixelBaseline+ ç±»ä¼¼; è€Œå¯¹äºå…·æœ‰ 847 ä¸ªç±»åˆ«çš„ ADE20K-Fullï¼ŒMaskFormer çš„æ€§èƒ½æ¯” PerPixelBaseline+ é«˜å‡º 3.5 mIoUã€‚

Although MaskFormer shows no improvement in mIoU for Cityscapes, the PQStSt{}^{\\text{St}} metric increases by 2.9 PQStSt{}^{\\text{St}}. We find MaskFormer performs better in terms of recognition quality (RQStSt{}^{\\text{St}}) while lagging in per-pixel segmentation quality (SQStSt{}^{\\text{St}}) (we refer to the appendix for detailed numbers). This observation suggests that on datasets where class recognition is relatively easy to solve, the main challenge for mask classification-based approaches is pixel-level accuracy (_i.e_., mask quality).  
å°½ç®¡ MaskFormer åœ¨åŸå¸‚æ™¯è§‚çš„ mIoU ä¸­æ²¡æœ‰æ˜¾ç¤ºä»»ä½•æ”¹è¿›ï¼Œä½† PQ StSt{}^{\\text{St}} æŒ‡æ ‡å¢åŠ äº† 2.9 PQ StSt{}^{\\text{St}} ã€‚æˆ‘ä»¬å‘ç° MaskFormer åœ¨è¯†åˆ«è´¨é‡ï¼ˆRQ StSt{}^{\\text{St}} ï¼‰æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œè€Œåœ¨æ¯åƒç´ åˆ†å‰²è´¨é‡ï¼ˆSQ StSt{}^{\\text{St}} ï¼‰æ–¹é¢è¡¨ç°è¾ƒå·®ï¼ˆè¯¦è§é™„å½•ï¼‰ã€‚è¿™ä¸€è§‚å¯Ÿç»“æœè¡¨æ˜ï¼Œåœ¨ç±»è¯†åˆ«ç›¸å¯¹å®¹æ˜“è§£å†³çš„æ•°æ®é›†ä¸Šï¼ŒåŸºäºæ©ç åˆ†ç±»çš„æ–¹æ³•çš„ä¸»è¦æŒ‘æˆ˜æ˜¯åƒç´ çº§ç²¾åº¦ï¼ˆå³æ©ç è´¨é‡ï¼‰ã€‚

Table 3: Panoptic segmentation on COCO panoptic val with 133 categories. MaskFormer seamlessly unifies semantic- and instance-level segmentation without modifying the model architecture or loss. Our model, which achieves better results, can be regarded as a box-free simplification of DETRÂ \[[4](#bib.bib4)\]. The major improvement comes from â€œstuffâ€ classes (PQStSt{}^{\\text{St}}) which are ambiguous to represent with bounding boxes. For MaskFormer (DETR) we use the exact same post-processing as DETR. Note, that in this setting MaskFormer performance is still better than DETR (+2.2 PQ). Our model also outperforms recently proposed Max-DeepLabÂ \[[42](#bib.bib42)\] without the need of sophisticated auxiliary losses, while being more efficient. FLOPs are computed as the average FLOPs over 100 validation images (COCO images have varying sizes). Frames-per-second (fps) is measured on a V100 GPU with a batch size of 1 by taking the average runtime on the entire val set _including post-processing time_. Backbones pre-trained on ImageNet-22K are marked with â€ â€ {}^{\\text{\\textdagger}}.  
è¡¨ 3ï¼šCOCO å…¨æ™¯å€¼çš„ 133 ä¸ªç±»åˆ«çš„å…¨æ™¯ç»†åˆ†ã€‚MaskFormer æ— ç¼ç»Ÿä¸€äº†è¯­ä¹‰çº§å’Œå®ä¾‹çº§åˆ†æ®µï¼Œè€Œä¸ä¼šä¿®æ”¹æ¨¡å‹æ¶æ„æˆ–ä¸¢å¤±ã€‚æˆ‘ä»¬çš„æ¨¡å‹å–å¾—äº†æ›´å¥½çš„ç»“æœï¼Œå¯ä»¥çœ‹ä½œæ˜¯ DETR çš„æ— æ¡†ç®€åŒ–\[4\]ã€‚ä¸»è¦çš„æ”¹è¿›æ¥è‡ªâ€œä¸œè¥¿â€ç±»ï¼ˆPQ StSt{}^{\\text{St}} ï¼‰ï¼Œè¿™äº›ç±»ç”¨è¾¹ç•Œæ¡†è¡¨ç¤ºæ˜¯æ¨¡æ£±ä¸¤å¯çš„ã€‚å¯¹äº MaskFormer ï¼ˆDETRï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸ DETR å®Œå…¨ç›¸åŒçš„åå¤„ç†ã€‚è¯·æ³¨æ„ï¼Œåœ¨æ­¤è®¾ç½®ä¸­ï¼ŒMaskFormer çš„æ€§èƒ½ä»ç„¶ä¼˜äº DETR ï¼ˆ+2.2 PQï¼‰ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä¹Ÿä¼˜äºæœ€è¿‘æå‡ºçš„ Max-DeepLab\[42\]ï¼Œä¸éœ€è¦å¤æ‚çš„è¾…åŠ©æŸè€—ï¼ŒåŒæ—¶æ•ˆç‡æ›´é«˜ã€‚FLOP è®¡ç®—ä¸º 100 ä¸ªéªŒè¯å›¾åƒçš„å¹³å‡ FLOPï¼ˆCOCO å›¾åƒçš„å¤§å°å„ä¸ç›¸åŒï¼‰ã€‚æ¯ç§’å¸§æ•° ï¼ˆfpsï¼‰ æ˜¯åœ¨æ‰¹å¤„ç†å¤§å°ä¸º 1 çš„ V100 GPU ä¸Šé€šè¿‡è·å–æ•´ä¸ª val é›†çš„å¹³å‡è¿è¡Œæ—¶é—´ï¼ˆåŒ…æ‹¬åå¤„ç†æ—¶é—´ï¼‰æ¥æµ‹é‡çš„ã€‚åœ¨ ImageNet-22K ä¸Šé¢„è®­ç»ƒçš„ä¸»å¹²ç½‘æ ‡æœ‰ â€ â€ {}^{\\text{\\textdagger}} ã€‚

|  | method | backbone | PQ | PQThTh{}^{\\text{Th}}é¤å‰ ThTh{}^{\\text{Th}} | PQStSt{}^{\\text{St}}é¤å‰ StSt{}^{\\text{St}} | SQ | RQ | #params. | FLOPs | fps |

| CNN backbonesCNN éª¨å¹²ç½‘ | DETRÂ \[[4](#bib.bib4)\] å¾·ç‰¹ \[4\] | 0R50 + 6 Enc | 43.4 | 48.2 (+0.2) | 36.3 (+2.4) | 79.3 | 53.8 | - | - | - |

| MaskFormer (DETR) æ©æ¨¡æˆå‹ ï¼ˆDETRï¼‰ | 0R50 + 6 Enc | 45.6 | 50.0 (+1.8) | 39.0 (+2.7) | 80.2 | 55.8 | - | - | - |

| MaskFormer (ours)MaskFormerï¼ˆæˆ‘ä»¬çš„ï¼‰ | 0R50 + 6 Enc | 46.5 | 51.0  (+2.8) | 39.8  (+3.5) | 80.4 | 56.8 | 045M | 0181G | 17.6 |

| DETRÂ \[[4](#bib.bib4)\] å¾·ç‰¹ \[4\] | R101 + 6 EncR101 + 6 æ©åŠ  | 45.1 | 50.5 (+0.2) | 37.0 (+2.4) | 79.9 | 55.5 | - | - | - |

| MaskFormer (ours)MaskFormerï¼ˆæˆ‘ä»¬çš„ï¼‰ | R101 + 6 EncR101 + 6 æ©åŠ  | 47.6 | 52.5  (+2.0) | 40.3  (+3.3) | 80.7 | 58.0 | 064M | 0248G | 14.0 |

| Transformer backbones å˜å‹å™¨ä¸»å¹²ç½‘ | Max-DeepLabÂ \[[42](#bib.bib42)\] é©¬å…‹æ–¯æ·±åº¦å®éªŒå®¤ \[42\] | Max-S | 48.4 | 53.0 (+0.2) | 41.5 (+0.2) | - | - | 062M | 0324G | 07.6 |

| Max-L | 51.1 | 57.0 (+0.2) | 42.2 (+0.2) | - | - | 451M | 3692G | - |

| MaskFormer (ours)MaskFormerï¼ˆæˆ‘ä»¬çš„ï¼‰ | Swin-Tâ€ â€ {}^{\\text{\\textdagger}}Swin-T â€ â€ {}^{\\text{\\textdagger}} å‹ | 47.7 | 51.7 (+0.2) | 41.7 (+0.2) | 80.4 | 58.3 | 042M | 0179G | 17.0 |

| Swin-Sâ€ â€ {}^{\\text{\\textdagger}}æ–¯æ¸© -S â€ â€ {}^{\\text{\\textdagger}} | 49.7 | 54.4 (+0.2) | 42.6 (+0.2) | 80.9 | 60.4 | 063M | 0259G | 12.4 |

| Swin-Bâ€ â€ {}^{\\text{\\textdagger}}Swin-B â€ â€ {}^{\\text{\\textdagger}} å‹ | 51.1 | 56.3 (+0.2) | 43.2 (+0.2) | 81.4 | 61.8 | 102M | 0411G | 08.4 |

| Swin-Bâ€ â€ {}^{\\text{\\textdagger}}Swin-B â€ â€ {}^{\\text{\\textdagger}} å‹ | 51.8 | 56.9 (+0.2) | 44.1  (+0.2) | 81.4 | 62.6 | 102M | 0411G | 08.4 |

| Swin-Lâ€ â€ {}^{\\text{\\textdagger}}æ–¯æ¸© -L â€ â€ {}^{\\text{\\textdagger}} | 52.7 | 58.5  (+0.2) | 44.0  (+0.2) | 81.8 | 63.5 | 212M | 0792G | 05.2 |

Panoptic segmentation. In TableÂ [3](#S4.T3 "Table 3 â€£ 4.3 Main results â€£ 4 Experiments â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"), we compare the same exact MaskFormer model with DETRÂ \[[4](#bib.bib4)\] on the COCO panoptic val set. To match the standard DETR design, we add 6 additional Transformer encoder layers after the CNN backbone. Unlike DETR, our model does not predict bounding boxes but instead predicts masks directly. MaskFormer achieves better results while being simpler than DETR. To disentangle the improvements from the model itself and our post-processing inference strategy we run our model following DETR post-processing (MaskFormer (DETR)) and observe that this setup outperforms DETR by 2.2 PQ. Overall, we observe a larger improvement in PQStSt{}^{\\text{St}} compared to PQThTh{}^{\\text{Th}}. This suggests that detecting â€œstuffâ€ with bounding boxes is suboptimal, and therefore, box-based segmentation models (_e.g_., Mask R-CNNÂ \[[21](#bib.bib21)\]) do not suit semantic segmentation. MaskFormer also outperforms recently proposed Max-DeepLabÂ \[[42](#bib.bib42)\] without the need of special network design as well as sophisticated auxiliary losses (_i.e_., instance discrimination loss, mask-ID cross entropy loss, and per-pixel classification loss inÂ \[[42](#bib.bib42)\]). _MaskFormer, for the first time, unifies semantic- and instance-level segmentation with the exact same model, loss, and training pipeline._  
å…¨æ™¯åˆ†å‰²ã€‚åœ¨è¡¨ 3 ä¸­ï¼Œæˆ‘ä»¬åœ¨ COCO å…¨æ™¯å€¼é›†ä¸Šæ¯”è¾ƒäº†ç›¸åŒçš„ MaskFormer æ¨¡å‹å’Œ DETR \[4\]ã€‚ä¸ºäº†åŒ¹é…æ ‡å‡†çš„ DETR è®¾è®¡ï¼Œæˆ‘ä»¬åœ¨ CNN ä¸»å¹²ç½‘ä¹‹åå¢åŠ äº† 6 ä¸ªé¢å¤–çš„ Transformer ç¼–ç å™¨å±‚ã€‚ä¸ DETR ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸é¢„æµ‹è¾¹ç•Œæ¡†ï¼Œè€Œæ˜¯ç›´æ¥é¢„æµ‹æ©ç ã€‚MaskFormer æ¯” DETR æ›´ç®€å•ï¼Œä½†æ•ˆæœæ›´å¥½ã€‚ä¸ºäº†å°†æ”¹è¿›ä¸æ¨¡å‹æœ¬èº«å’Œæˆ‘ä»¬çš„åå¤„ç†æ¨ç†ç­–ç•¥åŒºåˆ†å¼€æ¥ï¼Œæˆ‘ä»¬æŒ‰ç…§ DETR åå¤„ç† ï¼ˆMaskFormer ï¼ˆDETRï¼‰ï¼‰ è¿è¡Œæ¨¡å‹ï¼Œå¹¶è§‚å¯Ÿåˆ°æ­¤è®¾ç½®çš„æ€§èƒ½æ¯” DETR é«˜å‡º 2.2 PQã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸ PQ ThTh{}^{\\text{Th}} ç›¸æ¯”ï¼ŒPQ StSt{}^{\\text{St}} çš„æ”¹å–„æ›´å¤§ã€‚è¿™è¡¨æ˜ï¼Œç”¨è¾¹ç•Œæ¡†æ£€æµ‹â€œä¸œè¥¿â€æ˜¯æ¬¡ä¼˜çš„ï¼Œå› æ­¤ï¼ŒåŸºäºæ¡†çš„åˆ†å‰²æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼ŒMask R-CNN \[21\]ï¼‰ä¸é€‚åˆè¯­ä¹‰åˆ†å‰²ã€‚MaskFormer çš„æ€§èƒ½ä¹Ÿä¼˜äºæœ€è¿‘æå‡ºçš„ Max-DeepLab \[42\]ï¼Œæ— éœ€ç‰¹æ®Šçš„ç½‘ç»œè®¾è®¡ä»¥åŠå¤æ‚çš„è¾…åŠ©æŸå¤±ï¼ˆå³\[42\] ä¸­çš„å®ä¾‹åŒºåˆ†æŸå¤±ã€æ©ç  -ID äº¤å‰ç†µæŸå¤±å’Œæ¯åƒç´ åˆ†ç±»æŸå¤±ï¼‰ã€‚MaskFormer é¦–æ¬¡å°†è¯­ä¹‰çº§å’Œå®ä¾‹çº§åˆ†å‰²ä¸å®Œå…¨ç›¸åŒçš„æ¨¡å‹ã€æŸå¤±å’Œè®­ç»ƒç®¡é“ç»Ÿä¸€èµ·æ¥ã€‚

We further evaluate our model on the panoptic segmentation version of the ADE20K dataset. Our model also achieves state-of-the-art performance. We refer to the appendix for detailed results.  
æˆ‘ä»¬åœ¨ ADE20K æ•°æ®é›†çš„å…¨æ™¯åˆ†å‰²ç‰ˆæœ¬ä¸Šè¿›ä¸€æ­¥è¯„ä¼°äº†æˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹è¿˜å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æœ‰å…³è¯¦ç»†ç»“æœï¼Œè¯·å‚é˜…é™„å½•ã€‚

### 4.4 Ablation studies4.4 æ¶ˆèç ”ç©¶

We perform a series of ablation studies of MaskFormer using a single ResNet-50 backboneÂ \[[22](#bib.bib22)\].  
æˆ‘ä»¬ä½¿ç”¨å•ä¸ª ResNet-50 éª¨æ¶å¯¹ MaskFormer è¿›è¡Œäº†ä¸€ç³»åˆ—æ¶ˆèç ”ç©¶\[22\]ã€‚

Per-pixel _vs_. mask classification. In TableÂ [4b](#S4.T4.sf2 "In Table 4 â€£ 4.4 Ablation studies â€£ 4 Experiments â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"), we verify that the gains demonstrated by MaskFromer come from shifting the paradigm to mask classification. We start by comparing PerPixelBaseline+ and MaskFormer. The models are very similar and there are only 3 differences: 1) per-pixel _vs_. mask classification used by the models, 2) MaskFormer uses bipartite matching, and 3) the new model uses a combination of focal and dice losses as a mask loss, whereas PerPixelBaseline+ utilizes per-pixel cross entropy loss. First, we rule out the influence of loss differences by training PerPixelBaseline+ with exactly the same losses and observing no improvement. Next, in TableÂ [4a](#S4.T4.sf1 "In Table 4 â€£ 4.4 Ablation studies â€£ 4 Experiments â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"), we compare PerPixelBaseline+ with MaskFormer trained using a fixed matching (MaskFormer-fixed), _i.e_., N=Kğ‘ğ¾N=K and assignment done based on category label indices identically to the per-pixel classification setup. We observe that MaskFormer-fixed is 1.8 mIoU better than the baseline, suggesting that shifting from per-pixel classification to mask classification is indeed the main reason for the gains of MaskFormer. In TableÂ [4b](#S4.T4.sf2 "In Table 4 â€£ 4.4 Ablation studies â€£ 4 Experiments â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"), we further compare MaskFormer-fixed with MaskFormer trained with bipartite matching (MaskFormer-bipartite) and find bipartite matching is not only more flexible (allowing to predict less masks than the total number of categories) but also produces better results.  
æ¯åƒç´ ä¸è’™ç‰ˆåˆ†ç±»ã€‚åœ¨è¡¨ 4b ä¸­ï¼Œæˆ‘ä»¬éªŒè¯äº† MaskFromer æ‰€å±•ç¤ºçš„æ”¶ç›Šæ¥è‡ªå°†èŒƒå¼è½¬å˜ä¸ºæ©æ¨¡åˆ†ç±»ã€‚æˆ‘ä»¬é¦–å…ˆæ¯”è¾ƒ PerPixelBaseline+ å’Œ MaskFormerã€‚è¿™äº›æ¨¡å‹éå¸¸ç›¸ä¼¼ï¼Œåªæœ‰ 3 ä¸ªå·®å¼‚ï¼š1ï¼‰ æ¨¡å‹ä½¿ç”¨çš„æ¯åƒç´ ä¸æ©ç åˆ†ç±»ï¼Œ2ï¼‰ MaskFormer ä½¿ç”¨äºŒåˆ†åŒ¹é…ï¼Œä»¥åŠ 3ï¼‰ æ–°æ¨¡å‹ä½¿ç”¨ç„¦ç‚¹å’Œéª°å­æŸå¤±çš„ç»„åˆä½œä¸ºæ©ç æŸå¤±ï¼Œè€Œ PerPixelBaseline+ åˆ©ç”¨æ¯åƒç´ äº¤å‰ç†µæŸå¤±ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡è®­ç»ƒå…·æœ‰å®Œå…¨ç›¸åŒæŸå¤±çš„ PerPixelBaseline+ å¹¶ä¸”æ²¡æœ‰è§‚å¯Ÿåˆ°ä»»ä½•æ”¹è¿›æ¥æ’é™¤æŸå¤±å·®å¼‚çš„å½±å“ã€‚æ¥ä¸‹æ¥ï¼Œåœ¨è¡¨ 4a ä¸­ï¼Œæˆ‘ä»¬å°† PerPixelBaseline+ ä¸ä½¿ç”¨å›ºå®šåŒ¹é…ï¼ˆMaskFormer-fixedï¼‰è®­ç»ƒçš„ MaskFormer è¿›è¡Œæ¯”è¾ƒï¼Œ N=Kğ‘ğ¾N=K å³åŸºäºç±»åˆ«æ ‡ç­¾ç´¢å¼•å®Œæˆçš„èµ‹å€¼ä¸æ¯åƒç´ åˆ†ç±»è®¾ç½®ç›¸åŒã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ° MaskFormer-fixed æ¯”åŸºçº¿é«˜ 1.8 mIoUï¼Œè¿™è¡¨æ˜ä»æ¯åƒç´ åˆ†ç±»è½¬å‘æ©æ¨¡åˆ†ç±»ç¡®å®æ˜¯ MaskFormer æ”¶ç›Šçš„ä¸»è¦åŸå› ã€‚åœ¨è¡¨ 4b ä¸­ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¯”è¾ƒäº† MaskFormer-fixed å’Œç”¨äºŒåˆ†åŒ¹é…è®­ç»ƒçš„ MaskFormerï¼ˆMaskFormer-bipartiteï¼‰ï¼Œå‘ç°äºŒåˆ†åŒ¹é…ä¸ä»…æ›´çµæ´»ï¼ˆå…è®¸é¢„æµ‹çš„æ©ç å°‘äºç±»åˆ«æ€»æ•°ï¼‰ï¼Œè€Œä¸”äº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚

Table 4: Per-pixel _vs_. mask classification for semantic segmentation. All models use 150 queries for a fair comparison. We evaluate the models on ADE20K val with 150 categories. [4a](#S4.T4.sf1 "In Table 4 â€£ 4.4 Ablation studies â€£ 4 Experiments â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"): PerPixelBaseline+ and MaskFormer-fixed use similar fixed matching (_i.e_., matching by category index), this result confirms that the shift from per-pixel to mask classification is the key. [4b](#S4.T4.sf2 "In Table 4 â€£ 4.4 Ablation studies â€£ 4 Experiments â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"): bipartite matching is not only more flexible (can make less prediction than total class count) but also gives better results.  
è¡¨ 4ï¼šè¯­ä¹‰åˆ†å‰²çš„æ¯åƒç´ ä¸æ©ç åˆ†ç±»ã€‚æ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ 150 ä¸ªæŸ¥è¯¢è¿›è¡Œå…¬å¹³æ¯”è¾ƒã€‚æˆ‘ä»¬åœ¨ ADE20K val ä¸Šè¯„ä¼°äº† 150 ä¸ªç±»åˆ«çš„æ¨¡å‹ã€‚å›¾ 4aï¼šPerPixelBaseline+ å’Œ MaskFormer-fixed ä½¿ç”¨ç±»ä¼¼çš„å›ºå®šåŒ¹é…ï¼ˆå³æŒ‰ç±»åˆ«ç´¢å¼•åŒ¹é…ï¼‰ï¼Œæ­¤ç»“æœè¯å®äº†ä»æ¯åƒç´ åˆ°è’™ç‰ˆåˆ†ç±»çš„è½¬å˜æ˜¯å…³é”®ã€‚4Bï¼šäºŒåˆ†åŒ¹é…ä¸ä»…æ›´çµæ´»ï¼ˆå¯ä»¥åšå‡ºçš„é¢„æµ‹æ¯”æ€»ç±»æ•°å°‘ï¼‰ï¼Œè€Œä¸”æä¾›æ›´å¥½çš„ç»“æœã€‚

|  | mIoU | PQStSt{}^{\\text{St}}é¤å‰ StSt{}^{\\text{St}} |

| PerPixelBaseline+ | 41.9 (+0.0) | 28.3 (+0.0) |

| MaskFormer-fixed | 43.7  (+1.8) | 30.3  (+2.0) |

(a) Per-pixel _vs_. mask classification.  
ï¼ˆä¸€ï¼‰æ¯åƒç´ ä¸è’™ç‰ˆåˆ†ç±»ã€‚

|  | mIoU | PQStSt{}^{\\text{St}}é¤å‰ StSt{}^{\\text{St}} |

| MaskFormer-fixed | 43.7 (+0.0) | 30.3 (+0.0) |

| MaskFormer-bipartite (ours)  
MaskFormer-bipartite ï¼ˆæˆ‘ä»¬çš„ï¼‰ | 44.2  (+0.5) | 33.4  (+3.1) |

(b) Fixed _vs_. bipartite matching assignment.  
ï¼ˆäºŒï¼‰å›ºå®šåŒ¹é…åˆ†é…ä¸äºŒåˆ†åŒ¹é…åˆ†é…ã€‚

|  | ADE20K | COCO-Stuff | ADE20K-Full |

| \# of queries\# ä¸ªæŸ¥è¯¢ | mIoU | PQStSt{}^{\\text{St}}é¤å‰ StSt{}^{\\text{St}} | mIoU | PQStSt{}^{\\text{St}}é¤å‰ StSt{}^{\\text{St}} | mIoU | PQStSt{}^{\\text{St}}é¤å‰ StSt{}^{\\text{St}} |

| PerPixelBaseline+ | 41.9 | 28.3 | 34.2 | 24.6 | 13.9 | 9.0 |

| 20 | 42.9 | 32.6 | 35.0 | 27.6 | 14.1 | 10.8 |

| 50 | 43.9 | 32.7 | 35.5 | 27.9 | 15.4 | 11.1 |

| 100 | 44.5 | 33.4 | 37.1 | 28.9 | 16.0 | 11.9 |

| 150 | 44.2 | 33.4 | 37.0 | 28.9 | 15.5 | 11.5 |

| 300 | 43.5 | 32.3 | 36.1 | 29.1 | 14.2 | 10.3 |

| 1000 | 35.4 | 26.7 | 34.4 | 27.6 | 08.0 | 05.8 |

Number of queries. The table to the right shows results of MaskFormer trained with a varying number of queries on datasets with different number of categories. The model with 100 queries consistently performs the best across the studied datasets. This suggest we may not need to adjust the number of queries w.r.t. the number of categories or datasets much. Interestingly, even with 20 queries MaskFormer outperforms our per-pixel classification baseline.  
æŸ¥è¯¢æ•°ã€‚å³è¡¨æ˜¾ç¤ºäº† MaskFormer çš„ç»“æœï¼Œè¿™äº›ç»“æœä½¿ç”¨ä¸åŒæ•°é‡çš„æŸ¥è¯¢å¯¹å…·æœ‰ä¸åŒç±»åˆ«æ•°çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚å…·æœ‰ 100 ä¸ªæŸ¥è¯¢çš„æ¨¡å‹åœ¨æ‰€ç ”ç©¶çš„æ•°æ®é›†ä¸­å§‹ç»ˆè¡¨ç°æœ€ä½³ã€‚è¿™è¡¨æ˜æˆ‘ä»¬å¯èƒ½ä¸éœ€è¦è¿‡å¤šåœ°è°ƒæ•´æŸ¥è¯¢çš„æ•°é‡ï¼Œè€Œä¸æ˜¯ç±»åˆ«æˆ–æ•°æ®é›†çš„æ•°é‡ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå³ä½¿æœ‰ 20 ä¸ªæŸ¥è¯¢ï¼ŒMaskFormer çš„æ€§èƒ½ä¹Ÿä¼˜äºæˆ‘ä»¬çš„æ¯åƒç´ åˆ†ç±»åŸºçº¿ã€‚

We further calculate the number of classes which are on average present in a _training set_ image. We find these statistics to be similar across datasets despite the fact that the datasets have different number of total categories: 8.2 classes per image for ADE20K (150 classes), 6.6 classes per image for COCO-Stuff-10K (171 classes) and 9.1 classes per image for ADE20K-Full (847 classes). We hypothesize that each query is able to capture masks from multiple categories.  
æˆ‘ä»¬è¿›ä¸€æ­¥è®¡ç®—è®­ç»ƒé›†å›¾åƒä¸­å¹³å‡å­˜åœ¨çš„ç±»æ•°ã€‚æˆ‘ä»¬å‘ç°è¿™äº›ç»Ÿè®¡æ•°æ®åœ¨æ•°æ®é›†ä¸­æ˜¯ç›¸ä¼¼çš„ï¼Œå°½ç®¡æ•°æ®é›†çš„æ€»ç±»åˆ«æ•°é‡ä¸åŒï¼šADE20Kï¼ˆ150 ä¸ªç±»ï¼‰æ¯ä¸ªå›¾åƒ 8.2 ä¸ªç±»ï¼ŒCOCO-Stuff-10Kï¼ˆ171 ä¸ªç±»ï¼‰æ¯ä¸ªå›¾åƒ 6.6 ä¸ªç±»ï¼ŒADE20K-Fullï¼ˆ847 ä¸ªç±»ï¼‰æ¯ä¸ªå›¾åƒ 9.1 ä¸ªç±»ã€‚æˆ‘ä»¬å‡è®¾æ¯ä¸ªæŸ¥è¯¢éƒ½èƒ½å¤Ÿæ•è·æ¥è‡ªå¤šä¸ªç±»åˆ«çš„æ©ç ã€‚

| Number of unique classes predicted by each query on validation set  
éªŒè¯é›†ä¸Šæ¯ä¸ªæŸ¥è¯¢é¢„æµ‹çš„å”¯ä¸€ç±»æ•° | ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/x4.png)

 |

| (a) ADE20K (150 classes)ï¼ˆaï¼‰ ADE20Kï¼ˆ150 ç±»ï¼‰ |

| ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/x5.png)

 |

| (b) COCO-Stuff-10K (171 classes)  
ï¼ˆbï¼‰ COCO-Stuff-10Kï¼ˆ171 ç±»ï¼‰ |

| ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/x6.png)

 |

| (c) ADE20K-Full (847 classes)  
ï¼ˆcï¼‰ ADE20K-Full ï¼ˆ847 ç±»ï¼‰ |

The figure to the right shows the number of unique categories predicted by each query (sorted in descending order) of our MaskFormer model on the validation sets of the corresponding datasets. Interestingly, the number of unique categories per query does not follow a uniform distribution: some queries capture more classes than others. We try to analyze how MaskFormer queries group categories, but we do not observe any obvious pattern: there are queries capturing categories with similar semantics or shapes (_e.g_., â€œhouseâ€ and â€œbuildingâ€), but there are also queries capturing completely different categories (_e.g_., â€œwaterâ€ and â€œsofaâ€).  
å³å›¾æ˜¾ç¤ºäº† MaskFormer æ¨¡å‹çš„æ¯ä¸ªæŸ¥è¯¢ï¼ˆæŒ‰é™åºæ’åºï¼‰åœ¨ç›¸åº”æ•°æ®é›†çš„éªŒè¯é›†ä¸Šé¢„æµ‹çš„å”¯ä¸€ç±»åˆ«æ•°ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæ¯ä¸ªæŸ¥è¯¢çš„å”¯ä¸€ç±»åˆ«æ•°é‡å¹¶ä¸éµå¾ªå‡åŒ€åˆ†å¸ƒï¼šæŸäº›æŸ¥è¯¢æ•è·çš„ç±»æ¯”å…¶ä»–æŸ¥è¯¢æ›´å¤šã€‚æˆ‘ä»¬è¯•å›¾åˆ†æ MaskFormer æŸ¥è¯¢å¦‚ä½•å¯¹ç±»åˆ«è¿›è¡Œåˆ†ç»„ï¼Œä½†æˆ‘ä»¬æ²¡æœ‰è§‚å¯Ÿåˆ°ä»»ä½•æ˜æ˜¾çš„æ¨¡å¼ï¼šæœ‰äº›æŸ¥è¯¢æ•è·å…·æœ‰ç›¸ä¼¼è¯­ä¹‰æˆ–å½¢çŠ¶çš„ç±»åˆ«ï¼ˆä¾‹å¦‚ï¼Œâ€œæˆ¿å­â€å’Œâ€œå»ºç­‘ç‰©â€ï¼‰ï¼Œä½†ä¹Ÿæœ‰ä¸€äº›æŸ¥è¯¢æ•è·å®Œå…¨ä¸åŒçš„ç±»åˆ«ï¼ˆä¾‹å¦‚ï¼Œâ€œæ°´â€å’Œâ€œæ²™å‘â€ï¼‰ã€‚

Number of Transformer decoder layers. Interestingly, MaskFormer with even a single Transformer decoder layer already performs well for semantic segmentation and achieves better performance than our 6-layer-decoder PerPixelBaseline+. For panoptic segmentation, however, multiple decoder layers are required to achieve competitive performance. Please see the appendix for a detailed discussion.  
Transformer è§£ç å™¨å±‚æ•°ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå³ä½¿æ˜¯å•ä¸ª Transformer è§£ç å™¨å±‚çš„ MaskFormer åœ¨è¯­ä¹‰åˆ†å‰²æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”æ¯”æˆ‘ä»¬çš„ 6 å±‚è§£ç å™¨ PerPixelBaseline+ å…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºå…¨æ™¯åˆ†å‰²ï¼Œéœ€è¦å¤šä¸ªè§£ç å™¨å±‚æ‰èƒ½å®ç°æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¯¦ç»†è®¨è®ºè§é™„å½•ã€‚

## 5 Discussion è®¨è®º

Our main goal is to show that mask classification is a general segmentation paradigm that could be a competitive alternative to per-pixel classification for semantic segmentation. To better understand its potential for segmentation tasks, we focus on exploring mask classification independently of other factors like architecture, loss design, or augmentation strategy. We pick the DETRÂ \[[4](#bib.bib4)\] architecture as our baseline for its simplicity and deliberately make as few architectural changes as possible. Therefore, MaskFormer can be viewed as a â€œbox-freeâ€ version of DETR.  
æˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯è¡¨æ˜æ©ç åˆ†ç±»æ˜¯ä¸€ç§é€šç”¨çš„åˆ†å‰²èŒƒå¼ï¼Œå¯ä»¥æˆä¸ºè¯­ä¹‰åˆ†å‰²çš„æ¯åƒç´ åˆ†ç±»çš„ç«äº‰æ€§æ›¿ä»£æ–¹æ¡ˆã€‚ä¸ºäº†æ›´å¥½åœ°äº†è§£å…¶åœ¨åˆ†å‰²ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæ¢ç´¢ç‹¬ç«‹äºå…¶ä»–å› ç´ ï¼ˆå¦‚æ¶æ„ã€æŸå¤±è®¾è®¡æˆ–å¢å¼ºç­–ç•¥ï¼‰çš„æ©ç åˆ†ç±»ã€‚æˆ‘ä»¬é€‰æ‹© DETR \[4\] æ¶æ„ä½œä¸ºæˆ‘ä»¬çš„åŸºå‡†ï¼Œå› ä¸ºå®ƒå¾ˆç®€å•ï¼Œå¹¶ç‰¹æ„è¿›è¡Œå°½å¯èƒ½å°‘çš„æ¶æ„æ›´æ”¹ã€‚å› æ­¤ï¼ŒMaskFormer å¯ä»¥çœ‹ä½œæ˜¯ DETR çš„â€œæ— ç›’â€ç‰ˆæœ¬ã€‚

Table 5: Matching with masks _vs_. boxes. We compare DETRÂ \[[4](#bib.bib4)\] which uses box-based matching with two MaskFormer models trained with box- and mask-based matching respectively. To use box-based matching in MaskFormer we add to the model an additional box prediction head as in DETR. Note, that with box-based matching MaskFormer performs on par with DETR, whereas with mask-based matching it shows better results. The evaluation is done on COCO panoptic val set.  
è¡¨ 5ï¼šä¸è’™ç‰ˆä¸ç›’å­çš„åŒ¹é…ã€‚æˆ‘ä»¬å°†ä½¿ç”¨åŸºäºç›’çš„åŒ¹é…çš„ DETR \[4\] ä¸åˆ†åˆ«ä½¿ç”¨åŸºäºç›’å’ŒåŸºäºç›’çš„åŒ¹é…è®­ç»ƒçš„ä¸¤ä¸ª MaskFormer æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚ä¸ºäº†åœ¨ MaskFormer ä¸­ä½¿ç”¨åŸºäºæ¡†çš„åŒ¹é…ï¼Œæˆ‘ä»¬åœ¨æ¨¡å‹ä¸­æ·»åŠ äº†ä¸€ä¸ªé¢å¤–çš„æ¡†é¢„æµ‹å¤´ï¼Œå°±åƒåœ¨ DETR ä¸­ä¸€æ ·ã€‚è¯·æ³¨æ„ï¼Œä½¿ç”¨åŸºäºæ¡†çš„åŒ¹é…æ—¶ï¼ŒMaskFormer çš„æ€§èƒ½ä¸ DETR ç›¸å½“ï¼Œè€Œä½¿ç”¨åŸºäºæ©ç çš„åŒ¹é…æ—¶ï¼Œå®ƒæ˜¾ç¤ºçš„ç»“æœæ›´å¥½ã€‚è¯„ä¼°æ˜¯åœ¨ COCO å…¨æ™¯å€¼é›†ä¸Šå®Œæˆçš„ã€‚

| method | backbone | matching | PQ | PQThTh{}^{\\text{Th}}é¤å‰ ThTh{}^{\\text{Th}} | PQStSt{}^{\\text{St}}é¤å‰ StSt{}^{\\text{St}} |

| DETRÂ \[[4](#bib.bib4)\] å¾·ç‰¹ \[4\] | R50 + 6 EncR50 + 6 æ©åŠ  | by box æŒ‰åŒ…è£…ç›’ | 43.4 | 48.2 | 36.3 |

| MaskFormer (ours)MaskFormerï¼ˆæˆ‘ä»¬çš„ï¼‰ | R50 + 6 EncR50 + 6 æ©åŠ  | by box æŒ‰åŒ…è£…ç›’ | 43.7 | 49.2 | 35.3 |

| R50 + 6 EncR50 + 6 æ©åŠ  | by mask æŒ‰é¢å…· | 46.5 | 51.0 | 39.8 |

In this section, we discuss in detail the differences between MaskFormer and DETR and show how these changes are required to ensure that mask classification performs well. First, to achieve a pure mask classification setting we remove the box prediction head and perform matching between prediction and ground truth segments with masks instead of boxes. Secondly, we replace the compute-heavy _per-query_ mask head used in DETR with a more efficient _per-image_ FPN-based head to make end-to-end training without box supervision feasible.  
åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è¯¦ç»†è®¨è®º MaskFormer å’Œ DETR ä¹‹é—´çš„åŒºåˆ«ï¼Œå¹¶å±•ç¤ºå¦‚ä½•è¿›è¡Œè¿™äº›æ›´æ”¹ä»¥ç¡®ä¿æ©ç åˆ†ç±»æ€§èƒ½è‰¯å¥½ã€‚é¦–å…ˆï¼Œä¸ºäº†å®ç°çº¯æ©ç åˆ†ç±»è®¾ç½®ï¼Œæˆ‘ä»¬ç§»é™¤äº†æ¡†é¢„æµ‹å¤´ï¼Œå¹¶ä½¿ç”¨æ©ç è€Œä¸æ˜¯æ¡†åœ¨é¢„æµ‹å’ŒçœŸå®çº¿æ®µä¹‹é—´æ‰§è¡ŒåŒ¹é…ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°† DETR ä¸­ä½¿ç”¨çš„è®¡ç®—é‡å¤§çš„æ¯æŸ¥è¯¢æ©ç å¤´æ›¿æ¢ä¸ºæ›´é«˜æ•ˆçš„åŸºäºæ¯å›¾åƒ FPN çš„å¤´ï¼Œä½¿æ²¡æœ‰æ¡†ç›‘ç£çš„ç«¯åˆ°ç«¯è®­ç»ƒå˜å¾—å¯è¡Œã€‚

Matching with masks is superior to matching with boxes. We compare MaskFormer models trained using matching with boxes or masks in TableÂ [5](#S5.T5 "Table 5 â€£ 5 Discussion â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"). To do box-based matching, we add to MaskFormer an additional box prediction head as in DETRÂ \[[4](#bib.bib4)\]. Observe that MaskFormer, which directly matches with mask predictions, has a clear advantage. We hypothesize that matching with boxes is more ambiguous than matching with masks, especially for stuff categories where completely different masks can have similar boxes as stuff regions often spread over a large area in an image.  
ä¸å£ç½©æ­é…ä¼˜äºä¸ç›’å­æ­é…ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä½¿ç”¨è¡¨ 5 ä¸­çš„æ¡†æˆ–æ©ç åŒ¹é…è®­ç»ƒçš„ MaskFormer æ¨¡å‹ã€‚ä¸ºäº†è¿›è¡ŒåŸºäºæ¡†çš„åŒ¹é…ï¼Œæˆ‘ä»¬åœ¨ MaskFormer ä¸­æ·»åŠ äº†ä¸€ä¸ªé¢å¤–çš„æ¡†é¢„æµ‹å¤´ï¼Œå¦‚ DETR \[4\] æ‰€ç¤ºã€‚è§‚å¯Ÿä¸€ä¸‹ï¼Œä¸æ©ç é¢„æµ‹ç›´æ¥åŒ¹é…çš„ MaskFormer å…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬å‡è®¾ä¸æ¡†åŒ¹é…æ¯”ä¸è’™ç‰ˆåŒ¹é…æ›´æ¨¡ç³Šï¼Œç‰¹åˆ«æ˜¯å¯¹äºå®Œå…¨ä¸åŒçš„è’™ç‰ˆå¯èƒ½å…·æœ‰ç›¸ä¼¼æ¡†çš„ç´ æç±»åˆ«ï¼Œå› ä¸ºç´ æåŒºåŸŸé€šå¸¸åˆ†å¸ƒåœ¨å›¾åƒä¸­çš„å¤§é¢ç§¯åŒºåŸŸã€‚

MaskFormer mask head reduces computation. Results in TableÂ [5](#S5.T5 "Table 5 â€£ 5 Discussion â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation") also show that MaskFormer performs on par with DETR when the same matching strategy is used. This suggests that the difference in mask head designs between the models does not significantly influence the prediction quality. The new head, however, has significantly lower computational and memory costs in comparison with the original mask head used in DETR. In MaskFormer, we first upsample image features to get high-resolution per-pixel embeddings and directly generate binary mask predictions at a high-resolution. Note, that the per-pixel embeddings from the upsampling module (_i.e_., pixel decoder) are shared among all queries. In contrast, DETR first generates low-resolution attention maps and applies an independent upsampling module to each query. Thus, the mask head in DETR is Nğ‘N times more computationally expensive than the mask head in MaskFormer (where Nğ‘N is the number of queries).  
MaskFormer æ©æ¨¡å¤´å¯å‡å°‘è®¡ç®—é‡ã€‚è¡¨ 5 ä¸­çš„ç»“æœè¿˜è¡¨æ˜ï¼Œå½“ä½¿ç”¨ç›¸åŒçš„åŒ¹é…ç­–ç•¥æ—¶ï¼ŒMaskFormer çš„æ€§èƒ½ä¸ DETR ç›¸å½“ã€‚è¿™è¡¨æ˜æ¨¡å‹ä¹‹é—´æ©æ¨¡å¤´è®¾è®¡çš„å·®å¼‚ä¸ä¼šæ˜¾ç€å½±å“é¢„æµ‹è´¨é‡ã€‚ç„¶è€Œï¼Œä¸ DETR ä¸­ä½¿ç”¨çš„åŸå§‹æ©æ¨¡ç£å¤´ç›¸æ¯”ï¼Œæ–°ç£å¤´çš„è®¡ç®—å’Œå†…å­˜æˆæœ¬è¦ä½å¾—å¤šã€‚åœ¨ MaskFormer ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹å›¾åƒç‰¹å¾è¿›è¡Œä¸Šé‡‡æ ·ä»¥è·å¾—é«˜åˆ†è¾¨ç‡çš„æ¯åƒç´ åµŒå…¥ï¼Œå¹¶ç›´æ¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„äºŒè¿›åˆ¶æ©ç é¢„æµ‹ã€‚è¯·æ³¨æ„ï¼Œæ¥è‡ªä¸Šé‡‡æ ·æ¨¡å—ï¼ˆå³åƒç´ è§£ç å™¨ï¼‰çš„æ¯åƒç´ åµŒå…¥åœ¨æ‰€æœ‰æŸ¥è¯¢ä¹‹é—´å…±äº«ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒDETR é¦–å…ˆç”Ÿæˆä½åˆ†è¾¨ç‡çš„æ³¨æ„åŠ›å›¾ï¼Œå¹¶å¯¹æ¯ä¸ªæŸ¥è¯¢åº”ç”¨ç‹¬ç«‹çš„ä¸Šé‡‡æ ·æ¨¡å—ã€‚å› æ­¤ï¼ŒDETR ä¸­çš„æ©ç å¤´æ¯” MaskFormer ä¸­çš„æ©ç å¤´è®¡ç®—æˆæœ¬ Nğ‘N é«˜å‡ºå‡ å€ï¼ˆå…¶ä¸­ Nğ‘N æ˜¯æŸ¥è¯¢æ•°ï¼‰ã€‚

## 6 Conclusion ç»“è®º

The paradigm discrepancy between semantic- and instance-level segmentation results in entirely different models for each task, hindering development of image segmentation as a whole. We show that a simple mask classification model can outperform state-of-the-art per-pixel classification models, especially in the presence of large number of categories. Our model also remains competitive for panoptic segmentation, without a need to change model architecture, losses, or training procedure. We hope this unification spurs a joint effort across semantic- and instance-level segmentation tasks.  
è¯­ä¹‰çº§å’Œå®ä¾‹çº§åˆ†å‰²ä¹‹é—´çš„èŒƒå¼å·®å¼‚å¯¼è‡´æ¯ä¸ªä»»åŠ¡çš„æ¨¡å‹å®Œå…¨ä¸åŒï¼Œé˜»ç¢äº†æ•´ä¸ªå›¾åƒåˆ†å‰²çš„å‘å±•ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œç®€å•çš„æ©ç åˆ†ç±»æ¨¡å‹å¯ä»¥ä¼˜äºæœ€å…ˆè¿›çš„æ¯åƒç´ åˆ†ç±»æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨å­˜åœ¨å¤§é‡ç±»åˆ«çš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å…¨æ™¯åˆ†å‰²æ–¹é¢ä¹Ÿä¿æŒç«äº‰åŠ›ï¼Œæ— éœ€æ›´æ”¹æ¨¡å‹æ¶æ„ã€æŸå¤±æˆ–è®­ç»ƒç¨‹åºã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç§ç»Ÿä¸€èƒ½å¤Ÿä¿ƒè¿›è¯­ä¹‰çº§å’Œå®ä¾‹çº§åˆ†æ®µä»»åŠ¡çš„å…±åŒåŠªåŠ›ã€‚

Acknowledgments and Disclosure of Funding  
èµ„é‡‘çš„ç¡®è®¤å’ŒæŠ«éœ²
----------------------------------------------------

We thank Ross Girshick for insightful comments and suggestions. Work of UIUC authors Bowen Cheng and Alexander G. Schwing was supported in part by NSF under Grant #1718221, 2008387, 2045586, 2106825, MRI #1725729, NIFA award 2020-67021-32799 and Cisco Systems Inc.Â (Gift Award CG 1377144 - thanks for access to Arcetri).  
æˆ‘ä»¬æ„Ÿè°¢ Ross Girshick çš„æœ‰è§åœ°çš„è¯„è®ºå’Œå»ºè®®ã€‚UIUC ä½œè€… Bowen Cheng å’Œ Alexander G. Schwing çš„å·¥ä½œå¾—åˆ°äº† NSF çš„éƒ¨åˆ†æ”¯æŒï¼ŒåŒ…æ‹¬ Grant #1718221ã€2008387ã€2045586ã€2106825ã€MRI #1725729ã€NIFA å¥– 2020-67021-32799 å’Œ Cisco Systems Inc.ï¼ˆç¤¼å“å¥– CG 1377144 - æ„Ÿè°¢æ‚¨è®¿é—® Arcetriï¼‰ã€‚

Appendix é™„å½•

We first provide more information regarding the datasets used in our experimental evaluation of MaskFormer (AppendixÂ [A](#A1 "Appendix A Datasets description â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation")). Then, we provide detailed results of our model on more semantic (AppendixÂ [B](#A2 "Appendix B Semantic segmentation results â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation")) and panoptic (AppendixÂ [C](#A3 "Appendix C Panoptic segmentation results â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation")) segmentation datasets. Finally, we provide additional ablation studies (AppendixÂ [D](#A4 "Appendix D Additional ablation studies â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation")) and visualization (AppendixÂ [E](#A5 "Appendix E Visualization â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation")).  
æˆ‘ä»¬é¦–å…ˆæä¾›æœ‰å…³ MaskFormer å®éªŒè¯„ä¼°ä¸­ä½¿ç”¨çš„æ•°æ®é›†çš„æ›´å¤šä¿¡æ¯ï¼ˆé™„å½• Aï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨æ›´å¤šè¯­ä¹‰ï¼ˆé™„å½• Bï¼‰å’Œå…¨æ™¯ï¼ˆé™„å½• Cï¼‰åˆ†å‰²æ•°æ®é›†ä¸Šæä¾›äº†æˆ‘ä»¬çš„æ¨¡å‹çš„è¯¦ç»†ç»“æœã€‚æœ€åï¼Œæˆ‘ä»¬æä¾›äº†é¢å¤–çš„æ¶ˆèç ”ç©¶ï¼ˆé™„å½• Dï¼‰å’Œå¯è§†åŒ–ï¼ˆé™„å½• Eï¼‰ã€‚

Appendix A Datasets description  
é™„å½• ADatasets è¯´æ˜
-------------------------------------------------

We study MaskFormer using five semantic segmentation datasets and two panoptic segmentation datasets. Here, we provide more detailed information about these datasets.  
æˆ‘ä»¬ä½¿ç”¨äº”ä¸ªè¯­ä¹‰åˆ†å‰²æ•°æ®é›†å’Œä¸¤ä¸ªå…¨æ™¯åˆ†å‰²æ•°æ®é›†æ¥ç ”ç©¶ MaskFormerã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æä¾›äº†æœ‰å…³è¿™äº›æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

### A.1 Semantic segmentation datasets  

A.1 è¯­ä¹‰åˆ†å‰²æ•°æ®é›†

ADE20KÂ \[[55](#bib.bib55)\] contains 20k images for training and 2k images for validation. The data comes from the ADE20K-Full dataset where 150 semantic categories are selected to be included in evaluation from the SceneParse150 challengeÂ \[[54](#bib.bib54)\]. The images are resized such that the shortest side is no greater than 512 pixels. During inference, we resize the shorter side of the image to the corresponding crop size.  
ADE20K \[55\] åŒ…å«ç”¨äºè®­ç»ƒçš„ 20k å›¾åƒå’Œç”¨äºéªŒè¯çš„ 2k å›¾åƒã€‚æ•°æ®æ¥è‡ª ADE20K-Full æ•°æ®é›†ï¼Œå…¶ä¸­é€‰æ‹©äº† 150 ä¸ªè¯­ä¹‰ç±»åˆ«ï¼Œä»¥åŒ…å«åœ¨ SceneParse150 æŒ‘æˆ˜\[54\] çš„è¯„ä¼°ä¸­ã€‚è°ƒæ•´å›¾åƒå¤§å°ï¼Œä½¿æœ€çŸ­è¾¹ä¸å¤§äº 512 åƒç´ ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å›¾åƒçš„è¾ƒçŸ­è¾¹è°ƒæ•´ä¸ºç›¸åº”çš„è£å‰ªå¤§å°ã€‚

COCO-Stuff-10KÂ \[[3](#bib.bib3)\] has 171 semantic-level categories. There are 9k images for training and 1k images for testing. Images in the COCO-Stuff-10K datasets are a subset of the COCO datasetÂ \[[28](#bib.bib28)\]. During inference, we resize the shorter side of the image to the corresponding crop size.  
COCO-Stuff-10K \[3\] æœ‰ 171 ä¸ªè¯­ä¹‰çº§ç±»åˆ«ã€‚æœ‰ 9k å›¾åƒç”¨äºè®­ç»ƒï¼Œ1k å›¾åƒç”¨äºæµ‹è¯•ã€‚COCO-Stuff-10K æ•°æ®é›†ä¸­çš„å›¾åƒæ˜¯ COCO æ•°æ®é›†çš„ä¸€ä¸ªå­é›†\[28\]ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å›¾åƒçš„è¾ƒçŸ­è¾¹è°ƒæ•´ä¸ºç›¸åº”çš„è£å‰ªå¤§å°ã€‚

ADE20K-FullÂ \[[55](#bib.bib55)\] contains 25k images for training and 2k images for validation. The ADE20K-Full dataset is annotated in an open-vocabulary setting with more than 3000 semantic categories. We filter these categories by selecting those that are present in both training and validation sets, resulting in a total of 847 categories. We follow the same process as ADE20K-SceneParse150 to resize images such that the shortest side is no greater than 512 pixels. During inference, we resize the shorter side of the image to the corresponding crop size.  
ADE20K-Full \[55\] åŒ…å«ç”¨äºè®­ç»ƒçš„ 25k å›¾åƒå’Œç”¨äºéªŒè¯çš„ 2k å›¾åƒã€‚ADE20K-Full æ•°æ®é›†åœ¨å…·æœ‰ 3000 å¤šä¸ªè¯­ä¹‰ç±»åˆ«çš„å¼€æ”¾è¯æ±‡è®¾ç½®ä¸­è¿›è¡Œæ³¨é‡Šã€‚æˆ‘ä»¬é€šè¿‡é€‰æ‹©è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­å­˜åœ¨çš„ç±»åˆ«æ¥è¿‡æ»¤è¿™äº›ç±»åˆ«ï¼Œä»è€Œå¾—å‡ºæ€»å…± 847 ä¸ªç±»åˆ«ã€‚æˆ‘ä»¬éµå¾ªä¸ ADE20K-SceneParse150 ç›¸åŒçš„è¿‡ç¨‹æ¥è°ƒæ•´å›¾åƒå¤§å°ï¼Œä½¿æœ€çŸ­è¾¹ä¸å¤§äº 512 åƒç´ ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å›¾åƒçš„è¾ƒçŸ­è¾¹è°ƒæ•´ä¸ºç›¸åº”çš„è£å‰ªå¤§å°ã€‚

CityscapesÂ \[[15](#bib.bib15)\] is an urban egocentric street-view dataset with high-resolution images (1024Ã—2048102420481024\\times 2048 pixels). It contains 2975 images for training, 500 images for validation, and 1525 images for testing with a total of 19 classes. During training, we use a crop size of 512Ã—10245121024512\\times 1024, a batch size of 16 and train all models for 90k iterations. During inference, we operate on the whole image (1024Ã—2048102420481024\\times 2048).  
Cityscapes \[15\] æ˜¯ä¸€ä¸ªä»¥åŸå¸‚è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è¡—æ™¯æ•°æ®é›†ï¼Œå…·æœ‰é«˜åˆ†è¾¨ç‡å›¾åƒï¼ˆ 1024Ã—2048102420481024\\times 2048 åƒç´ ï¼‰ã€‚å®ƒåŒ…å« 2975 å¼ ç”¨äºè®­ç»ƒçš„å›¾åƒã€500 å¼ ç”¨äºéªŒè¯çš„å›¾åƒå’Œ 1525 å¼ ç”¨äºæµ‹è¯•çš„å›¾åƒï¼Œå…± 19 ä¸ªç±»ã€‚åœ¨è®­ç»ƒæœŸé—´ï¼Œæˆ‘ä»¬ä½¿ç”¨ 512Ã—10245121024512\\times 1024 è£å‰ªå¤§å° ï¼Œæ‰¹å¤„ç†å¤§å°ä¸º 16ï¼Œå¹¶è®­ç»ƒæ‰€æœ‰æ¨¡å‹è¿›è¡Œ 90k è¿­ä»£ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯¹æ•´ä¸ªå›¾åƒ ï¼ˆ 1024Ã—2048102420481024\\times 2048 ï¼‰ è¿›è¡Œæ“ä½œã€‚

Mapillary VistasÂ \[[34](#bib.bib34)\] is a large-scale urban street-view dataset with 65 categories. It contains 18k, 2k, and 5k images for training, validation and testing with a variety of image resolutions, ranging from 1024Ã—76810247681024\\times 768 to 4000Ã—6000400060004000\\times 6000. During training, we resize the short side of images to 2048 before applying scale augmentation. We use a crop size of 1280Ã—1280128012801280\\times 1280, a batch size of 161616 and train all models for 300k iterations. During inference, we resize the longer side of the image to 2048 and only use three scales (0.5, 1.0 and 1.5) for multi-scale testing due to GPU memory constraints.  
Mapillary Vistas \[34\] æ˜¯ä¸€ä¸ªåŒ…å« 65 ä¸ªç±»åˆ«çš„å¤§è§„æ¨¡åŸå¸‚è¡—æ™¯æ•°æ®é›†ã€‚å®ƒåŒ…å« 18kã€2k å’Œ 5k å›¾åƒï¼Œç”¨äºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•ï¼Œå…·æœ‰å„ç§å›¾åƒåˆ†è¾¨ç‡ï¼ŒèŒƒå›´ 1024Ã—76810247681024\\times 768 4000Ã—6000400060004000\\times 6000 ä» .åœ¨è®­ç»ƒæœŸé—´ï¼Œæˆ‘ä»¬å°†å›¾åƒçš„çŸ­è¾¹è°ƒæ•´ä¸º 2048ï¼Œç„¶åå†åº”ç”¨æ¯”ä¾‹å¢å¼ºã€‚æˆ‘ä»¬ä½¿ç”¨ è£ 1280Ã—1280128012801280\\times 1280 å‰ªå¤§å° ï¼Œæ‰¹å¤„ç†å¤§å° ï¼Œ 161616 å¹¶è®­ç»ƒæ‰€æœ‰æ¨¡å‹è¿›è¡Œ 300k è¿­ä»£ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç”±äº GPU å†…å­˜é™åˆ¶ï¼Œæˆ‘ä»¬å°†å›¾åƒçš„è¾ƒé•¿è¾¹è°ƒæ•´ä¸º 2048ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨ä¸‰ä¸ªæ¯”ä¾‹ï¼ˆ0.5ã€1.0 å’Œ 1.5ï¼‰è¿›è¡Œå¤šæ¯”ä¾‹æµ‹è¯•ã€‚

### A.2 Panoptic segmentation datasets  

A.2 å…¨æ™¯åˆ†å‰²æ•°æ®é›†

COCO panopticÂ \[[24](#bib.bib24)\] is one of the most commonly used datasets for panoptic segmentation. It has 133 categories (80 â€œthingâ€ categories with instance-level annotation and 53 â€œstuffâ€ categories) in 118k images for training and 5k images for validation. All images are from the COCO datasetÂ \[[28](#bib.bib28)\].  
COCO panoptic \[24\] æ˜¯æœ€å¸¸ç”¨çš„å…¨æ™¯åˆ†å‰²æ•°æ®é›†ä¹‹ä¸€ã€‚å®ƒæœ‰ 133 ä¸ªç±»åˆ«ï¼ˆ80 ä¸ªå…·æœ‰å®ä¾‹çº§æ³¨é‡Šçš„â€œäº‹ç‰©â€ç±»åˆ«å’Œ 53 ä¸ªâ€œä¸œè¥¿â€ç±»åˆ«ï¼‰ï¼Œå…¶ä¸­ 118k å›¾åƒç”¨äºè®­ç»ƒï¼Œ5k å›¾åƒç”¨äºéªŒè¯ã€‚æ‰€æœ‰å›¾åƒå‡æ¥è‡ª COCO æ•°æ®é›†\[28\]ã€‚

ADE20K panopticÂ \[[55](#bib.bib55)\] combines the ADE20K semantic segmentation annotation for semantic segmentation from the SceneParse150 challengeÂ \[[54](#bib.bib54)\] and ADE20K instance annotation from the COCO+Places challengeÂ \[[1](#bib.bib1)\]. Among the 150 categories, there are 100 â€œthingâ€ categories with instance-level annotation. We find filtering masks with a lower threshold (we use 0.7 for ADE20K) than COCO (which uses 0.8) gives slightly better performance.  
ADE20K å…¨æ™¯\[55\] ç»“åˆäº†æ¥è‡ª SceneParse150 æŒ‘æˆ˜\[54\] çš„ ADE20K è¯­ä¹‰åˆ†å‰²æ³¨é‡Šå’Œæ¥è‡ª COCO+Places æŒ‘æˆ˜\[1\] çš„ ADE20K å®ä¾‹æ³¨é‡Šã€‚åœ¨ 150 ä¸ªç±»åˆ«ä¸­ï¼Œæœ‰ 100 ä¸ªå…·æœ‰å®ä¾‹çº§æ³¨é‡Šçš„â€œäº‹ç‰©â€ç±»åˆ«ã€‚æˆ‘ä»¬å‘ç°é˜ˆå€¼æ›´ä½ï¼ˆADE20K ä½¿ç”¨ 0.7ï¼‰çš„æ»¤æ³¢æ©ç æ¯” COCOï¼ˆä½¿ç”¨ 0.8ï¼‰çš„æ€§èƒ½ç•¥å¥½ã€‚

Table I: Semantic segmentation on ADE20K test with 150 categories. MaskFormer outperforms previous state-of-the-art methods on all three metrics: pixel accuracy (P.A.), mIoU, as well as the final test score (average of P.A. and mIoU). We train our model on the union of ADE20K train and val set with ImageNet-22K pre-trained checkpoint followingÂ \[[29](#bib.bib29)\] and use multi-scale inference.  
è¡¨ Iï¼šADE20K æµ‹è¯•çš„ 150 ä¸ªç±»åˆ«çš„è¯­ä¹‰åˆ†å‰² MaskFormer åœ¨æ‰€æœ‰ä¸‰ä¸ªæŒ‡æ ‡ä¸Šéƒ½ä¼˜äºä»¥å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼šåƒç´ ç²¾åº¦ï¼ˆPAï¼‰ã€mIoU ä»¥åŠæœ€ç»ˆæµ‹è¯•åˆ†æ•°ï¼ˆPA å’Œ mIoU çš„å¹³å‡å€¼ï¼‰ã€‚æˆ‘ä»¬åœ¨ ADE20K è®­ç»ƒå’Œ val é›†çš„å¹¶é›†ä¸Šè®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¹¶éµå¾ª ImageNet-22K é¢„è®­ç»ƒæ£€æŸ¥ç‚¹\[29\]ï¼Œå¹¶ä½¿ç”¨å¤šå°ºåº¦æ¨ç†ã€‚

| method | backbone | P.A. | mIoU | score |

| SETRÂ \[[53](#bib.bib53)\] å¡ç‰¹ \[53\] | ViT-L | 78.35 | 45.03 | 61.69 |

| Swin-UperNetÂ \[[29](#bib.bib29), [49](#bib.bib49)\] æ–¯æ¸© - ä¹Œç€ç½‘ \[29ï¼Œ 49\] | Swin-L | 78.42 | 47.07 | 62.75 |

| MaskFormer (ours)MaskFormerï¼ˆæˆ‘ä»¬çš„ï¼‰ | Swin-L | 79.36 | 49.67 | 64.51 |

Table II: Semantic segmentation on COCO-Stuff-10K test with 171 categories and ADE20K-Full val with 847 categories. TableÂ [IIa](#A1.T2.sf1 "In Table II â€£ A.2 Panoptic segmentation datasets â€£ Appendix A Datasets description â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"): MaskFormer is competitive on COCO-Stuff-10K, showing the generality of mask-classification. TableÂ [IIb](#A1.T2.sf2 "In Table II â€£ A.2 Panoptic segmentation datasets â€£ Appendix A Datasets description â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"): MaskFormer results on the harder large-vocabulary semantic segmentation. MaskFormer performs better than per-pixel classification and requires less memory during training, thanks to decoupling the number of masks from the number of classes. mIoU (s.s.) and mIoU (m.s.) are the mIoU of single-scale and multi-scale inference with Â±plus-or-minus\\pmstd.  
è¡¨äºŒï¼šCOCO-Stuff-10K æ£€éªŒï¼ˆ171 ä¸ªç±»åˆ«ï¼‰å’Œ ADE20K-Full valï¼ˆ847 ä¸ªç±»åˆ«ï¼‰çš„è¯­ä¹‰åˆ†å‰²è¡¨ IIaï¼šMaskFormer åœ¨ COCO-Stuff-10K ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œæ˜¾ç¤ºäº†æ©æ¨¡åˆ†ç±»çš„é€šç”¨æ€§ã€‚è¡¨ IIbï¼šMaskFormer å¯¹è¾ƒéš¾çš„å¤§è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„ç»“æœã€‚MaskFormer çš„æ€§èƒ½ä¼˜äºæ¯åƒç´ åˆ†ç±»ï¼Œå¹¶ä¸”ç”±äºå°†æ©ç æ•°é‡ä¸ç±»æ•°è§£è€¦ï¼Œå› æ­¤åœ¨è®­ç»ƒæœŸé—´éœ€è¦æ›´å°‘çš„å†…å­˜ã€‚mIoU ï¼ˆs.s.ï¼‰ å’Œ mIoU ï¼ˆm.s.ï¼‰ æ˜¯å•å°ºåº¦å’Œå¤šå°ºåº¦æ¨ç†çš„ Â±plus-or-minus\\pm mIoUã€‚

| method | backbone | mIoU (s.s.)mIoU ï¼ˆs.s.ï¼‰ | mIoU (m.s.)mIoU ï¼ˆç¡•å£«ï¼‰ |

| OCRNetÂ \[[50](#bib.bib50)\] | R101c | \- Â±plus-or-minus\\pm0.5 | 39.5 Â±plus-or-minus\\pm0.5 |

| PerPixelBaseline | 0R50c | 32.4 Â±plus-or-minus\\pm0.2 | 34.4 Â±plus-or-minus\\pm0.4 |

| PerPixelBaseline+ | 0R50c | 34.2 Â±plus-or-minus\\pm0.2 | 35.8 Â±plus-or-minus\\pm0.4 |

| MaskFormer (ours)MaskFormerï¼ˆæˆ‘ä»¬çš„ï¼‰ | 0R50c | 37.1 Â±plus-or-minus\\pm0.4 | 38.9 Â±plus-or-minus\\pm0.2 |

| R101c | 38.1  Â±plus-or-minus\\pm0.3 | 39.8  Â±plus-or-minus\\pm0.6 |

| R101c | 38.0 Â±plus-or-minus\\pm0.3 | 39.3 Â±plus-or-minus\\pm0.4 |

(a) COCO-Stuff-10K.ï¼ˆä¸€ï¼‰COCO- ä¸œè¥¿ -10Kã€‚

| mIoU (s.s.)mIoU ï¼ˆs.s.ï¼‰ | training memory è®­ç»ƒè®°å¿† |

| \- Â±plus-or-minus\\pm0.5 | - |

| 12.4 Â±plus-or-minus\\pm0.2 | 08030M |

| 13.9 Â±plus-or-minus\\pm0.1 | 26698M |

| 16.0 Â±plus-or-minus\\pm0.3 | 06529M |

| 16.8 Â±plus-or-minus\\pm0.2 | 06894M |

| 17.4  Â±plus-or-minus\\pm0.4 | 06904M |

(b) ADE20K-Full.ï¼ˆäºŒï¼‰ADE20K- å…¨ã€‚

Table III: Semantic segmentation on Cityscapes val with 19 categories. [IIIa](#A1.T3.sf1 "In Table III â€£ A.2 Panoptic segmentation datasets â€£ Appendix A Datasets description â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"): MaskFormer is on-par with state-of-the-art methods on Cityscapes which has fewer categories than other considered datasets. We report multi-scale (m.s.) inference results with Â±plus-or-minus\\pmstd for a fair comparison across methods. [IIIb](#A1.T3.sf2 "In Table III â€£ A.2 Panoptic segmentation datasets â€£ Appendix A Datasets description â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"): We analyze MaskFormer with a complimentary PQStSt{}^{\\text{St}} metric, by treating all categories as â€œstuff.â€ The breakdown of PQStSt{}^{\\text{St}} suggests mask classification-based MaskFormer is better at recognizing regions (RQStSt{}^{\\text{St}}) while slightly lagging in generation of high-quality masks (SQStSt{}^{\\text{St}}).  
è¡¨ä¸‰ï¼š19 ä¸ªç±»åˆ«çš„åŸå¸‚æ™¯è§‚è¯­ä¹‰ç»†åˆ† IIIaï¼šMaskFormer ä¸ Cityscapes ä¸Šæœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼Œå…¶ç±»åˆ«æ¯”å…¶ä»–è€ƒè™‘çš„æ•°æ®é›†å°‘ã€‚æˆ‘ä»¬ç”¨ Â±plus-or-minus\\pm std æŠ¥å‘Šäº†å¤šå°ºåº¦ ï¼ˆm.s.ï¼‰ æ¨ç†ç»“æœï¼Œä»¥ä¾¿å¯¹ä¸åŒæ–¹æ³•è¿›è¡Œå…¬å¹³æ¯”è¾ƒã€‚IIIbï¼šæˆ‘ä»¬ä½¿ç”¨å…è´¹çš„ PQ StSt{}^{\\text{St}} æŒ‡æ ‡æ¥åˆ†æ MaskFormerï¼Œå°†æ‰€æœ‰ç±»åˆ«è§†ä¸ºâ€œä¸œè¥¿â€ã€‚PQ StSt{}^{\\text{St}} çš„ç»†åˆ†è¡¨æ˜ï¼ŒåŸºäºæ©æ¨¡åˆ†ç±»çš„ MaskFormer åœ¨è¯†åˆ«åŒºåŸŸï¼ˆRQ StSt{}^{\\text{St}} ï¼‰æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œè€Œåœ¨é«˜è´¨é‡æ©æ¨¡ï¼ˆSQ StSt{}^{\\text{St}} ï¼‰çš„ç”Ÿæˆæ–¹é¢ç•¥æœ‰æ»åã€‚

| method | backbone | mIoU (m.s.)mIoU ï¼ˆç¡•å£«ï¼‰ |

| Panoptic-DeepLabÂ \[[11](#bib.bib11)\] | X71Â \[[12](#bib.bib12)\] | 81.5 Â±plus-or-minus\\pm0.2 |

| OCRNetÂ \[[50](#bib.bib50)\] | R101c | 82.0  Â±plus-or-minus\\pm0.2 |

| MaskFormer (ours)MaskFormerï¼ˆæˆ‘ä»¬çš„ï¼‰ | R101c | 80.3 Â±plus-or-minus\\pm0.1 |

| R101c | 81.4 Â±plus-or-minus\\pm0.2 |

(a) Cityscapes standard mIoU metric.  
ï¼ˆä¸€ï¼‰Cityscapes æ ‡å‡† mIoU æŒ‡æ ‡ã€‚

| PQStSt{}^{\\text{St}} (m.s.)PQ StSt{}^{\\text{St}} ï¼ˆç¡•å£«ï¼‰ | SQStSt{}^{\\text{St}} (m.s.)SQ StSt{}^{\\text{St}} ï¼ˆç¡•å£«ï¼‰ | RQStSt{}^{\\text{St}} (m.s.)RQ StSt{}^{\\text{St}} ï¼ˆç¡•å£«ï¼‰ |

| 66.6 | 82.9 | 79.4 |

| 66.1 | 82.6 | 79.1 |

| 65.9 | 81.5 | 79.7 |

| 66.9 | 82.0 | 80.5 |

(b) Cityscapes analysis with PQStSt{}^{\\text{St}} metric suit.  
ï¼ˆäºŒï¼‰ä½¿ç”¨ PQ StSt{}^{\\text{St}} å…¬åˆ¶å¥—è£…è¿›è¡ŒåŸå¸‚æ™¯è§‚åˆ†æã€‚

Table IV: Semantic segmentation on Mapillary Vistas val with 65 categories. MaskFormer outperforms per-pixel classification methods on high-resolution images without the need of multi-scale inference, thanks to global context captured by the Transformer decoder. mIoU (s.s.) and mIoU (m.s.) are the mIoU of single-scale and multi-scale inference.  
è¡¨å››ï¼š65 ä¸ªç±»åˆ«çš„ Mapillary Vistas val çš„è¯­ä¹‰ç»†åˆ†ã€‚MaskFormer åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒä¸Šä¼˜äºæ¯åƒç´ åˆ†ç±»æ–¹æ³•ï¼Œè€Œæ— éœ€å¤šå°ºåº¦æ¨ç†ï¼Œè¿™è¦å½’åŠŸäº Transformer è§£ç å™¨æ•è·çš„å…¨å±€ä¸Šä¸‹æ–‡ã€‚mIoU ï¼ˆs.s.ï¼‰ å’Œ mIoU ï¼ˆm.s.ï¼‰ æ˜¯å•å°ºåº¦å’Œå¤šå°ºåº¦æ¨ç†çš„ mIoUã€‚

| method | backbone | mIoU (s.s.)mIoU ï¼ˆs.s.ï¼‰ | mIoU (m.s.)mIoU ï¼ˆç¡•å£«ï¼‰ |

| DeepLabV3+Â \[[9](#bib.bib9)\]DeepLabV3+ï¼ˆæ·±å®éªŒå®¤ V3+ï¼‰ \[9\] | R50 | 47.7 | 49.4 |

| HMSANetÂ \[[38](#bib.bib38)\] | R50 | - | 52.2 |

| MaskFormer (ours)MaskFormerï¼ˆæˆ‘ä»¬çš„ï¼‰ | R50 | 53.1 | 55.4 |

Table V: Panoptic segmentation on COCO panoptic test-dev with 133 categories. MaskFormer outperforms previous state-of-the-art Max-DeepLabÂ \[[42](#bib.bib42)\] on the test-dev set as well. We only train our model on the COCO train2017 set with ImageNet-22K pre-trained checkpoint.  
è¡¨äº”ï¼šCOCO å…¨æ™¯æµ‹è¯•å¼€å‘çš„å…¨æ™¯ç»†åˆ†ï¼Œ133 ä¸ªç±»åˆ«ã€‚MaskFormer åœ¨æµ‹è¯•å¼€å‘é›†ä¸Šä¹Ÿä¼˜äºä»¥å‰æœ€å…ˆè¿›çš„ Max-DeepLab \[42\]ã€‚æˆ‘ä»¬åªåœ¨å¸¦æœ‰ ImageNet-22K é¢„è®­ç»ƒæ£€æŸ¥ç‚¹çš„ COCO train2017 é›†ä¸Šè®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚

| method | backbone | PQ | PQThTh{}^{\\text{Th}}é¤å‰ ThTh{}^{\\text{Th}} | PQStSt{}^{\\text{St}}é¤å‰ StSt{}^{\\text{St}} | SQ | RQ |

| Max-DeepLabÂ \[[42](#bib.bib42)\] é©¬å…‹æ–¯æ·±åº¦å®éªŒå®¤ \[42\] | Max-L | 51.3 | 57.2 | 42.4 | 82.5 | 61.3 |

| MaskFormer (ours)MaskFormerï¼ˆæˆ‘ä»¬çš„ï¼‰ | Swin-L | 53.3 | 59.1 | 44.5 | 82.0 | 64.1 |

Table VI: Panoptic segmentation on ADE20K panoptic val with 150 categories. Following DETRÂ \[[4](#bib.bib4)\], we add 6 additional Transformer encoders when using ResNetÂ \[[22](#bib.bib22)\] (R50 + 6 Enc and R101 + 6 Enc) backbones. MaskFormer achieves competitive results on ADE20K panotic, showing the generality of our model for panoptic segmentation.  
è¡¨ VIï¼šADE20K å…¨æ™¯å€¼çš„å…¨æ™¯åˆ†å‰²ï¼Œ150 ä¸ªç±»åˆ«ã€‚ç»§ DETR \[4\] ä¹‹åï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨ ResNet \[22\]ï¼ˆR50 + 6 Enc å’Œ R101 + 6 Encï¼‰ä¸»å¹²ç½‘æ—¶å¢åŠ äº† 6 ä¸ªé¢å¤–çš„ Transformer ç¼–ç å™¨ã€‚MaskFormer åœ¨ ADE20K å…¨æ™¯ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œæ˜¾ç¤ºäº†æˆ‘ä»¬çš„å…¨æ™¯åˆ†å‰²æ¨¡å‹çš„é€šç”¨æ€§ã€‚

| method | backbone | PQ | PQThTh{}^{\\text{Th}}é¤å‰ ThTh{}^{\\text{Th}} | PQStSt{}^{\\text{St}}é¤å‰ StSt{}^{\\text{St}} | SQ | RQ |

| BGRNetÂ \[[47](#bib.bib47)\]BGRNet çš„ \[47\] | R50 | 31.8 | - | - | - | - |

| Auto-PanopticÂ \[[48](#bib.bib48)\] è‡ªåŠ¨å…¨æ™¯ \[48\] | ShuffleNetV2Â \[[32](#bib.bib32)\] éšæœºç½‘ç»œ V2 \[32\] | 32.4 | - | - | - | - |

| MaskFormer (ours)MaskFormerï¼ˆæˆ‘ä»¬çš„ï¼‰ | 0R50 + 6 Enc | 34.7 | 32.2 | 39.7 | 76.7 | 42.8 |

| R101 + 6 EncR101 + 6 æ©åŠ  | 35.7 | 34.5 | 38.0 | 77.4 | 43.8 |

Appendix B Semantic segmentation results  
é™„å½• B è¯­ä¹‰åˆ†å‰²ç»“æœ
-----------------------------------------------------

ADE20K test. TableÂ [I](#A1.T1 "Table I â€£ A.2 Panoptic segmentation datasets â€£ Appendix A Datasets description â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation") compares MaskFormer with previous state-of-the-art methods on the ADE20K test set. FollowingÂ \[[29](#bib.bib29)\], we train MaskFormer on the union of ADE20K train and val set with ImageNet-22K pre-trained checkpoint and use multi-scale inference. MaskFormer outperforms previous state-of-the-art methods on all three metrics with a large margin.  
ADE20K æµ‹è¯•ã€‚è¡¨ I å°† MaskFormer ä¸ ADE20K æµ‹è¯•è£…ç½®ä¸Šä»¥å‰æœ€å…ˆè¿›çš„æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æŒ‰ç…§\[29\]ï¼Œæˆ‘ä»¬åœ¨ ADE20K è®­ç»ƒå’Œ val é›†ä¸ ImageNet-22K é¢„è®­ç»ƒæ£€æŸ¥ç‚¹çš„ç»“åˆä¸Šè®­ç»ƒ MaskFormerï¼Œå¹¶ä½¿ç”¨å¤šå°ºåº¦æ¨ç†ã€‚MaskFormer åœ¨æ‰€æœ‰ä¸‰ä¸ªæŒ‡æ ‡ä¸Šéƒ½æ¯”ä»¥å‰æœ€å…ˆè¿›çš„æ–¹æ³•æ›´èƒœä¸€ç­¹ã€‚

COCO-Stuff-10K. TableÂ [IIa](#A1.T2.sf1 "In Table II â€£ A.2 Panoptic segmentation datasets â€£ Appendix A Datasets description â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation") compares MaskFormer with our baselines as well as the state-of-the-art OCRNet modelÂ \[[50](#bib.bib50)\] on the COCO-Stuff-10KÂ \[[3](#bib.bib3)\] dataset. MaskFormer outperforms our per-pixel classification baselines by a large margin and achieves competitive performances compared to OCRNet. These results demonstrate the generality of the MaskFormer model.  
COCO- ä¸œè¥¿ -10Kã€‚è¡¨ IIa å°† MaskFormer ä¸æˆ‘ä»¬çš„åŸºçº¿ä»¥åŠ COCO-Stuff-10K \[3\] æ•°æ®é›†ä¸Šæœ€å…ˆè¿›çš„ OCRNet æ¨¡å‹\[50\] è¿›è¡Œäº†æ¯”è¾ƒã€‚ä¸ OCRNet ç›¸æ¯”ï¼ŒMaskFormer çš„æ€§èƒ½è¿œè¿œè¶…è¿‡æˆ‘ä»¬çš„æ¯åƒç´ åˆ†ç±»åŸºå‡†ï¼Œå¹¶å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœè¯æ˜äº† MaskFormer æ¨¡å‹çš„é€šç”¨æ€§ã€‚

ADE20K-Full. We further demonstrate the benefits in large-vocabulary semantic segmentation in TableÂ [IIb](#A1.T2.sf2 "In Table II â€£ A.2 Panoptic segmentation datasets â€£ Appendix A Datasets description â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"). Since we are the first to report performance on this dataset, we only compare MaskFormer with our per-pixel classification baselines. MaskFormer not only achieves better performance, but is also more memory efficient on the ADE20K-Full dataset with 847 categories, thanks to decoupling the number of masks from the number of classes. These results show that our MaskFormer has the potential to deal with real-world segmentation problems with thousands of categories.  
ADE20K- å…¨ã€‚æˆ‘ä»¬åœ¨è¡¨ IIb ä¸­è¿›ä¸€æ­¥è¯æ˜äº†å¤§è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„å¥½å¤„ã€‚ç”±äºæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªæŠ¥å‘Šæ­¤æ•°æ®é›†æ€§èƒ½çš„å…¬å¸ï¼Œå› æ­¤æˆ‘ä»¬ä»…å°† MaskFormer ä¸æ¯åƒç´ åˆ†ç±»åŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚MaskFormer ä¸ä»…å®ç°äº†æ›´å¥½çš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨åŒ…å« 847 ä¸ªç±»åˆ«çš„ ADE20K-Full æ•°æ®é›†ä¸Šä¹Ÿå…·æœ‰æ›´é«˜çš„å†…å­˜æ•ˆç‡ï¼Œè¿™è¦å½’åŠŸäºæ©ç æ•°é‡ä¸ç±»æ•°çš„è§£è€¦ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ MaskFormer å…·æœ‰å¤„ç†æ•°åƒä¸ªç±»åˆ«çš„å®é™…ç»†åˆ†é—®é¢˜çš„æ½œåŠ›ã€‚

Cityscapes. In TableÂ [IIIa](#A1.T3.sf1 "In Table III â€£ A.2 Panoptic segmentation datasets â€£ Appendix A Datasets description â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"), we report MaskFormer performance on Cityscapes, the standard testbed for modern semantic segmentation methods. The dataset has only 19 categories and therefore, the recognition aspect of the dataset is less challenging than in other considered datasets. We observe that MaskFormer performs on par with the best per-pixel classification methods. To better analyze MaskFormer, in TableÂ [IIIb](#A1.T3.sf2 "In Table III â€£ A.2 Panoptic segmentation datasets â€£ Appendix A Datasets description â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"), we further report PQStSt{}^{\\text{St}}. We find MaskFormer performs better in terms of recognition quality (RQStSt{}^{\\text{St}}) while lagging in per-pixel segmentation quality (SQStSt{}^{\\text{St}}). This suggests that on datasets, where recognition is relatively easy to solve, the main challenge for mask classification-based approaches is pixel-level accuracy.  
åŸå¸‚æ™¯è§‚ã€‚åœ¨è¡¨ IIIa ä¸­ï¼Œæˆ‘ä»¬æŠ¥å‘Šäº† MaskFormer åœ¨ Cityscapes ä¸Šçš„æ€§èƒ½ï¼ŒCityscapes æ˜¯ç°ä»£è¯­ä¹‰åˆ†å‰²æ–¹æ³•çš„æ ‡å‡†æµ‹è¯•å¹³å°ã€‚è¯¥æ•°æ®é›†åªæœ‰ 19 ä¸ªç±»åˆ«ï¼Œå› æ­¤ï¼Œä¸å…¶ä»–è€ƒè™‘çš„æ•°æ®é›†ç›¸æ¯”ï¼Œæ•°æ®é›†çš„è¯†åˆ«æ–¹é¢æ›´å…·æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ° MaskFormer çš„æ€§èƒ½ä¸æœ€ä½³çš„æ¯åƒç´ åˆ†ç±»æ–¹æ³•ç›¸å½“ã€‚ä¸ºäº†æ›´å¥½åœ°åˆ†æ MaskFormerï¼Œåœ¨è¡¨ IIIb ä¸­ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æŠ¥å‘Šäº† PQ StSt{}^{\\text{St}} ã€‚æˆ‘ä»¬å‘ç° MaskFormer åœ¨è¯†åˆ«è´¨é‡ï¼ˆRQ StSt{}^{\\text{St}} ï¼‰æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œè€Œåœ¨æ¯åƒç´ åˆ†å‰²è´¨é‡ï¼ˆSQ StSt{}^{\\text{St}} ï¼‰æ–¹é¢è½åã€‚è¿™è¡¨æ˜ï¼Œåœ¨è¯†åˆ«ç›¸å¯¹å®¹æ˜“è§£å†³çš„æ•°æ®é›†ä¸Šï¼ŒåŸºäºæ©ç åˆ†ç±»çš„æ–¹æ³•çš„ä¸»è¦æŒ‘æˆ˜æ˜¯åƒç´ çº§ç²¾åº¦ã€‚

Mapillary Vistas. TableÂ [IV](#A1.T4 "Table IV â€£ A.2 Panoptic segmentation datasets â€£ Appendix A Datasets description â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation") compares MaskFormer with state-of-the-art per-pixel classification models on the high-resolution Mapillary Vistas dataset which contains images up to 4000Ã—6000400060004000\\times 6000 resolution. We observe: (1) MaskFormer is able to handle high-resolution images, and (2) MaskFormer outperforms mulit-scale per-pixel classification models even without the need of mult-scale inference. We believe the Transformer decoder in MaskFormer is able to capture global context even for high-resolution images.  
Mapillary Vistasï¼ˆçŠ¶è¿œæ™¯ï¼‰ã€‚è¡¨ IV å°† MaskFormer ä¸é«˜åˆ†è¾¨ç‡ Mapillary Vistas æ•°æ®é›†ä¸Šæœ€å…ˆè¿›çš„æ¯åƒç´ åˆ†ç±»æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«é«˜è¾¾ 4000Ã—6000400060004000\\times 6000 åˆ†è¾¨ç‡çš„å›¾åƒã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼šï¼ˆ1ï¼‰ MaskFormer èƒ½å¤Ÿå¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œä»¥åŠ ï¼ˆ2ï¼‰ å³ä½¿ä¸éœ€è¦å¤šå°ºåº¦æ¨ç†ï¼ŒMaskFormer ä¹Ÿä¼˜äºå¤šå°ºåº¦æ¯åƒç´ åˆ†ç±»æ¨¡å‹ã€‚æˆ‘ä»¬ç›¸ä¿¡ MaskFormer ä¸­çš„ Transformer è§£ç å™¨èƒ½å¤Ÿæ•è·å…¨å±€ä¸Šä¸‹æ–‡ï¼Œå³ä½¿æ˜¯é«˜åˆ†è¾¨ç‡å›¾åƒä¹Ÿæ˜¯å¦‚æ­¤ã€‚

Appendix C Panoptic segmentation results  
é™„å½• CPanoptic åˆ†å‰²ç»“æœ
------------------------------------------------------------

COCO panoptic test-dev. TableÂ [V](#A1.T5 "Table V â€£ A.2 Panoptic segmentation datasets â€£ Appendix A Datasets description â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation") compares MaskFormer with previous state-of-the-art methods on the COCO panoptic test-dev set. We only train our model on the COCO train2017 set with ImageNet-22K pre-trained checkpoint and outperforms previos state-of-the-art by 2 PQ.  
COCO å…¨æ™¯æµ‹è¯•å¼€å‘è¡¨ V å°† MaskFormer ä¸ä»¥å‰åœ¨ COCO å…¨æ™¯æµ‹è¯•å¼€å‘é›†ä¸Šæœ€å…ˆè¿›çš„æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬åªåœ¨å¸¦æœ‰ ImageNet-22K é¢„è®­ç»ƒæ£€æŸ¥ç‚¹çš„ COCO train2017 é›†ä¸Šè®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¹¶ä¸”æ€§èƒ½ä¼˜äº previos æœ€å…ˆè¿›çš„ 2 PQã€‚

ADE20K panoptic. We demonstrate the generality of our model for panoptic segmentation on the ADE20K panoptic dataset in TableÂ [VI](#A1.T6 "Table VI â€£ A.2 Panoptic segmentation datasets â€£ Appendix A Datasets description â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"), where MaskFormer is competitive with the state-of-the-art methods.  
ADE20K å…¨æ™¯å…‰å­¦ã€‚æˆ‘ä»¬åœ¨è¡¨ VI ä¸­æ¼”ç¤ºäº† ADE20K å…¨æ™¯æ•°æ®é›†ä¸Šå…¨æ™¯åˆ†å‰²æ¨¡å‹çš„é€šç”¨æ€§ï¼Œå…¶ä¸­ MaskFormer ä¸æœ€å…ˆè¿›çš„æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚

Appendix D Additional ablation studies  
é™„å½• Ddditional æ¶ˆèç ”ç©¶
-----------------------------------------------------------

We perform additional ablation studies of MaskFormer for semantic segmentation using the same setting as that in the main paper: a single ResNet-50 backboneÂ \[[22](#bib.bib22)\], and we report both the mIoU and the PQStSt{}^{\\text{St}}. The default setting of our MaskFormer is: 100 queries and 6 Transformer decoder layers.  
æˆ‘ä»¬ä½¿ç”¨ä¸ä¸»è¦è®ºæ–‡ç›¸åŒçš„è®¾ç½®å¯¹ MaskFormer è¿›è¡Œé¢å¤–çš„æ¶ˆèç ”ç©¶ï¼Œä»¥è¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼šå•ä¸ª ResNet-50 éª¨å¹² \[22\]ï¼Œæˆ‘ä»¬æŠ¥å‘Šäº† mIoU å’Œ PQ StSt{}^{\\text{St}} ã€‚MaskFormer çš„é»˜è®¤è®¾ç½®æ˜¯ï¼š100 ä¸ªæŸ¥è¯¢å’Œ 6 ä¸ª Transformer è§£ç å™¨å±‚ã€‚

Table VII: Inference strategies for semantic segmentation. _general:_ general inference (SectionÂ [3.4](#S3.SS4 "3.4 Mask-classification inference â€£ 3 From Per-Pixel to Mask Classification â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation")) which first filters low-confidence masks (using a threshold of 0.3) and assigns labels to the remaining ones. _semantic:_ the default semantic inference (SectionÂ [3.4](#S3.SS4 "3.4 Mask-classification inference â€£ 3 From Per-Pixel to Mask Classification â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation")) for semantic segmentation.  
è¡¨ä¸ƒï¼šè¯­ä¹‰åˆ†å‰²çš„æ¨ç†ç­–ç•¥ã€‚generalï¼šä¸€èˆ¬æ¨ç†ï¼ˆç¬¬ 3.4 èŠ‚ï¼‰ï¼Œé¦–å…ˆè¿‡æ»¤ä½ç½®ä¿¡åº¦æ©ç ï¼ˆä½¿ç”¨é˜ˆå€¼ 0.3ï¼‰å¹¶å°†æ ‡ç­¾åˆ†é…ç»™å…¶ä½™æ©ç ã€‚semanticï¼šè¯­ä¹‰åˆ†å‰²çš„é»˜è®¤è¯­ä¹‰æ¨ç†ï¼ˆç¬¬ 3.4 èŠ‚ï¼‰ã€‚

|  | ADE20K (150 classes)ADE20Kï¼ˆ150 èŠ‚è¯¾ï¼‰ | COCO-Stuff (171 classes)COCO-Stuff ï¼ˆ171 ç±»ï¼‰ | ADE20K-Full (847 classes)  
ADE20K-Full ï¼ˆ847 ç±»ï¼‰ |

| inference | mIoU | PQStSt{}^{\\text{St}}é¤å‰ StSt{}^{\\text{St}} | SQStSt{}^{\\text{St}}å¹³æ–¹ StSt{}^{\\text{St}} | RQStSt{}^{\\text{St}}RQï¼ˆè‹±è¯­ï¼šRQï¼‰ StSt{}^{\\text{St}} | mIoU | PQStSt{}^{\\text{St}}é¤å‰ StSt{}^{\\text{St}} | SQStSt{}^{\\text{St}}å¹³æ–¹ StSt{}^{\\text{St}} | RQStSt{}^{\\text{St}}RQï¼ˆè‹±è¯­ï¼šRQï¼‰ StSt{}^{\\text{St}} | mIoU | PQStSt{}^{\\text{St}}é¤å‰ StSt{}^{\\text{St}} | SQStSt{}^{\\text{St}}å¹³æ–¹ StSt{}^{\\text{St}} | RQStSt{}^{\\text{St}}RQï¼ˆè‹±è¯­ï¼šRQï¼‰ StSt{}^{\\text{St}} |

| PerPixelBaseline+ | 41.9 | 28.3 | 71.9 | 36.2 | 34.2 | 24.6 | 62.6 | 31.2 | 13.9 | 09.0 | 24.5 | 12.0 |

| general | 42.4 | 34.2 | 74.4 | 43.5 | 35.5 | 29.7 | 66.3 | 37.0 | 15.1 | 11.6 | 28.3 | 15.3 |

| semantic | 44.5 | 33.4 | 75.4 | 42.4 | 37.1 | 28.9 | 66.3 | 35.9 | 16.0 | 11.9 | 28.6 | 15.7 |

Table VIII: Ablation on number of Transformer decoder layers in MaskFormer. We find that MaskFormer with only one Transformer decoder layer is already able to achieve reasonable semantic segmentation performance. Stacking more decoder layers mainly improves the recognition quality.  
è¡¨ VIIIï¼šMaskFormer ä¸­ Transformer è§£ç å™¨å±‚æ•°çš„çƒ§èš€ã€‚æˆ‘ä»¬å‘ç°ï¼Œåªæœ‰ä¸€ä¸ª Transformer è§£ç å™¨å±‚çš„ MaskFormer å·²ç»èƒ½å¤Ÿå®ç°åˆç†çš„è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚å †å æ›´å¤šçš„è§£ç å±‚ä¸»è¦æé«˜è¯†åˆ«è´¨é‡ã€‚

|  | ADE20K-Semantic | ADE20K-Panoptic |

| \# of decoder layers\# è§£ç å™¨å±‚æ•° | mIoU | PQStSt{}^{\\text{St}}é¤å‰ StSt{}^{\\text{St}} | SQStSt{}^{\\text{St}}å¹³æ–¹ StSt{}^{\\text{St}} | RQStSt{}^{\\text{St}}RQï¼ˆè‹±è¯­ï¼šRQï¼‰ StSt{}^{\\text{St}} | PQ | PQThTh{}^{\\text{Th}}é¤å‰ ThTh{}^{\\text{Th}} | PQStSt{}^{\\text{St}}é¤å‰ StSt{}^{\\text{St}} | SQ | RQ |

| 6 (PerPixelBaseline+)6 ï¼ˆPerPixelBaseline+ï¼‰ | 41.9 | 28.3 | 71.9 | 36.2 | - | - | - | - | - |

| 1 | 43.0 | 31.1 | 74.3 | 39.7 | 31.9 | 29.6 | 36.6 | 76.6 | 39.6 |

| 6 | 44.5 | 33.4 | 75.4 | 42.4 | 34.7 | 32.2 | 39.7 | 76.7 | 42.8 |

| 6 (no self-attention)6ï¼ˆæ— è‡ªæˆ‘å…³æ³¨ï¼‰ | 44.6 | 32.8 | 74.5 | 41.5 | 32.6 | 29.9 | 38.2 | 75.6 | 40.4 |

| MaskFormer trained for semantic segmentation  
MaskFormer ç»è¿‡è¯­ä¹‰åˆ†å‰²è®­ç»ƒ | MaskFormer trained for panoptic segmentation  
MaskFormer æ¥å—è¿‡å…¨æ™¯åˆ†å‰²è®­ç»ƒ |

| ground truth åœ°é¢å®å†µ | prediction | ground truth åœ°é¢å®å†µ | prediction |

| ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/figure1_jpg/gt.jpg)

 | ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/figure1_jpg/dt.jpg)

 | ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/figure1_jpg/gt_pan_seg.jpg)

 | ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/figure1_jpg/dt_pan_seg.jpg)

 |

| semantic query prediction  
è¯­ä¹‰æŸ¥è¯¢é¢„æµ‹ | panoptic query prediction  
å…¨æ™¯æŸ¥è¯¢é¢„æµ‹ |

| ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/figure1_jpg/query_62_label_20_score_1.0.jpg)

 | ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/figure1_jpg/pan_seg_query_29_label_20_score_0.96.jpg)

 | ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/figure1_jpg/pan_seg_query_78_label_20_score_1.0.jpg)

 |

Figure I: Visualization of â€œsemanticâ€ queries and â€œpanopticâ€ queries. Unlike the behavior in a MaskFormer model trained for panoptic segmentation (right), a single query is used to capture multiple instances in a MaskFormer model trained for semantic segmentation (left). Our model has the capacity to adapt to different types of tasks given different ground truth annotations.  
å›¾ Iï¼šâ€œè¯­ä¹‰â€æŸ¥è¯¢å’Œâ€œå…¨æ™¯â€æŸ¥è¯¢çš„å¯è§†åŒ–ã€‚ä¸ä¸ºå…¨æ™¯åˆ†å‰²è®­ç»ƒçš„ MaskFormer æ¨¡å‹ä¸­çš„è¡Œä¸ºï¼ˆå³å›¾ï¼‰ä¸åŒï¼Œå•ä¸ªæŸ¥è¯¢ç”¨äºæ•è·ä¸ºè¯­ä¹‰åˆ†å‰²è®­ç»ƒçš„ MaskFormer æ¨¡å‹ä¸­çš„å¤šä¸ªå®ä¾‹ï¼ˆå·¦å›¾ï¼‰ã€‚æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿé€‚åº”ä¸åŒç±»å‹çš„ä»»åŠ¡ï¼Œç»™å®šä¸åŒçš„åœ°é¢å®å†µæ³¨é‡Šã€‚

Inference strategies. In TableÂ [VII](#A4.T7 "Table VII â€£ Appendix D Additional ablation studies â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"), we ablate inference strategies for mask classification-based models performing semantic segmentation (discussed in SectionÂ [3.4](#S3.SS4 "3.4 Mask-classification inference â€£ 3 From Per-Pixel to Mask Classification â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation")). We compare our default semantic inference strategy and the general inference strategy which first filters out low-confidence masks (a threshold of 0.3 is used) and assigns the class labels to the remaining masks. We observe 1) general inference is only slightly better than the PerPixelBaseline+ in terms of the mIoU metric, and 2) on multiple datasets the general inference strategy performs worse in terms of the mIoU metric than the default semantic inference. However, the general inference has higher PQStSt{}^{\\text{St}}, due to better recognition quality (RQStSt{}^{\\text{St}}). We hypothesize that the filtering step removes false positives which increases the RQStSt{}^{\\text{St}}. In contrast, the semantic inference aggregates mask predictions from multiple queries thus it has better mask quality (SQStSt{}^{\\text{St}}). This observation suggests that semantic and instance-level segmentation can be unified with a single inference strategy (_i.e_., our general inference) and _the choice of inference strategy largely depends on the evaluation metric instead of the task_.  
æ¨ç†ç­–ç•¥ã€‚åœ¨è¡¨ VII ä¸­ï¼Œæˆ‘ä»¬æ¶ˆèäº†åŸºäºæ©ç åˆ†ç±»çš„æ¨¡å‹çš„æ¨ç†ç­–ç•¥ï¼Œè¿™äº›æ¨¡å‹æ‰§è¡Œè¯­ä¹‰åˆ†å‰²ï¼ˆåœ¨ç¬¬ 3.4 èŠ‚ä¸­è®¨è®ºï¼‰ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†é»˜è®¤çš„è¯­ä¹‰æ¨ç†ç­–ç•¥å’Œé€šç”¨æ¨ç†ç­–ç•¥ï¼Œåè€…é¦–å…ˆè¿‡æ»¤æ‰ä½ç½®ä¿¡åº¦æ©ç ï¼ˆä½¿ç”¨é˜ˆå€¼ 0.3ï¼‰ï¼Œå¹¶å°†ç±»æ ‡ç­¾åˆ†é…ç»™å‰©ä½™çš„æ©ç ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ° 1ï¼‰ å°± mIoU æŒ‡æ ‡è€Œè¨€ï¼Œä¸€èˆ¬æ¨ç†ä»…ç•¥ä¼˜äº PerPixelBaseline+ï¼Œä»¥åŠ 2ï¼‰ åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼Œä¸€èˆ¬æ¨ç†ç­–ç•¥åœ¨ mIoU æŒ‡æ ‡æ–¹é¢çš„è¡¨ç°æ¯”é»˜è®¤è¯­ä¹‰æ¨ç†å·®ã€‚ç„¶è€Œï¼Œç”±äºæ›´å¥½çš„è¯†åˆ«è´¨é‡ï¼ˆRQ StSt{}^{\\text{St}} ï¼‰ï¼Œä¸€èˆ¬æ¨ç†å…·æœ‰æ›´é«˜çš„ PQ StSt{}^{\\text{St}} ã€‚æˆ‘ä»¬å‡è®¾è¿‡æ»¤æ­¥éª¤æ¶ˆé™¤äº†è¯¯æŠ¥ï¼Œä»è€Œå¢åŠ äº† RQ StSt{}^{\\text{St}} ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè¯­ä¹‰æ¨ç†èšåˆäº†æ¥è‡ªå¤šä¸ªæŸ¥è¯¢çš„æ©ç é¢„æµ‹ï¼Œå› æ­¤å®ƒå…·æœ‰æ›´å¥½çš„æ©ç è´¨é‡ ï¼ˆSQ StSt{}^{\\text{St}} ï¼‰ã€‚è¿™ä¸€è§‚å¯Ÿç»“æœè¡¨æ˜ï¼Œè¯­ä¹‰å’Œå®ä¾‹çº§åˆ†å‰²å¯ä»¥ä¸å•ä¸€çš„æ¨ç†ç­–ç•¥ï¼ˆå³æˆ‘ä»¬çš„ä¸€èˆ¬æ¨ç†ï¼‰ç»Ÿä¸€èµ·æ¥ï¼Œæ¨ç†ç­–ç•¥çš„é€‰æ‹©åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºè¯„ä¼°æŒ‡æ ‡è€Œä¸æ˜¯ä»»åŠ¡ã€‚

Number of Transformer decoder layers. In TableÂ [VIII](#A4.T8 "Table VIII â€£ Appendix D Additional ablation studies â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation"), we ablate the effect of the number of Transformer decoder layers on ADE20KÂ \[[55](#bib.bib55)\] for both semantic and panoptic segmentation. Surprisingly, we find a MaskFormer with even a single Transformer decoder layer already performs reasonably well for semantic segmentation and achieves better performance than our 6-layer-decoder per-pixel classification baseline PerPixelBaseline+. Whereas, for panoptic segmentation, the number of decoder layers is more important. We hypothesize that stacking more decoder layers is helpful to de-duplicate predictions which is required by the panoptic segmentation task.  
Transformer è§£ç å™¨å±‚æ•°ã€‚åœ¨è¡¨ VIII ä¸­ï¼Œæˆ‘ä»¬æ¶ˆèäº† Transformer è§£ç å™¨å±‚æ•°å¯¹ ADE20K \[55\] è¯­ä¹‰å’Œå…¨æ™¯åˆ†å‰²çš„å½±å“ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ï¼Œå³ä½¿æ˜¯å•ä¸ª Transformer è§£ç å™¨å±‚çš„ MaskFormer åœ¨è¯­ä¹‰åˆ†å‰²æ–¹é¢çš„è¡¨ç°ä¹Ÿç›¸å½“ä¸é”™ï¼Œå¹¶ä¸”æ¯”æˆ‘ä»¬çš„ 6 å±‚è§£ç å™¨æ¯åƒç´ åˆ†ç±»åŸºçº¿ PerPixelBaseline+ å…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºå…¨æ™¯åˆ†å‰²ï¼Œè§£ç å™¨å±‚çš„æ•°é‡æ›´ä¸ºé‡è¦ã€‚æˆ‘ä»¬å‡è®¾å †å æ›´å¤šçš„è§£ç å™¨å±‚æœ‰åŠ©äºæ¶ˆé™¤é‡å¤é¢„æµ‹ï¼Œè¿™æ˜¯å…¨æ™¯åˆ†å‰²ä»»åŠ¡æ‰€å¿…éœ€çš„ã€‚

To verify this hypothesis, we train MaskFormer models _without_ self-attention in all 6 Transformer decoder layers. On semantic segmentation, we observe MaskFormer without self-attention performs similarly well in terms of the mIoU metric, however, the per-mask metric PQStSt{}^{\\text{St}} is slightly worse. On panoptic segmentation, MaskFormer models without self-attention performs worse across all metrics.  
ä¸ºäº†éªŒè¯è¿™ä¸€å‡è®¾ï¼Œæˆ‘ä»¬åœ¨æ‰€æœ‰ 6 ä¸ª Transformer è§£ç å™¨å±‚ä¸­è®­ç»ƒ MaskFormer æ¨¡å‹ï¼Œè€Œæ— éœ€è‡ªæˆ‘å…³æ³¨ã€‚åœ¨è¯­ä¹‰åˆ†å‰²æ–¹é¢ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ²¡æœ‰è‡ªæˆ‘æ³¨æ„åŠ›çš„ MaskFormer åœ¨ mIoU æŒ‡æ ‡æ–¹é¢è¡¨ç°åŒæ ·å‡ºè‰²ï¼Œä½†æ˜¯ï¼Œæ¯ä¸ªæ©ç æŒ‡æ ‡çš„ PQ StSt{}^{\\text{St}} ç•¥å·®ã€‚åœ¨å…¨æ™¯åˆ†å‰²ä¸Šï¼Œæ²¡æœ‰è‡ªæˆ‘æ³¨æ„åŠ›çš„ MaskFormer æ¨¡å‹åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šçš„è¡¨ç°éƒ½è¾ƒå·®ã€‚

â€œSemanticâ€ queries _vs_. â€œpanopticâ€ queries. In FigureÂ [I](#A4.F1 "Figure I â€£ Appendix D Additional ablation studies â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation") we visualize predictions for the â€œcarâ€ category from MaskFormer trained with semantic-level and instance-level ground truth data. In the case of semantic-level data, the matching cost and loss used for mask prediction force a single query to predict one mask that combines all cars together. In contrast, with instance-level ground truth, MaskFormer uses different queries to make mask predictions for each car. This observation suggests that our model has the capacity to adapt to different types of tasks given different ground truth annotations.  
â€œè¯­ä¹‰â€æŸ¥è¯¢ä¸â€œå…¨æ™¯â€æŸ¥è¯¢ã€‚åœ¨å›¾ I ä¸­ï¼Œæˆ‘ä»¬å¯è§†åŒ–äº†ä½¿ç”¨è¯­ä¹‰çº§å’Œå®ä¾‹çº§åœ°é¢å®å†µæ•°æ®è®­ç»ƒçš„ MaskFormer å¯¹â€œæ±½è½¦â€ç±»åˆ«çš„é¢„æµ‹ã€‚å¯¹äºè¯­ä¹‰çº§æ•°æ®ï¼Œç”¨äºæ©ç é¢„æµ‹çš„åŒ¹é…æˆæœ¬å’ŒæŸå¤±ä¼šå¼ºåˆ¶å•ä¸ªæŸ¥è¯¢é¢„æµ‹ä¸€ä¸ªå°†æ‰€æœ‰æ±½è½¦ç»„åˆåœ¨ä¸€èµ·çš„æ©ç ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¯¹äºå®ä¾‹çº§åœ°é¢äº‹å®ï¼ŒMaskFormer ä½¿ç”¨ä¸åŒçš„æŸ¥è¯¢æ¥å¯¹æ¯è¾†è½¦è¿›è¡Œæ©ç é¢„æµ‹ã€‚è¿™ä¸€è§‚å¯Ÿç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æœ‰èƒ½åŠ›é€‚åº”ä¸åŒç±»å‹çš„ä»»åŠ¡ï¼Œç»™å®šä¸åŒçš„åœ°é¢å®å†µæ³¨é‡Šã€‚

| ground truth åœ°é¢å®å†µ | prediction | ground truth åœ°é¢å®å†µ | prediction |

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00000908_gt.jpg)

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00000908_dt.jpg)

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00001785_gt.jpg)

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00001785_dt.jpg) 

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00001827_gt.jpg)

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00001827_dt.jpg)

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00001831_gt.jpg)

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00001831_dt.jpg) 

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00001795_gt.jpg)

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00001795_dt.jpg)

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00001839_gt.jpg)

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00001839_dt.jpg) 

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00000134_gt.jpg)

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00000134_dt.jpg)

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00001853_gt.jpg)

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00001853_dt.jpg) 

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00000001_gt.jpg)

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00000001_dt.jpg)

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00000939_gt.jpg)

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00000939_dt.jpg) 

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00000485_gt.jpg)

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00000485_dt.jpg)

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00000506_gt.jpg)

 ![](https://ar5iv.labs.arxiv.org/html/2107.06278/assets/figures/visualization/semseg_predictions_jpg/ADE_val_00000506_dt.jpg) 

Figure II: Visualization of MaskFormer semantic segmentation predictions on the ADE20K dataset. We visualize the MaskFormer with Swin-L backbone which achieves 55.6 mIoU (multi-scale) on the validation set. First and third columns: ground truth. Second and fourth columns: prediction.  
å›¾äºŒï¼šADE20K æ•°æ®é›†ä¸Š MaskFormer è¯­ä¹‰åˆ†å‰²é¢„æµ‹çš„å¯è§†åŒ–ã€‚æˆ‘ä»¬å¯è§†åŒ–äº†å¸¦æœ‰ Swin-L ä¸»å¹²çš„ MaskFormerï¼Œå®ƒåœ¨éªŒè¯é›†ä¸Šå®ç°äº† 55.6 mIoUï¼ˆå¤šå°ºåº¦ï¼‰ã€‚ç¬¬ä¸€åˆ—å’Œç¬¬ä¸‰åˆ—ï¼šåœ°é¢å®å†µã€‚ç¬¬äºŒåˆ—å’Œç¬¬å››åˆ—ï¼šé¢„æµ‹ã€‚

## Appendix E Visualization é™„å½• E æ ‡å‡†åŒ–

We visualize sample semantic segmentation predictions of the MaskFormer model with Swin-LÂ \[[29](#bib.bib29)\] backbone (55.6 mIoU) on the ADE20K validation set in FigureÂ [II](#A4.F2 "Figure II â€£ Appendix D Additional ablation studies â€£ Per-Pixel Classification is Not All You Need for Semantic Segmentation").  
æˆ‘ä»¬åœ¨å›¾ II ä¸­çš„ ADE20K éªŒè¯é›†ä¸Šå¯è§†åŒ–äº†å…·æœ‰ Swin-L \[29\] ä¸»å¹²ï¼ˆ55.6 mIoUï¼‰çš„ MaskFormer æ¨¡å‹çš„æ ·æœ¬è¯­ä¹‰åˆ†å‰²é¢„æµ‹ã€‚

## References å¼•ç”¨


*   \[1\] COCO + Places Challenges 2017. [https://places-coco2017.github.io/](https://places-coco2017.github.io/), 2016.  
*   \[2\] Pablo ArbelÃ¡ez, Jordi Pont-Tuset, JonathanÂ T Barron, Ferran Marques, and Jitendra Malik. Multiscale combinatorial grouping. In CVPR, 2014.  
*   \[3\] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. COCO-Stuff: Thing and stuff classes in context. In CVPR, 2018.  
*   \[4\] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.  
*   \[5\] Joao Carreira, Rui Caseiro, Jorge Batista, and Cristian Sminchisescu. Semantic segmentation with second-order pooling. In ECCV, 2012.  
*   \[6\] Joao Carreira and Cristian Sminchisescu. CPMC: Automatic object segmentation using constrained parametric min-cuts. PAMI, 2011.  
*   \[7\] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and AlanÂ L Yuille. DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. PAMI, 2018.  
*   \[8\] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv:1706.05587, 2017.  
*   \[9\] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018.  
*   \[10\] Bowen Cheng, Liang-Chieh Chen, Yunchao Wei, Yukun Zhu, Zilong Huang, Jinjun Xiong, ThomasÂ S Huang, Wen-Mei Hwu, and Honghui Shi. SPGNet: Semantic prediction guidance for scene parsing. In ICCV, 2019.  
*   \[11\] Bowen Cheng, MaxwellÂ D Collins, Yukun Zhu, Ting Liu, ThomasÂ S Huang, Hartwig Adam, and Liang-Chieh Chen. Panoptic-DeepLab: A simple, strong, and fast baseline for bottom-up panoptic segmentation. In CVPR, 2020.  
*   \[12\] FranÃ§ois Chollet. Xception: Deep learning with depthwise separable convolutions. In CVPR, 2017.  
*   \[13\] Dorin Comaniciu and Peter Meer. Robust Analysis of Feature Spaces: Color Image Segmentation. In CVPR, 1997.  
*   \[14\] MMSegmentation Contributors. MMSegmentation: OpenMMLab semantic segmentation toolbox and benchmark. [https://github.com/open-mmlab/mmsegmentation](https://github.com/open-mmlab/mmsegmentation), 2020.  
*   \[15\] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes dataset for semantic urban scene understanding. In CVPR, 2016.  
*   \[16\] Jifeng Dai, Kaiming He, and Jian Sun. Convolutional feature masking for joint object and stuff segmentation. In CVPR, 2015.  
*   \[17\] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, etÂ al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.  
*   \[18\] Mark Everingham, SMÂ Ali Eslami, Luc VanÂ Gool, ChristopherÂ KI Williams, John Winn, and Andrew Zisserman. The PASCAL visual object classes challenge: A retrospective. IJCV, 2015.
*   \[19\] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention network for scene segmentation. In CVPR, 2019.
*   \[20\] Bharath Hariharan, Pablo ArbelÃ¡ez, Ross Girshick, and Jitendra Malik. Simultaneous detection and segmentation. In ECCV, 2014.
*   \[21\] Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick. Mask R-CNN. In ICCV, 2017.
*   \[22\] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.
*   \[23\] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. CCNet: Criss-cross attention for semantic segmentation. In ICCV, 2019.
*   \[24\] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr DollÃ¡r. Panoptic segmentation. In CVPR, 2019.
*   \[25\] Scott Konishi and Alan Yuille. Statistical Cues for Domain Specific Image Segmentation with Performance Analysis. In CVPR, 2000.
*   \[26\] Tsung-Yi Lin, Piotr DollÃ¡r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, 2017.
*   \[27\] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss for dense object detection. In ICCV, 2017.
*   \[28\] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and CÂ Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014.
*   \[29\] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv:2103.14030, 2021.
*   \[30\] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.
*   \[31\] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.
*   \[32\] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. ShuffleNet V2: Practical guidelines for efficient cnn architecture design. In ECCV, 2018.
*   \[33\] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-Net: Fully convolutional neural networks for volumetric medical image segmentation. In 3DV, 2016.
*   \[34\] Gerhard Neuhold, Tobias Ollmann, Samuel RotaÂ BulÃ², and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In CVPR, 2017.
*   \[35\] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, AlexanderÂ C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015.
*   \[36\] Jianbo Shi and Jitendra Malik. Normalized Cuts and Image Segmentation. PAMI, 2000.
*   \[37\] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. arXiv:2105.05633, 2021.
*   \[38\] Andrew Tao, Karan Sapra, and Bryan Catanzaro. Hierarchical multi-scale attention for semantic segmentation. arXiv:2005.10821, 2020.
*   \[39\] Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In ECCV, 2020.
*   \[40\] JasperÂ RR Uijlings, KoenÂ EA Van DeÂ Sande, Theo Gevers, and ArnoldÂ WM Smeulders. Selective search for object recognition. IJCV, 2013.
*   \[41\] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, AidanÂ N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.
*   \[42\] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. MaX-DeepLab: End-to-end panoptic segmentation with mask transformers. In CVPR, 2021.
*   \[43\] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR, 2018.
*   \[44\] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. SOLOv2: Dynamic and fast instance segmentation. NeurIPS, 2020.
*   \[45\] Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.
*   \[46\] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. [https://github.com/facebookresearch/detectron2](https://github.com/facebookresearch/detectron2), 2019.
*   \[47\] Yangxin Wu, Gengwei Zhang, Yiming Gao, Xiajun Deng, Ke Gong, Xiaodan Liang, and Liang Lin. Bidirectional graph reasoning network for panoptic segmentation. In CVPR, 2020.
*   \[48\] Yangxin Wu, Gengwei Zhang, Hang Xu, Xiaodan Liang, and Liang Lin. Auto-panoptic: Cooperative multi-component architecture search for panoptic segmentation. In NeurIPS, 2020.
*   \[49\] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In ECCV, 2018.
*   \[50\] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation. In ECCV, 2020.
*   \[51\] Yuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang, Xilin Chen, and Jingdong Wang. OCNet: Object context for semantic segmentation. IJCV, 2021.
*   \[52\] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, 2017.
*   \[53\] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, PhilipÂ HS Torr, etÂ al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In CVPR, 2021.
*   \[54\] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing challenge 2016. [http://sceneparsing.csail.mit.edu/index_challenge.html](http://sceneparsing.csail.mit.edu/index_challenge.html), 2016.
*   \[55\] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ADE20K dataset. In CVPR, 2017.


# æ”¶è·
## Dice æŸå¤±