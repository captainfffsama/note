#图像配准
[toc]

# GOCor: Bringing Globally Optimized Correspondence Volumes into Your Neural Network
- 文章:<https://readpaper.com/pdf-annotate/note?pdfId=4545181502207057921&noteId=730210243820609536>
- 代码:<https://github.com/PruneTruong/GOCor>
- 会议: NIPS2020

## 摘要
在许多涉及图像对的密集匹配的计算机视觉任务中,特征互相关层是其中的一个关键网络模块.通过计算图像对的局部特征向量的稠密标量积来预测correspondence volume.然而,这种点到点的特征对比在消除图片中多个相似区域的二义性时是不够高效的,这导致了最终性能的下降.我们提出了 GOCor,一个完全可微的稠密匹配模块,可以直接替换特征相关层.我们的模块产生的correspondence volume是一个内部优化过程的结果，明确说明了在场景中类似的区域.此外,我们方法可以有效学习到空间匹配的先验来解决匹配中的二义性.我们还进行了大量的消融实验来分析 GOCor 模块.当在 SOTA 网络中插入这个模块时,在几何匹配,光流估计和稠密语义匹配任务中,其效果都优于特征互相关层.代码参见 <github.com/PruneTruong/GOCor>.

## 1. 引言
寻找图片对中的像素级对应关系是诸如光流估计,几何匹配,差异估计(disparity extimation)等众多视觉任务的基础.最近的一些 SOTA 方法都依赖于特征互相关层来估计两图片的特征表征之间的像素级相似性.其结果是一个四阶张量,用来表示图片局部的匹配置信度.这可以称为光流估计的有效线索.在语义匹配,视频目标分割,few-shot 分割领域,这种稠密 dense correspondences 的封装取得了广泛的成功.因此,在各种重要视觉应用的网络结构设计中,特征互相关层都是一个关键的构建模块.

在特征互相关层中,correspondence volume中每个置信度是通过计算两个特征向量的标量积得到的,特征向量提取自两图片中特定的区域,这里我们将两图片分布称为 reference 图片和 query 图片.然而,为了消除图像中多个相似区域,仅仅依靠郑重点到点的特征对比是远远不够的.如图 1 所示,在图片中存在大量重复模式时,特征互相关层会产生一些难辨别,不准确的匹配置信度(图 1d).这是特征互相关层的一个重要缺点,因为重复图案,少纹理,和出现共同目标在计算机视觉应用中普遍存在.

![GOCor_fig1](../../Attachments/GOCor_fig1.png)
 
我们设计了一个新的稠密匹配模块,旨在通过探索特征相关层未利用的信息来解决上述问题.我们发现特征互相关层生成的 correspondences volume 的置信度仅仅取决于一对 reference 和query 在局部上提出的特征向量.但是 reference 的信息还很可能和 query 其他地方的外观信息很相似,比如场景中有相似区域.另外,特征互相关层会忽略 query 中的一些先验知识和约束,比如对应的唯一性和空间平滑性.本文提出的匹配模块将把上述信息和约束封装到一个可学习的目标函数中.通过最小化这个目标,我们可以前项推理出增强的correspondence volume.这样,我们就可以预测全局优化的对应量,有效的考虑相似图像区域和匹配约束,如图 1e.

**贡献:**
1. 本文模块把在前项推理时最小化自定义匹配目标这一步骤公式化成一个内部优化过程,为有效整合显式可学习的匹配约束提供了一个通用框架.
2. 我们的目标函数很鲁棒,可以聚合场景中的相似区域的信息,使得GOCor模块可以更好的辨别他们.
3. 引入了一个科学系的目标来捕捉query图片的约束和先验信息
4. 我们应用了有效的展开优化(对优化目标进行了求解?),配合准确的初始化,可以保证高效的端到端学习和推理
5. 在几何匹配和光流估计任务上,我们做了广泛的消融实验.实验证明 GOCor 可以在稠密语义匹配任务上具有通用性.另外,GOCor 具有广泛的域适应性.

## 2. 相关工作
**相关量强化(Enhancing the correlation volume):** 由于correspondences volume 的质量是最为重要的因素,因此一些工作专注于使用可学习的后处理技术来提升它.比如,Rocco 等人提出了可训练的邻居共识网络,NCNET,通常将它应用到互相关层之后来过滤模糊的匹配.而本文方法则是直接在互相关操作之前从特征图操作上入手.文献[27]和文献[48]和本文工作有一些联系,文献[27]根据输入动态生成过滤器,文献[48] features updated with an attentional graph neural network, whose edges are defined within the same or the other image of a pair.文献[63]则引入了一个可学习的 cost volume,使得特征可以适应一个椭圆内积点乘空间.

**基于优化的元学习:** 本文工作和基于优化的元学习也有所联系.实际上,GOCor 可以视为内部学习的过程,它解决了我们优化目标中的回归问题.我们受到了文献[5,6]中优化策略极大的启发.从元学习的角度看,我们方法为标准设置提供了有趣的补充.与元学习中实例任务不同,在 few-shot 分类,跟踪任务中,GOCor 是一个大架构中的内部小模块,在元学习训练过程中, GOCor 没法接受到直接的监督信号.通过引入一个可学习的目标函数,我们的可学习模块将充分利用 query 样本.

## 3. 方法
### 3.1 特征互相关层
在许多任务中,特征互相关层是其网络设计中的一个重要模块,用于估计两图像之间的稠密对应关系估计.特征互相关层,计算两张图片局部特征的点积,形成4D 的 _correspondence volume_.考虑 $f^r=\phi(I^r)$ 和 $f^q=\phi(I^q)$ 是网络 $\phi$ 从 reference 图片 $I^r$ 和 query 图片 $I^q$ 提取出的特征张量,其形状为 $H \times W \times D$.我们将空间位置 (i,j) 的特征向量记为 $f_{ij}^r$.那么互相关层计算两张图片之间的点积就是$(f_{ij}^r)^Tf_{kl}^q$.互相关层有两种常见的变体,但是都依赖同样的局部特征点乘操作,但是有一些区别,接下来我们将定义这些操作.

**全局互相关层**
它会评估 reference 和 query 所有位置像素的相似性,公式如下:
$$
C_G(f^r,f^q)_{ijkl}=(f^r_{ij})^Tf_{kl}^q,  (i,j),(k,l) \in \{1,...,H\} \times \{1,...,W\}  \tag{1}
$$
显然 $C_G(f^r,f^q)_{ijkl} \in R^{H \times W \times H \times W}$ 是一个4D张量,它可以表示所有局部特征对之间的相似性.  

**局部互相关层**
这里点乘仅仅涉及 $f^r_{ij}$ 和其在 query $f^q$ 上一小片邻域.
$$
C_L(f^r,f^q)_{ijkl}=(f^r_{ij})^Tf_{i+k,j+l}^q,  (i,j) \in \{1,...,H\} \times \{1,...,W\},,(k,l) \in \{-R,...,R\}^2  \tag{2}
$$
$(k,l)$ 代表相对 reference 中 $(i,j)$ 的偏移,最大值是 $R$.这种限制对于一些大图片也是很有用的,但是它不能捕获超出限制之外的相似性.

### 3.2 动机
互相关层的主要作用是预测两种图片之间的稠密匹配置信度,如公式1和2所示.但是这个操作在建立稠密对应关系时忽略了两个重要点:

**reference 图片信息:**  
匹配置信度$C_L(f^r,f^q)_{ij..} \in R^{H \times W}$ 仅仅考虑 $(i,j)$ 而不考虑其他位置性信息.当 reference 图片中包含多个相似区域,比如重复纹理和纯色区域(如图1),这可能是个大问题.由于 query 通常是和 reference 不同视角或者时间,但具有相同场景,因此这些区域也很可能存在query中.因此,理想情况下,匹配置信度估计应该复用 reference 图片已知的相似性.

**query 图片信息:**
特征互相关层没有利用的第二个信息是可以从 query 中得到的先验和匹配约束.一个重要约束是每个 reference 局部的$f^r_{ij}$ 在 query 中最多只有一个最佳匹配 $f^q_{kl}$.此外,图像对之间的密集匹配通常遵循空间平滑的规律,这是由于底层3D场景的时空连续性导致的.这是一个很强的先验.

综上,我们设计了一个新的稠密匹配模块来充分利用这些信息.

### 3.3 通用形式
本节,我们将构建 GOCor 这是一个端到端可微分的网络模块,可以产生比特征互相关层更加精确的 correspondence volume.我们使用 $w^*$ 替换公式1和2中的 $f^r$,我们将其称为 _过滤器映射(filter map)_.与直接计算 $f^r$ 和$f^q$ 相关性不同,我们首先预测过滤映射$w^*$.通过对 query 特征 $f^q$ 应用 $w^*$ 来得到最终的 correspondence volume $C(w^*,f^q)$.无论是公式1全局相关,还是公式2的局部相关,我们都使用 $C$ 来表示.最终我们通过丰富互相关层的输入来增强互相关层的输出.

接下来我们将专注于讨论如何设计一个合适的过滤映射 $w^*$. 总体上,我们将其设计为一个可微分的函数 $w^*=P_{\theta}(f^r,f^q)$,以 reference 和query 特征作为输入, $\theta$ 是它的可训练参数.比如,简单的让 $P_{\theta}(f^r,f^q)=f^r$ 就可以得到原始的特征互相关层 $C(f^r,f^q)$. 然而设计 3.2节中所描述的,可以充分利用信息的模块并不容易.此外,我们还要求模块可以泛化到新的数据域,即使图片内容和运动模式是未见过的.

我们使用一个目标函数L 来表示上述挑战, 构建网络模块 $P_{\theta}(f^r,f^q)$ 来产生使优化目标最小的过滤映射,
$$
w^*=P_{\theta}(f^r,f^q)=\underset{w}{argmin}L(w;f^r,f^q,\theta)   \tag{3}
$$

这个公式表明了我们可以通过设计目标 L 和合适的优化算法来构建过滤预测模块 $P_{\theta}$.这个框架可以集成 3.2 节中讨论的限制,同时拥有很强的可解释性.在接下来章节中,我们将构建目标函数 L. 3.4 节中,我们将 reference 特征 $f^r$ 集成到优化目标公式 (3) 中. 3.5节,我们将结合 query $f^q$ 的信息来扩展 L. 3.6节,我们将讨论优化方法.图2显示了我们方法的大致结构.

![gocor_fig2](../../Attachments/gocor_fig2.png)

### 3.4 Reference 图像目标
本节,我们引入一个灵活的优化目标来利用 reference 特征 $f^r$ 的全局信息.这里我们不妨沿用公式1的全局相关中的习惯,使用下标来表示空间中的绝对位置.当针对 reference 图片位置(i,j)建立匹配置信度,特征互相关层 $C(f^r,f^q)$ 仅仅利用来位置 (i,j) 的编码特征 $f^r_{ij}$.然而, reference 特征图 $f^r$ 还包含来其他区域(k,l) 中的编码特征 $f^r_{kl}$, query $f^q$ 也存在同样的问题.为了利用这个信息,首先,我们使用了过滤映射 $w$ 来替换特征图 $f^r$. 过滤映射的目的是使对应于 reference 位置 (i,j) 的匹配置信度尽量高 $C(w,f^r)_{ijij}=w^T_{ij}f^r_{ij} \approx 1$,同时保证对于 reference 特征图中其他位置 $(k,l) \neq (i,j)$ 具有低置信度$C(w,f^r)_{ijkl}=w^T_{ij}f^r_{kl} \approx 0$.这样设计的目的是显式的抑制其他和$f^r_{ij}$具有相似外观的区域$f^r_{kl}$,对于 $f^q$ 也有这样的情况.

首先我们尝试 将上述 reference 图像约束通过最小化 $||C(w,f^r) - \delta||^2$. $\delta$ 代表期望相关的响应,具体而言就是在公式1 全局相关中, $(i,j) == (k,l)$ 的地方 $\delta_{ijkl}=1$ 其余地方为0.这个二次优化目标是可用特别有效的优化方法来求解的.但这个简单的二次目标也容易受到异常值影响.在我们的设计中,当不匹配的对产生一个大的负相关输出 $w^T_{ij}f^r_{kl}<<0$,优化目标应该不受影响.这是因为任何零置信度或者负置信度都足矣用来表示不匹配.但是一个过大的负值在二次目标中也会形成一个很大影响,这使得具有类似外观的挑战性区域会降低 correspondence volume 的质量.

基于以上问题,我们设计了一个鲁棒的非线性二次优化目标.即对于不匹配的区域产生一个正的互相关输出达到抑制相似外观的目的,忽略掉负的互相关输出.然后我们对于正负互相关输出引入单独的惩罚权重 $v^+_{ijkl}$和$v^-_{ijkl}$来解释这种不对称性.使用以下函数 $\sigma$ 对置信值进行映射.
$$
\sigma(c;v^+,v^-)=
\begin{cases}
    v^+c, & \text {c>=0} \\
    v^-,  & \text {c<0}
\end{cases}   
\tag{4a}
$$

$$
\sigma_\eta(c;v^+,v^-)=\frac{v^+-v^-}{2} (\sqrt{c^2+\eta^2}-\eta)+\frac{v^++v^-}{2}c  \tag{4b}
$$

这里我们设计来一个平滑的近似 $\sigma_\eta$,当 $\eta>0$可以避免函数$\sigma$在置信度 $C(w,f^r)=0$ 时的不连续.当 $\eta=0$,公式 4b 将退化成 4a.
