# Vision-Language-Action Models: Concepts, Progress, Applications and Challenges

#具身智能 #VLA #综述 

- 论文：[[2505.04769] Vision-Language-Action Models: Concepts, Progress, Applications and Challenges](https://arxiv.org/abs/2505.04769)
- 作者：1. 康奈尔大学 2. 香港科技大学 3. 伯罗奔尼撒大学
## 摘要

本文是 VLA 领域的进出行研究综述。我们首先确立了 VLA 系统的基础概念和演变，梳理了过去三年里 80 多个 VLA 模型。其中包含诸如架构创新，高效参数训练，和实时推理。我们还探讨了 VLA 的多样化应用领域，包括人形机器人，自动驾驶，医疗，农业工业机器人，现实导航等等。我们还进一步探讨了关于实时控制，多模态动作表示，系统可扩展性和新任务泛化，伦理部署等挑战。借由一些先进技术，我们也提出了一些针对性解决方案，涉及自适应的 AI agent，跨具身泛化，统一神经符号规划等等。

## 1. 引言

在 VLA 之前，视觉语言运动控制相互独立发展，这些系统在复杂环境下难以协同工作取得较好效果。

具体而言，如图 1，基于传统 CNN 的计算机视觉模型虽可以“看到”果园中苹果，但是不能对语言进行理解，进而转化为有目的的行为。LLM 虽然彻底改变了文本理解和生成，但是没有能力感知和推理物理世界。依赖手工制订的策略或强化学习的机器人行动系统，虽然能实现特定行为，但是依赖复杂工程无法泛化。

VLM 虽然有较大进展，但是依然不能依据跨模态输入来生成连贯动作。多数 AI 系统最多专注两种模态（视觉语言，视觉动作，语言动作），很少有三者完全整合到统一的端到端框架中。导致具身 AI 依赖随便花的管道架构，泛化差，工程化难度大。这凸显了具身 AI 的一个关键瓶颈：不能联合感知，理解和行动的系统，智能自主行为仍将是一个难以实现的目标。

VLA 模型在 2021-2022 年形成概念，诸如 google 的 RT-2，引入了统一的架构，将感知推理和控制整合到一个框架中。早期的 VLA 方法是将机器人运动命令 token 化，然后扩展到视觉语言模型中。

![](../../Attachments/Vision-Language-Action%20Models_Concepts,%20Progress,%20Applications%20and%20Challenges_fig1.png)

> 图 1：从孤立模态到统一视觉 - 语言 - 动作模型的演变。此图展示了从各自独立的视觉、语言和动作系统（每个系统仅限于其自身领域）到集成 VLA 模型的转变。VLA 模型使机器人能够共同感知、理解语言并采取行动，克服了先前方法的碎片化，标志着向自适应、可泛化和智能具身代理迈出的重大一步。

本文，我们将系统分析 VLA 模型的基础原理，发展进程和技术挑战，期以来巩固对 VLA 的理解，识别起局限性，并为未来发展提供方向。本文将参照图 2 从关键基础概念的详细考察站靠，对 VLA 模型的构成，历史演变，多模态集成机制和基于语言的 token 化和编码策略展开探讨，理解 VLA 是如何在各种模态中进行构建和运作的。

![](../../Attachments/Vision-Language-Action%20Models_Concepts,%20Progress,%20Applications%20and%20Challenges_fig2.png)

> 图 2：VLA 概念思维导图。此图概述了视觉 - 语言 - 行动模型的基础组件，包括其定义、历史发展、多模态信号的整合、标记化技术和自适应执行。它为理解 VLA 的结构和目的奠定了概念基础。

图 3 展示了 VLA 的近期发展，图 4 展示了 VLA 面临的局限性。

![Vision-Language-Action Models_Concepts, Progress, Applications and Challenges_fig3](../../Attachments/Vision-Language-Action%20Models_Concepts,%20Progress,%20Applications%20and%20Challenges_fig3.png)

> 图 3：思维导图 - VLA 进展与训练效率。此图结合了架构进步与训练优化方法，如数据高效学习、参数减少和模型加速。它提供了推动 VLA 可扩展性和实际部署的技术进展的视觉总结。

![](../../Attachments/Vision-Language-Action%20Models_Concepts,%20Progress,%20Applications%20and%20Challenges_fig4.png)

> 图 4：VLA 挑战思维导图。此图展示了稳健 VLA 部署的关键障碍，包括推理限制、偏差、系统复杂性、泛化差距和伦理问题。同时，它也激发了寻求创新解决方案和未来研究方向以克服这些挑战的需求。

## 2. VLA 概念

VLA 模型代表一类新的智能系统，它在动态环境中联合处理视觉输入，自然语言并生成可执行动作。技术上，VLA 结合视觉编码器（ViTs, CNNs），语言模型（LLMs）以及 policy 模块来实验任务条件控制。利用交叉注意力，嵌入拼接，token unification（标记统一），将 observations 和文本指令对齐。

### 2.1 时间线
1. **基础集成（2022-2023）.** 早期视觉 - 语言 - 动作模型通过多模态融合架构建立了基本的视觉运动协调。[ 157] 首次将 CLIP 嵌入与运动基元相结合，而 [ 141] 在 604 个任务中展示了通用能力。[ 18] 通过缩放模仿学习实现了 97% 的操纵成功率，[ 86] 通过基于 transformer 的计划者引入了时间推理。到 2023 年，[ 224] 实现了视觉思维链推理，[ 34] 通过扩散过程推进了随机动作预测。这些基础解决了低级控制问题，但缺乏组合推理 [ 216]，从而促使对能力基础定位的创新 [ 78]。
2. **专业化与具身推理（2024）.** 第二代视觉语言动作模型（VLAs）融入了领域特定的归纳偏差。[202] 通过检索增强训练增强了少样本适应能力，而 [210] 通过 3D 场景图集成优化了导航。[39] 引入了可逆架构以提高内存效率，[183] 利用物理信息注意力解决了部分可观测性问题。同时，[5] 通过以对象为中心的解耦提高了组合理解能力，[220] 通过多模态传感器融合将应用扩展到自动驾驶。这些进展需要新的基准测试方法 [196]。
3. **泛化与安全关键部署（2025）.** 当前系统优先考虑鲁棒性和与人类的对齐。文献 [205] 通过集成形式化验证实现了风险感知决策，而文献 [42] 通过分层视觉语言动作（VLA）展示了全身控制。文献 [19] 优化了嵌入式部署的计算效率，文献 [102] 结合了神经符号推理进行因果推断。像文献 [100] 的适应性链和文献 [13] 的模拟到现实迁移学习这样的新兴范式解决了跨具身化挑战，而文献 [108] 通过自然语言锚定将 VLA 与人类在环接口连接起来。

图 6 展示了一个全面的时序图，突出了 2022 年至 2025 年间开发的 47 个 VLA 模型的演变。最早的 VLA 系统，包括 CLIPort[157]、Gato[141]、RT-1[18] 和 VIMA[86]，通过结合预训练的视觉 - 语言表示与任务条件化的策略来操作和控制，奠定了基础。随后是 ACT[216]、RT-2[224] 和 VoxPoser[78]，它们整合了视觉思维链推理和可及性定位。Diffusion Policy[34] 和 Octo[167] 等模型引入了随机建模和可扩展的数据管道。到 2024 年，Deer-VLA[202]、ReVLA[39] 和 Uni-NaVid[210] 等系统增加了领域专业化和内存高效设计，而 Occllama[183] 和 ShowUI[108] 则解决了部分可观测性和用户交互问题。轨迹继续扩展到以机器人为重点的 VLA，如 Quar-VLA[43] 和 RoboMamba[111]。最近的创新强调了一般化和部署：SafeVLA[205]、Humanoid-VLA[42] 和 MoManipVLA[190] 结合了验证、全身控制和记忆系统。 模型如 Gr00t N1 [13] 和 SpatialVLA [136] 进一步促进了从模拟到现实的迁移和空间定位。这个时间线展示了视觉语言动作（VLA）如何从模块化学习发展到通用、安全和具身智能。

![](../../Attachments/Vision-Language-Action%20Models_Concepts,Progress,Applications%20and%20Challenges_fig6.png.png)

图 6：视觉 - 语言 - 动作模型（2022-2025）综合时间线，展示从基础到 45 个专业 VLA 系统的演变过程。按时间顺序和主题分组组织。

## 2.2 多模态集成：从孤立 pipeline 到统一 agents

传统机器人系统将感知，文本指令理解和控制视为独立模块，通过手动定义接口或数据转换来进行连接。比如经典的 pipeline 要求感知模型输出符号标签，通过规划器（planner）将其映射到特定动作（动作是领域特定的手工设计的）。这类方法泛化差。

而现代 VLA 则使用统一的大规模预训练编码器和 Transformer 架构进行端到端融合。

这种多模态协同最早在 CLIPort 上体现，它使用 CLIP 嵌入进行语义定位，然后使用卷积进行像素级操作。

值得注意的是最近的 Octo，引入了一种记忆增强的 Transformer，使得在多种场景中实现长时域决策成为可能，展示了联合感知 - 语言 - 动作学习的可扩展性。Occllama [183] 通过注意力机制处理遮挡物体引用，而 ShowUI [108] 展示了自然语言界面，允许非专业用户通过语音或键盘输入来指挥代理。

## 2.3 token 化：VLA 编码世界的方式

现代 VLA 使用离散标记来编码世界，这些标记统一了所有模态——视觉、语言、状态和动作到一个共享的嵌入空间。

**前缀 token：** 记录 observation，将其编程 token
**状态 token：** 记录机器人本身配置，包括力矩度数，关节位置，执行器状态等等，甚至可能包括附件物体位置。
**动作 token：** 表示运动控制中的动作