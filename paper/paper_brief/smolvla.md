# smolvla
#具身智能 #VLA

```gate
url: https://www.zhixi.com/embed/3fdd54a4
height: 800
zoomFactor: 1
```


## 模型架构


### 感知


#### 输入


##### 传感器状态


##### 多个 RGB 摄像头图像


##### 任务到语言指令


#### 结构


##### SmolVLM-2


###### 图像


   * SigLIP


   * 为了推理时间，微调仅使用全局图像结合 pixel shuffle，每张图片 token 为 64


###### 语言


   * 分词成 token


###### 投影层


   * 单个 token


### 动作专家


#### 跳层


##### 下游最佳特征不一定是 VLM 最后一层


##### 使用了总层数的一半（16 层）


#### 结构


##### flow matching


###### hidden dim 是 VLM 到 75%


###### 交替 CA 和 SA


   * CA


      * 输入 VLM KV，Q 为动作专家特征


   * SA


      * SA 互相关注动作标记


      * 使用 causal attention mask


      * 经验：自注意力更加有助于平滑动作片段


## 目的


### 建立小巧高效的 vla


## 贡献


### 轻量化关键设计


#### 最少数量视觉 token


#### 小型预训练 VLM


#### 交替使用较轻到跨注意力层 (CA) 和自我注意力层 (SA)


### 基于社区数据集预训练


#### 不到 30000，比之前到少了一个数量级


### 异步推理


#### 常见方式


##### 预测 N 个动作，都执行完再预测 N 个


###### 缺点


   * 开环


##### 每个时间步长都推理，然后重叠的部分加权输出


###### 缺点


   * 慢


#### 本文做法


##### 设置一个待执行动作队列，当队列中待执行动作比例小于阈值，就取一个新的 observation（会和以前老的 observation 做去重，一般用 joint-space）去给到 policy 推理出一个新队列，然后将新旧队列合并起来，作为待执行队列。如果队列空了，就直接取最近的 observation


##### 这里队列阈值的计算公式如：


###### 即 服务器推理时间/帧速率/动作块大小


## 实验


### 真实数据集


#### MetaWorld


##### task 50 个，每个 task 包含 50 个动作序列


#### Knight 相关数据集


##### 每个数据集包含一个任务，任务是从 5 个不同起始位置各含 10 各动作轨迹


### 模拟数据集


#### LIBERO


#### Meta-World


### 训练配方


#### batchsize：256

step：200000

warm_up_step:100

lr:1e-4

lr scheduler: cos final lr 2.5e-6

optimizer：AdamW

img_size:512x512

action chunk:10

冻结 VLM

#### 使用了 4GPU，30k 个 GPU 小时


### 经验


#### CA 和 SA 好，两者交错结合更佳


#### action chunk 里面，动作和动作之间用 causal 注意力比双向好，可以防止未来动作泄露。如果动作和动作之间完全不做注意力也行，对 VLM 特征进行条件化本身也可以取得较好的性能。


#### 在大的 VLM 上隔一层用一次比训练小的 VLM 更好，但是最佳的还是直接使用前一半的层


#### 使用 flow matching 相比使用 L1 回归，要高 5 个点


#### 如果只用 SA，那么 state 输入 action expert 比输入给 vlm 好，如果用 CA，那么 state 输入给 VLM 更好！参见表 11


#### action chunk 10 到 50 大小比较合适


#### 1~10 步采样新观察结果更好（毕竟更加频繁，模型对环境的变化感知就更好）


## 数据集情况


### 来自社区


### 包含了 481 个数据集，共 22.9K 个 episode，10.6M 帧


### 预处理方式


#### 任务描述补全


##### 针对任务描述模糊或者没有的，使用 Qwen2.5-VL-3B-Instruct 生成任务描述


###### 提示方式是抽取代表帧，然后用指令提示生成


#### 相机排序一致


##### 所有相机都按照顶部，手腕，侧面来表示，额外未用到视角删除


## 本文局限


### 数据集机体单一（只有 SO100），多机体训练肯定有益于模型对新机体的泛化


### 数据集太小啦


### 模型体积和效率的平衡还有较大探索空间


### 可以选个更好的 VLM


### 模态太少啦


### smolvla 短时长任务还行，长时长还是考虑分层策略和多级规划机制


### 试试强化学习


