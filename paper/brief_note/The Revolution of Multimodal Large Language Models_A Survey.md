- 地址：<https://arxiv.org/abs/2402.12451>

# 要点

MLLM 一般由视觉编码器，语言模型，以及将视觉嵌入链接到文本嵌入的适配器模块构成。

![](../../Attachments/Pasted%20image%2020241209171104.png)

*Parameter-efficient fine-tuning, PEFT* 该类方法和量化方法不冲突，可以一起用，LoRA 是其中一种。

## 视觉编码器

一般使用冻结的预训练编码器，且通常使用固定宽高比的低分辨率图像。

一个潜在隐患是语言和视觉模型之间参数量不平衡。

一些学者发现使用冻结的视觉模型存在局限。从视觉模型中提取的稠密特征会使得图像信息碎片化，且 token 太长运算量太大。使用可训练的视觉骨干倒是能提高 VQA 等任务表现，但是可能导致其他任务表现下滑，存在知识遗忘和对一般性视觉表示的损害。

## 适配器
### 线性映射或者 MLP

LLaMA-Adapter 仅使用了单层线性层来进行多模态连接, 而 LLaVA-1.5 等人则采用两层 MLP。这种方法虽然简单，但是仍然十分有效。

### Q-Former

由 BLIP-2 提出，利用 transformer 中互相关最终输出形状仅与 Query 形状有关，这样可以压缩图片的 token 数量。

### 额外交叉注意力层

Flamingo 中提出在预训练层中集成跨注意力模块。

## 训练
### 单阶段训练

很多工作会引入一些额外可训练参数，同时冻结语言和视觉模型。

### 两阶段训练
一般是一阶段训练使图像特征和文本嵌入空间对齐，在此之后，输出往往碎片化且不连贯了。第二