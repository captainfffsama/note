# Pure Vision Language Action (VLA) Models: A Comprehensive Survey
- 地址：[[2509.19012] Pure Vision Language Action (VLA) Models: A Comprehensive Survey](https://arxiv.org/abs/2509.19012)

<iframe id="embed_dom" name="embed_dom" frameborder="0" style="display:block;width:600px;height:600px;" src="https://www.zhixi.com/embed/5bc387f5#"></iframe>


## 自回归总结

| 方法 / 模型                                                                    | Year | 自回归模型关键创新                                     |
| -------------------------------------------------------------------------- | ---- | --------------------------------------------- |
| **(A) 自回归通用 VLA 方法论**                                                        |      |                                               |
| Gato [31]                                                                  | 2022 | 具有分词视觉、语言、状态和动作的统一多模态 Transformer。             |
| RT-1 [29]                                                                  | 2022 | 真实世界机器人 Transformer 在 130k 个演示上训练，采用基于 FiLM 的多模态融合。 |
| PaLM-E [32]                                                                | 2023 | 具身多模态 LLM 结合 PaLM 和 ViT，用于 VOA、导航和操作。               |
| RT-2 [33]                                                                  | 2023 | 扩展的 PaLM-E，包含动作 token 和网络规模 VLM 知识，用于开放词汇抓取。       |
| LEO [34]                                                                   | 2024 | 两阶段训练用于 3D 视觉 - 语言对齐和 VLA 微调。                       |
| Octo [35]                                                                  | 2024 | 基于 150 万视频实例的开源跨机器人策略，采用无奖励模仿进行训练。              |
| RUM [36]                                                                   | 2024 | 基于代理的评分，支持鲁棒零件本部署和新共性。                        |
| RoboMV [37]                                                                | 2024 | 多模态融合与模态掩码在 RoboData 上实现 SOTA。                   |
| UP-VLA [38]                                                                | 2025 | 统一视 - 触框架合并任务、视觉和动作以获得更好的小样本性能。                 |
| UniAct [39]                                                                | 2025 | 定义通用原子动作以解决跨本体异构性。                            |
| Humanoid-VLA [40]                                                          | 2025 | 将 VLA 应用于人形控制，结合多视角 RGB 和语言提示。                    |
| NORA [41]                                                                  | 2025 | 轻量级开源 VLA 使用 FAST-tokenizer 和 970k 演示。              |
| OneTwoVLA [42]                                                             | 2025 | 自适应系统 1 & 2 推理用于长时程规划和错误恢复。                     |
| VOTE [43]                                                                  | 2025 | 投票式集成减少动作标记以实现更快推理和训练。                        |
| UniVLA [44]                                                                | 2025 | 从多样化视频中学到任务中心潜在动作表示。                          |
| OF-VLA [45]                                                                | 2025 | 扩展虚拟语言模型以支持开放式多模态指令遵循。                        |
| **(B) 自回归推理和基于 LLMs 的语义规划**                                                  |      |                                               |
| 内心独白 [46]                                                                  | 2022 | 引入了内部语言驱动推理循环，提升了具身任务的成功率。                    |
| Instruct2Act [47]                                                          | 2023 | 提出了具有语义中介的视觉 - 语言 - 任务脚本 - 动作管道。                    |
| RoboFlamingo [48]                                                          | 2023 | 将 OpenFlamingo 适配到机器人学，实现高效的 VLM 到 VLA 迁移。          |
| LLaRA [49]                                                                 | 2024 | 通过对活任务增强轨迹，以提升数据和迁移。                          |
| EGOT [50]                                                                  | 2024 | 将具身思维链推理形式化，以提升 OpenVLA。                       |
| 移动 VLA [51]                                                                 | 2024 | 集成长上下文 VLM 与导航，用于多模态指令跟随。                       |
| UAV-VLA [52]                                                               | 2025 | 使用基于 VLM 的规则从卫星图像生成 UAV 任务。                       |
| 触觉 -VLA [53]                                                                | 2025 | 组合触觉反馈与推理，用于精确自适应控制。                          |
| Shake-VLA [54]                                                             | 2025 | 引入了多模态双臂系统，集成了语音、视觉和触觉。                       |
| I1 Robot [55]                                                              | 2025 | 采用分层粗到精规划，用于长指令跟踪。                            |
| DexGraspVLA [56]                                                           | 2025 | 结合 VLM 规划与扩散，实现鲁棒灵巧抓取。                          |
| HAMSTER [57]                                                               | 2025 | 分层 VLA 利用域外数据进行广泛泛化。                            |
| CoT-VLA [58]                                                               | 2025 | 实现了具有预测目标的视觉思维链推理。                            |
| Gemini Robotics [59]                                                       | 2025 | 构建了基于 Gemini 2.0 的平台，用于鲁棒的多任务具身推理。              |
| CognitiveDrone [60]                                                        | 2025 | 开发了认知无人机 VLA，在空中基准测试中取得高成功率。                   |
| π0.5 [61]                                                                  | 2025 | 在异构机器人上训练以实现开放世界泛化。                           |
| InSpire [62]                                                               | 2025 | 引入空间推理提示以减少虚假关联。                              |
| **(C) Autoregressive Trajectory Generation and Visual Alignment Modeling** |      |                                               |
| LATTE [63]                                                                 | 2022 | 使用 Transformer 解码器将自然语言映射到运动轨迹。                 |
| VIMA [64]                                                                  | 2023 | 统一多模态标记用于语言、视觉和动作，具有强大的零样本泛化能力。               |
| InstructRL [65]                                                            | 2023 | 使用视觉 - 语言联合编码器和策略 Transformer 以实现更好的对齐。           |
| CrossFormer [66]                                                           | 2024 | 可扩展的 Transformer 策略处理多主体数据以实现泛化。                |
| GR-1 [67]                                                                  | 2024 | 通过视频预训练迁移实现时空建模的统一视频和动作标记。                    |
| GR-2 [68]                                                                  | 2024 | 扩展 GR-1，具有大规模视频 - 语言监督，用于零样本操作。                  |
| OpenVLA [21]                                                               | 2024 | 发布 70B 开源 VLA，基于 970k 轨迹训练，超越 RT-2-X。               |
| Bi-VLA [69]                                                                | 2024 | 采用双臂预测器进行协调双手操作。                              |
| RoboNurse-VLA [70]                                                         | 2024 | 在医疗环境中使用 VLA 策略实现了高精度手术抓取。                      |
| Moto [71]                                                                  | 2025 | 使用运动语言标记连接视频预训练和执行。                           |
| OpenDriveVLA [72]                                                          | 2025 | 将 2D/3D 感知对齐到统一语义空间以用于驾驶轨迹。                     |
| GraspVLA [73]                                                              | 2025 | 预训练的 GPT 风格解码器与 GraspVerse 中真实世界抓取迁移。             |
| RaceVLA [74]                                                               | 2025 | 生成的视觉 - 语言轨迹用于动态无人机高速控制。                        |
| VTLLA [75]                                                                 | 2025 | 融合视觉 - 触觉输入，偏好优化 >90% 未见过成功的。                    |
| PointVLA [76]                                                              | 2025 | 将点云注入预测 VLA，用于轻量级三维推理。                         |
| WorldVLA [30]                                                              | 2025 | 通过联合视觉 - 动作建模减轻召回误差传播。                          |
| TraceVLA [77]                                                              | 2025 | 使用视觉轨迹提示来捕捉长时间任务中的线索。                         |
| Interleave-VLA [78]                                                        | 2025 | 支持交错图像 - 文本输入以提升零样本操作。                          |
| MoMani-VLA [79]                                                            | 2025 | 统一基座 - 手臂轨迹对齐用于移动操作转移。                          |
| **(D) 结构优化和 Efficient VLA 中的高效推理机制**                                         |      |                                               |
| HIP [80]                                                                   | 2023 | 引入了分层规划与 token 预测，用于长时程操作。                      |
| Acera [81]                                                                 | 2024 | 优化 Transformer，结合轨迹注意力和可学习查询以降低开销。             |
| Decr-VLA [82]                                                              | 2024 | 多出口架构支持早期退出以减少延迟。                             |
| FAST [22]                                                                  | 2025 | 生成可变长度动作 token 以实现高效长时程执行。                      |
| SpatialVLA [83]                                                            | 2025 | 利用像素网格和空间注意力增强几何感知。                           |
| VLA-Cache [84]                                                             | 2025 | 重用 Transformer 键值状态，通过自适应缓存提高效率。                |
| 超越视觉 [85]                                                                  | 2025 | 运言引导感知适应提升了多模态传感器融合。                          |
| MoE-VLA [86]                                                               | 2025 | 专家混合路由器支持动态跳过，将成本降低 40%。                       |
| PD-VLA [87]                                                                | 2025 | 应用并行定点解码以加速推理，无需重新训练。                         |
| BiVLA [88]                                                                 | 2025 | 使用 1 位量化，将内存减少至少 30% 同时保持性能。                      |
| GR-MG [89]                                                                 | 2025 | 利用扩散生成的目标进行平衡监督自回归学习。                         |
| LoHoVLA [90]                                                               | 2025 | 统一分层控制用于超长时程闭环任务。                             |
| OTTER [91]                                                                 | 2025 | 使用 TAVE 将语言注入视觉编码以实现更强的对齐。                      |
| ChatVLA [92]                                                               | 2025 | 具有专家路由和分阶段对齐的统一架构以实现可扩展性。                     |

## 扩散总结

| 方法 / 模型                     | Year | 扩散模型关键创新                                   |
| --------------------------- | ---- | ------------------------------------------ |
| **(A) 扩散通用 VLA 方法论**        |      |                                            |
| SE(3)-DiffusionFields [129] | 2023 | 扩展扩散到 SF(3) 位置，学习抓取和运动规划的平滑成本。             |
| UPDP [130]                  | 2023 | 将决策过程建模为以图像为界面和语言指导的视频生成。                  |
| StructDiffusion [131]       | 2023 | 结合扩散和以对象为中心的 Transformer 进行语言引导的 3D 结构创建。  |
| Diffusion Policy [7]        | 2024 | 将动作建模为条件扩散，优于行为克隆。                         |
| 3D Diffuser Actor [132]     | 2024 | 嵌入式 3D 场景，使用条件扩散进行轨迹生成。                    |
| ADVC [133]                  | 2024 | 通过光流和运动重建从视频中学习视觉运动策略。                     |
| RDT-1B [134]                | 2025 | 用于双手操作的大规模扩散模型，具有时间条件。                     |
| TUDP [135]                  | 2025 | 统一扩散通过速度场和动作区分跨时间进行。                       |
| GDP [136]                   | 2025 | 通过历史动作条件和缓存机制改进连贯性。                        |
| DD VLA [137]                | 2025 | 建模离散动作块，使用离散扩散和交叉熵训练。                      |
| TRETL [138]                 | 2025 | 通过跨演示轨迹平均隐藏状态重组任务行为。                       |
| **(B) 基于扩散的多模态架构融合**        |      |                                            |
| SuSIE [139]                 | 2023 | 用于通过目标图像生成实现零样本机器人操作的预训练图像编辑扩散模型。          |
| M-DiT [140]                 | 2024 | 统一多模态标记支持灵活的语言 - 图像 - 位置目标条件。              |
| CogACT [141]                | 2024 | 引入认知模块，通过语义图连接感知和行为。                       |
| PERIA [142]                 | 2024 | 联合微调多模态语言模型和图像编辑模型，用于推理和子目标规划。             |
| Chain-of-Affordance [143]   | 2024 | 将解析任务分解为具有明确的感知 - 动作对的顺序可供性子目标。            |
| π0 [2]                      | 2024 | 将编码视频和语言作为潜在标记，在观察 - 理解 - 执行循环中。           |
| Track2Act [144]             | 2024 | 利用网络视频来预测点轨迹并将它们映射到机器人动作。                  |
| Dita [145]                  | 2025 | 可扩展扩散 Transformer 直接去噪连续动作。                |
| Diffusion-VLA [1]           | 2025 | 通过符号中间层将自生成推理与扩散策略集成。                      |
| ForceVLA [146]              | 2025 | 结合六轴力感知与力感知 MoE 融合，实现实时控制。                 |
| **(C) 基于扩散模型的 VLA 应用优化与部署** |      |                                            |
| NoMaD [147]                 | 2023 | 将目标导向导航和任务无关探索统一于扩散策略中。                    |
| TinyVLA [148]               | 2025 | 使用 LoRA 微调，仅 5% 的可训练参数，有效降低训练成本。           |
| SmolVLA [149]               | 2025 | 在消费级硬件上部署轻量级 VLA，进行异步推理。                   |
| VQ-VLA [?]                  | 2025 | 采用矢量量化分词器来减少模拟到现实的差距。                      |
| DexVLG [150]                | 2025 | 在 DexGraspNet 上训练大规模抓取模型以实现灵巧手抓取，用于零样本灵巧性。 |
| AC-DiT [151]                | 2025 | 采用具有多模态条件的扩散 Transformer 进行适配。             |
| DexVLA [152]                | 2025 | 应用形态学学习进行跨机器人适应。                           |
| MinD [153]                  | 2025 | 结合视觉想象与高维扩散策略。                             |
| Hume [154]                  | 2025 | 集成双系统值引导推理和快速去噪。                           |
| TriVLA [155]                | 2025 | 使用三系统 VLA，具有 36Hz 动态和推理层。                  |
| BYOVLA [156]                | 2025 | 添加运行时干预以增强鲁棒性，无需重新训练。                      |
| DreamVLA [157]              | 2025 | 采用动态自反射循环，具有 CoT、错误标记和专家层。                 |
| GEVRM [158]                 | 2025 | 使用 IMC 原理与文本引导视频生成技术以增强鲁棒性。                |
| DriveMoE [159]              | 2025 | 部署场景/动作专用专家混合架构以实现自动驾驶。                    |
| DreamGen [160]              | 2025 | 生成神经轨迹使人类机器人能够学习新任务。                       |
| EnerVerse [161]             | 2025 | 使用多视角自回归视频扩散预测具身未来。                        |
| VidBot [162]                | 2025 | 使用深度先验从单目视频重建 3D 可供电。                      |
| OFT [114]                   | 2025 | 通过并行解码和分块动作优化微调。                           |
| TrackVLA [163]              | 2025 | 使用共享的 LLM 头部和扩散进行具身视觉跟踪。                   |
| FPS [164]                   | 2025 | 构建用于操作的规模化 3D 基础扩散策略。                      |
| GROOT N1 [165]              | 2025 | 为 humanoid 基础模型提供多模态 Transformer 控制。       |
| ObjectVLA [166]             | 2025 | 支持开放世界对象操作，无需人类演示。                         |
| SwitchVLA [167]             | 2025 | 模型从状态 - 上下文信号中执行感知任务切换。                    |
| A0 [168]                    | 2025 | 可供性感知模型将任务分解为空间可及性和低级执行。                   |
| LangToMo [169]              | 2025 | 预测像素运动作为动作的中间推理。                           |
| Evo-0 [170]                 | 2025 | 通过视觉几何基础模块注入 3D 几何特征。                      |

## 强化学习总结

| 方法                 | Year | 基于强化学习的 VLA 创新                    |
| ------------------ | ---- | --------------------------------- |
| VIP [172]          | 2023 | 从视觉预训练为未见任务生成密集奖励函数。              |
| LIV [173]          | 2023 | 从无动作视频和文本中学习联合视觉 - 语言奖励。          |
| PR2L [174]         | 2024 | 利用 VLM 世界知识与强化学习进行机器人操作。          |
| ALGAE [175]        | 2024 | 引入语言引导的抽象来解释强化学习驱动的行为。            |
| GRAPE [176]        | 2024 | 在轨迹级别上对 VLAs 进行对齐，建模成功和失败的奖励。     |
| RLDG [177]         | 2024 | 使用强化学习生成通用策略微调的训练数据。              |
| ELEMENTAL [178]    | 2025 | 通过基于 VLM 的语义映射从演示中学习奖励代理。         |
| NaVILA [179]       | 2025 | 将多模态 VLA 与强化学习结合，用于复杂地形上的四足机器人导航。 |
| SafeVLA [180]      | 2025 | 约束强化学习对齐，以防止风险行为，同时保持性能。          |
| iRe-VLA [181]      | 2025 | 结合 SFT 稳定性与强化学习探索，以提升 VLA 能力。     |
| ReinboT [182]      | 2025 | 使用强化学习增强机器人 GPT 以最大化奖励和数据质量。      |
| ConRFT [183]       | 2025 | 结合离线 BC、Q 学习和在线一致性以实现安全高效的强化学习。   |
| MoRE [184]         | 2025 | 使用强化学习和混合质量数据集微调扩展四足 VLA。         |
| SimpleVLA-RL [185] | 2025 | 使用一个轨迹进行在线强化学习，采用二元（0/1）奖励。       |
| LeVERB [186]       | 2025 | 将 VLA 推理与强化学习动力学结合，用于人形全身控制。      |
| AutoVLA [187]      | 2025 | 采用思维链推理与强化学习优化，用于自动驾驶。            |
| RPD [188]          | 2025 | 使用基于强化学习的精炼从 VLA 教师蒸馏学生。          |
| RLRC [189]         | 2025 | 通过剪枝、SFT+RL 恢复和量化压缩 VLA。          |
| VLA-RL [190]       | 2025 | 使用在线强化学习和基于视觉语言模型的奖励建模提高鲁棒性。      |
| AutoDrive-R² [191] | 2025 | 通过思维链推理和强化学习自我反思增强自动驾驶。           |
| ReWiND [192]       | 2025 | 使用语言条件化奖励进行小演示的离线强化学习。            |
| Embodied-R1 [193]  | 2025 | 使用强化微调和多任务奖励课程进行训练。               |
| IRL-VLA [194]      | 2025 | 在 VLA 框架内的奖励世界模型中应用逆强化学习。         |
| ThinkAct [195]     | 2025 | 通过强化视觉潜在规划连接推理与执行。                |

## 混合架构

| 方法 | Year | 混合与专用架构中的关键创新 |
|------|------|---------------------------|
| **(A) VLA 中的混合架构和多范式集成** |  |  |
| VRB [197] | 2023 | 从人类视频中学习可供电性，以实现复杂的机器人交互。 |
| YAY Robot [198] | 2024 | 将实时自然语言纠正集成到机器人训练中。 |
| HybridVLA [199] | 2025 | 在一个框架中结合了基于扩散的轨迹与自回归推理。 |
| RationalVLA [200] | 2025 | 通过潜在嵌入将高级推理与低级策略联系起来。 |
| OpenHelix [201] | 2025 | 通过大规模实证研究提供标准化的混合 VLA 设计。 |
| EgoVLA [202] | 2025 | 将人类手/腕/手部动作从视频迁移至机器人动作。 |
| ACTLLM [203] | 2025 | 强制动作一致性，以使感知与可执行动作对齐。 |
| TUDP [135] | 2025 | 构建具有速度和区分信号的时间统一扩散策略。 |
| **(B) VLA 中多模态融合与空间理解的进展** |  |  |
| CLIPort [204] | 2021 | 使用 CLIP 分离“什么”和“哪里”路径，生成动作热图。 |
| 3D-VLA [205] | 2024 | 通过生成式 3D 世界建模整合感知、语言和动作。 |
| ReKep [206] | 2024 | 利用关系键点图进行精确的时空推理。 |
| RoboPoint [207] | 2024 | 预测可供性地图作为下游机器人规划的先验。 |
| GMLLMA [208] | 2024 | 在不同本体和动作空间中适配 MLLMs，用于基于事实的推理。 |
| BridgeVLA [209] | 2025 | 将 3D 观测对齐到 2D 热图，实现样本高效的动作预测。 |
| GeoManip [210] | 2025 | 嵌入几何约束以在不重新训练的情况下泛化动作。 |
| TTF-VLA [211] | 2025 | 融合过去和当前视觉标记，实现无需训练的推理改进。 |
| MemoryVLA [212] | 2025 | 使用感知和认知标记以及记忆库构建工作记忆。 |
| **(C) VLA 中的专用领域适配与应用** |  |  |
| GenAug [213] | 2023 | 使用语义数据增强，以最少真实数据重新定位行为。 |
| ROSIE [214] | 2023 | 应用基于扩散的增强来丰富操作数据集。 |
| CoVLA [215] | 2024 | 为自动驾驶构建大型 VLA 数据集，包含配对指令和轨迹。 |
| LeVERB [186] | 2025 | 为类人机器人运动控制设计分层策略，实现鲁棒的仿真到现实迁移。 |
| Helix [216] | 2024 | 统一类人机器人控制，用于操作、移动和协作。 |
| AutoRT [217] | 2024 | 通过观察 - 推理 - 执行流程，利用基础模型协调机器人舰队。 |
| MoManiVLA [218] | 2025 | 通过航点和运动优化，将固定基座 VLA 适配到移动机器人。 |
| CubeRobot [219] | 2025 | 使用双环 VisionCoT 和记忆流解决魔方，实现高成功率。 |
| EAV-VLA [220] | 2025 | 开发对抗性补丁攻击，以破坏或重定向机器人动作。 |
| **(D) VLA 中的基础模型与大规模训练** |  |  |
| R3M [221] | 2022 | 通过时间对比和视频 - 语言对齐学习紧凑视觉表示。 |
| CACTI [222] | 2023 | 建立可扩展的四阶段管道，用于模拟和现实世界的多任务机器人学习。 |
| VC-1 [223] | 2024 | 研究使用 4000+ 小时视频预训练的 MAE 训练 Transformer 的数据规模。 |
| DROID [224] | 2025 | 提供包含 15 万 + 轨迹、跨越 1000+ 对象和任务的大规模多模态数据集。 |
| ViSA-Flow [225] | 2025 | 在大规模人类 - 物体交互视频的动作流上预训练生成模型。 |
| Fine-tuned GMPs [226] | 2024 | 基准测试通用策略在泛化动作空间和信号上的微调策略。 |
| Robot CoT [227] | 2025 | 引入轻量级思维链推理，用于具身策略，推理速度提升 3 倍。 |
| CAST [228] | 2025 | 通过反事实语言和动作生成增强数据多样性。 |
| RoboBrain [229] | 2025 | 提出统一的具身基础模型，用于感知、推理和规划。 |
| **(E) VLA 模型部署：效率、安全与人机协作** |  |  |
| EdgeVLA [230] | 2024 | 通过移除依赖并使用紧凑型 LLM，实现 6 倍更快推理。 |
| DecR-VLA [231] | 2024 | 基于置信度的早期退出，降低控制成本。 |
| Dual Process VLA [232] | 2024 | 将推理和运动控制分配到双模型以提高效率。 |
| CEED-VLA [233] | 2025 | 通过一致性蒸馏和早期退出，将推理速度提升 4 倍。 |
| RoboMamba [234] | 2024 | 使用轻量级融合，适用于资源受限的部署。 |
| ReVLA [235] | 2025 | 跨视觉领域适应，以提高鲁棒性。 |
| SAFE [236] | 2025 | 主动检测来自内部 VLA 信号的故障。 |
| DyWA [237] | 2025 | 对不确定环境中的自适应控制建模状态。 |
| CrayonRobo [238] | 2025 | 通过以对象为中心的提示，提供可解释的 grounding。 |
| cVLA [239] | 2025 | 通过 2D 航点预测增强仿真到现实迁移。 |
| RTC [240] | 2025 | 支持分块策略的平滑异步执行。 |

## 数据集

# 经验摘要

DeeR‑VLA 等框架能够基于任务复杂度提前终止解码。

Dita 和 Diffusion Transformer Policy 表明，将基于注意力的架构扩展到小动作头之外，可以显著改进连续动作建模。

特定领域的 ForceVLA[146] 设计将力感知视为一等模态，使用力感知的专家混合模型将触觉反馈与视觉语言集成嵌入，显著提高了接触丰富的操作能力

AC‑DiT [151] 等框架通过移动到身体条件化将感知和驱动统一起来

BYOVLA [156] 等轻量级干预策略在推理时动态编辑无关的视觉区域，无需微调

FP3 [164] 引入了一个在 60,000 条轨迹上预训练的大规模 3D 策略模型

ObjectVLA 和 SwitchVLA [166], [167] 展示了开放世界物体操作和执行感知任务切换的可行性，强调了动态环境中的灵活性。同时，LangToMo 和 Evo‑0 [169], [170] 引入了新的中间表示和几何感知插件模块，表明结构化的感知先验可以显著增强跨任务的适应性。

SafeVLA [180] 从安全视角探索视觉语言模型，解决了在开放环境中部署视觉语言模型的风险。它提出了一种约束学习对齐机制，以防止高风险行为，同时保持任务性能。该方法将安全评价网络结合到视觉语言模型架构中，以估计风险等级，并采用约束策略优化 （CPO）框架，在确保安全损失保持在预定义阈值以下的条件下最大化策略奖励。SafeVLA 在多任务测试中显著减少了风险事件——包括操作、导航和处理——特别是在模糊自然语言指令增加策略不确定性的场景中，从而展示了卓越的安全性和稳定性。这项工作为在现实世界应用中部署视觉语言模型提供了基本的安全机制。

CubeRobot 在低和中复杂度任务中实现了近乎完美的成功率，并在高难 度场景中表现出色。

[[2410.01220] Effective Tuning Strategies for Generalist Robot Manipulation Policies]( https://arxiv.org/abs/2410.01220 ) 展示了训练大规模 VLA 基座的经验。

EdgeVLA [230] 消除了末端执行器预测中的自回归依赖，并集成了紧凑语言模型，在性能下降最小的情况下实现了 6×加速

DeeR-VLA 采用动态退出机制，使得置信度阈值达到就终止推理，降低在线控制成本