#manipulation #具身智能 #强化学习 #搬箱子 #alphaXiv 

# Sim-to-Real Learning for Humanoid Box Loco-Manipulation
- 时间：2023.10.5
- 论文：[[2310.03191] Sim-to-Real Learning for Humanoid Box Loco-Manipulation](https://arxiv.org/abs/2310.03191)
# 背景和动机

人形机器人代表了机器人领域最宏伟的目标之一：创造能够在人类环境中以类人灵巧性和移动性操作的机器。尽管在教会机器人独立行走或操纵物体方面取得了显著进展，但运动和操纵的结合——称为“运动 - 操纵”——仍然是一个极具挑战性的问题。这项任务需要同时协调全身运动、平衡维护和精确的物体交互，同时还要管理人形机器人特有的高自由度。

![](../../Attachments/Sim-to-Real%20Learning%20for%20Humanoid%20Box%20Loco-Manipulation_fig1.png)

_图 1：在 Digit 人形机器人上演示的五种学习行为。顶行显示真实世界执行，底行显示相应的模拟训练环境。每种行为都作为单独的策略学习，并且可以转换到其他行为。_

传统的人形机器人运动 - 操纵方法严重依赖基于模型的控制方法，这些方法通常会产生过于保守和缓慢的动作以保证稳定性。其他方法则使用人类演示的模仿学习，但这些方法可能会出现轨迹跟踪误差和泛化能力有限的问题。本文通过首次展示通过模拟到真实强化学习在真实硬件上实现人形机器人运动 - 操纵的完全学习控制器来解决这些限制。

# 技术方法

作者通过将复杂的运动 - 操纵任务分解为五种不同的、可学习的行为来解决：行走（WALK）、站立（STAND）、拾取（PICKUP）、带箱行走（WALKWITHBOX）和带箱站立（STANDWITHBOX）。每种行为都由其自己的策略控制，该策略实现为具有两个 128 维循环隐藏层的 LSTM 神经网络，以 50Hz 运行。

![](../../Attachments/Sim-to-Real%20Learning%20for%20Humanoid%20Box%20Loco-Manipulation_fig2.png)

_图 2：状态转换图，显示五种学习策略如何连接以实现完整的运动 - 操纵任务。箭头表示行为之间允许的转换。_

这些策略将一个所有行为通用的 67 维机器人状态向量作为输入，以及包含相关任务参数的策略特定命令向量。输出包括机器人 20 个驱动自由度的 PD 设定点目标，然后由以 2kHz 运行的低级 PD 控制器执行。

## 奖励函数设计

这项工作的一个关键贡献在于仔细设计了每种行为的奖励函数，特别是 PICKUP 策略。奖励结构遵循以下形式：

$$
R = \sum_i w_i \exp(-r_i)
$$

其中每个 $r_i$ 代表一个特定的成本分量。对于 PICKUP 策略，奖励函数结构为：

$$
R = R_{traj} + R_{box} + R_{stand} + R_{reg}
$$

轨迹奖励 $R_{traj}$ 引导机器人的手通过预定义的路径，该路径包括“接触阶段”（移动到箱子两侧）和“举升阶段”（将箱子移动到目标位置）。箱子交互奖励 $R_{box}$ 鼓励正确的接触时机和箱子定位。稳定性奖励 $R_{stand}$ 通过奖励居中的压力中心、直立姿态和最小速度来保持机器人平衡。最后，正则化奖励 $R_{reg}$ 促进平滑、温和的动作。

![](../../Attachments/Sim-to-Real%20Learning%20for%20Humanoid%20Box%20Loco-Manipulation_fig3.png)

_图 3：PICKUP 策略奖励函数中使用的手部轨迹可视化。轨迹引导手部通过接触和举升阶段，箱子显示为蓝色，手部位置在关键点处标记。_

## 动作空间和训练策略

这项研究的一个重要发现与操作任务的行动空间选择有关。PICKUP 策略使用“相对”行动空间，其中策略输出被添加到当前电机位置，而不是固定的中立姿态。事实证明，这种方法在学习显著偏离静态姿势的复杂操作运动方面效率更高。

训练在 MuJoCo 物理模拟器中使用近端策略优化（PPO），并进行广泛的领域随机化以促进从模拟到真实的迁移。训练期间随机化了包括物体质量、关节阻尼、地面摩擦和质心位置在内的参数。此外，特定场景的随机化改变了箱子的尺寸（20-45 厘米）、质量（1-10 千克）、起始姿势和目标位置。

# 实验结果与发现

学习到的策略在模拟和真实硬件上都表现出令人印象深刻的性能。在 10,000 个回合中平均的模拟结果显示，所有行为都取得了高成功率：PICKUP 实现了 96.15% 的成功率，平均位置误差为 7.93 厘米，而运动策略（WALK，STAND）则达到了 96-100% 的成功率。

![](../../Attachments/Sim-to-Real%20Learning%20for%20Humanoid%20Box%20Loco-Manipulation_fig4.png)

_图 4：PICKUP 策略消融研究的训练曲线。基线方法（蓝色）显著优于没有手部轨迹指导（橙色）或使用绝对行动空间（绿色）的变体，达到相似性能所需的训练时间大约减半。_

特别值得注意的是 PICKUP 策略超越其训练分布的泛化能力。该策略成功处理了高达 22.9 千克的箱子（训练时为 1-10 千克）、高达 64 厘米的尺寸（训练时为 20-45 厘米）以及远超训练范围的位置。这表明了强化学习和领域随机化在开发广泛适用技能方面的有效性。

消融研究揭示了两个关键的设计见解。首先，移除显式的手部轨迹奖励会显著阻碍学习效率，需要大约两倍的训练样本才能达到相似的性能。其次，相对行动空间对于操作任务至关重要，而绝对行动空间则表现出类似的性能下降。

# 真实世界实施与挑战

训练好的策略成功迁移到 Digit 人形机器人上，实现了完整的移动 - 操作序列：走到一个箱子旁边，拿起它，搬到另一个位置，然后放下。该系统可以处理 1 千克到 8 千克的箱子，并适应不同的起始配置。

然而，从模拟到真实的迁移揭示了一些挑战。作者观察到硬件执行中的方向偏差，这归因于模拟和现实之间 IMU 和姿态估计的差异。这需要实施特定策略的“质心调整”，对于较重的箱子需要更大的偏移量。这些问题突出了模拟和真实世界动力学之间仍然存在的差距，尤其是在力和接触建模方面。

# 贡献与意义

这项工作对人形机器人学和强化学习做出了几项重要贡献。最重要的是，它首次展示了一个完全学习的控制器通过从模拟到真实的迁移在真实人形硬件上实现移动 - 操作。这一成就超越了基于模型的控制和模仿学习方法的局限性。

方法学上的贡献包括针对复杂操作任务的有效奖励函数设计原则，证明了通过轨迹奖励和接触时间进行结构化指导的重要性。发现相对行动空间在操作任务中显著优于绝对行动空间，为未来的基于强化学习的机器人研究提供了宝贵的见解。

模块化、基于技能的方法提供了一个可扩展的框架，用于开发日益复杂的人形机器人行为。通过将任务分解为具有明确转换的可学习组件，系统在保持灵活性的同时确保了协调的全身控制。

这项研究是迈向真正在人类环境中运行的自主、多功能人形机器人的关键一步。同时保持平衡和操作不同属性物体的能力解决了双足机器人学的根本挑战，并为在需要类人灵巧性和移动性的应用中实际部署开辟了道路。

# 相关引用

## 箱体移动操作的分层规划与控制

这篇论文明确地将其识别为“最相关的先行研究”，因为它针对的是人形机器人抓取箱子并进行移动的相同任务。当前的研究建立在将任务分解为技能的类似策略之上，但旨在通过消除对轨迹跟踪的依赖并专注于硬件执行来改进它。

\[22\] Z. Xie, J. Tseng, S. Starke, M. van de Panne, and C. K. Liu, “Hierarchical planning and control for box loco-manipulation,” 6 2023. \[Online\]. Available: <http://arxiv.org/abs/2306.09532>

## 在未感知动态载荷下的双足行走虚实迁移学习

本文提供了当前工作所基于的基础学习框架，包括 LSTM 神经网络架构以及学习运动策略的通用设置。作者直接指出，他们在策略表示和基础运动技能学习方面遵循了这项前期工作。

\[23\] J. Dao, K. Green, H. Duan, A. Fern, and J. Hurst, “Sim-to-real learning for bipedal locomotion under unsensed dynamic loads.” IEEE, 5 2022, pp. 10 449–10 455.

## 采用动力学随机化的机器人控制仿真到实物迁移

这项工作是论文“虚实迁移”（sim-to-real）方面的重要基础，而“虚实迁移”正是论文的一个核心贡献。在此引用文献中详述的动力学随机化技术，被明确用于促进所学策略从 MuJoCo 仿真器成功迁移到物理 Digit 机器人上。

\[26\] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, “Sim-to-Real Transfer of Robotic Control with Dynamics Randomization,” in 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, may 2018, pp. 3803–3810. \[Online\]. Available: <https://ieeexplore.ieee.org/document/8460528/>

## 基于周期性奖励合成的所有常见双足步态虚实迁移学习

本文被引作奖励函数设计的主要灵感来源，尤其是在运动的不同阶段规定接触的原则。本文将这一核心思想从周期性运动任务改编应用于非周期性、有限时域的拾取箱子任务。

\[5\] J. Siekmann, Y. Godse, A. Fern, and J. Hurst, “Sim-to-real learning of all common bipedal gaits via periodic reward composition,” in 2021 IEEE International Conference on Robotics and Automation (ICRA), 2021, pp. 7309–7315.