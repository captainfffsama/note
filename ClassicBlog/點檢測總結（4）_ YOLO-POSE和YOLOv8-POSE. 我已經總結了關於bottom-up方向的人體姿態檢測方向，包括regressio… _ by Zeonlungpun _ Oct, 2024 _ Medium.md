[

![](https://miro.medium.com/v2/da:true/resize:fill:88:88/0*GA26dCHZTpPXzAgD)






](https://medium.com/@zeonlungpun?source=post_page---byline--739a75310ef4--------------------------------)

æˆ‘å·²ç¶“ç¸½çµäº†é—œæ–¼bottom-upæ–¹å‘çš„äººé«”å§¿æ…‹æª¢æ¸¬æ–¹å‘ï¼ŒåŒ…æ‹¬regression basedå’Œheatmap baseçš„æ–¹æ³•ã€‚å¯ä»¥çœ‹å‡ºï¼Œheatmap baseçš„æ–¹æ³•å¾Œè™•ç†æœƒç›¸å°è¤‡é›œï¼Œä¸”å°æ–¼å¤šäººçš„å§¿æ…‹æª¢æ¸¬æ™‚ï¼Œæ›´é¡¯å¾—è¤‡é›œã€‚æ¥ä¸‹ä¾†é€™å€‹blogæˆ‘è¦ç ”ç©¶ã€ç¸½çµä¸€ä¸‹top-downæ–¹æ³•ï¼Œå³å…ˆç”¨detectoræª¢æ¸¬å‡ºäººï¼Œå†æª¢æ¸¬å‡ºé—œéµé»ã€‚é€™é¡æ–¹æ³•ä¹Ÿå¯ä»¥çœ‹ä½œç›´æ¥å›æ­¸å‡ºé»ã€‚

ä¸»è¦ç ”ç©¶å…©å€‹æ¨¡å‹ï¼šYOLO-POSEï¼ˆä¸»è¦åŸºæ–¼YOLOv5ï¼‰å’ŒYOLOv8-POSEã€‚

é€™å…©ç¨®æ–¹æ³•æœ¬è³ªä¸Šå°±æ˜¯åœ¨YOLOv5æˆ–è€…YOLOv8çš„åŸºç¤ä¸ŠåŠ ä¸€å€‹poseçš„é æ¸¬é ­ï¼Œåœ¨åšç›®æ¨™æª¢æ¸¬çš„åŒæ™‚å°äººé«”é—œéµé»é€²è¡Œæª¢æ¸¬ã€‚ä¸»è¦æ€è·¯å’Œå¥½è™•å¦‚ä¸‹æ‰€ç¤ºï¼š

> _Existing heatmap based two-stage approaches are sub-optimal as they are not end-to-end trainable and training relies on a surrogate L1 loss that is not equivalent to maximizing the evaluation metric, i.e. Object Keypoint Similarity (OKS). Our framework allows us to train the model end-to-end and optimize the OKS metric itself. The proposed model learns to jointly detect bounding boxes for multiple persons and their corresponding 2D poses in a single forward pass and thus bringing in the best of both top-down and bottom-up approaches. Proposed approach doesnâ€™t require the postprocessing of bottom-up approaches to group detected keypoints into a skeleton as each bounding box has an associated pose, resulting in an inherent grouping of the keypoints._

![](https://miro.medium.com/v2/resize:fit:875/0*AGMylKSlM4SdzJRn)

æ·»åŠ å›¾ç‰‡æ³¨é‡Šï¼Œä¸è¶…è¿‡ 140 å­—ï¼ˆå¯é€‰ï¼‰

ç¶²çµ¡çš„è¼¸å‡ºç¾åœ¨è®Šæˆé€™æ¨£ï¼šbounding boxçš„4å€‹åº§æ¨™ï¼Œæ¡†çš„ç½®ä¿¡åº¦ï¼Œäººé¡åˆ¥çš„ç½®ä¿¡åº¦ï¼›ä»¥åŠæ¯å€‹äººé æ¸¬å‡º17å€‹keypointsï¼Œæ¯å€‹keypointçš„åº§æ¨™å’Œç½®ä¿¡åº¦ï¼š

![](https://miro.medium.com/v2/resize:fit:875/0*6bqtLD5-bCt8JcmQ)

æ·»åŠ å›¾ç‰‡æ³¨é‡Šï¼Œä¸è¶…è¿‡ 140 å­—ï¼ˆå¯é€‰ï¼‰

å„ªé»ç¸½çµå°±æ˜¯ï¼š

*   å¤šäººé—œéµé»æª¢æ¸¬æ›´å¿«é€Ÿã€æ–¹ä¾¿

> _solving multi-person pose estimation in line with object detection since major challenges like scale variation and occlusion are common to both. Thus, taking the first step toward unifying these two fields. Our approach will directly benefit from any advancement in the field of Object detection._

*   é¿å…ç¹ç‘£çš„å¾Œè™•ç†æ­¥é©Ÿ

> _heatmap-free approach uses standard OD postprocessing instead of complex post-processing involving Pixel level NMS, adjustment, refinement, line-integral, and various grouping algorithms. The approach is robust because of end-to-end training without independent post-processing._

*   å¯ä»¥ç›´æ¥åˆ©ç”¨ç¶²çµ¡å„ªåŒ–è©•åƒ¹æŒ‡æ¨™OKS

> _Extended the idea of IoU loss from box detection to keypoints. Object keypoint similarity (OKS) is not just used for evaluation but as a loss for training. OKS loss is scale-invariant and inherently gives different weighting to different keypoints_

é¡ä¼¼æ–¼IOU LOSSï¼Œé€™è£¡æå‡ºäº†OKS lossã€‚è©²æ–¹æ³•çš„å¥½è™•å°±æ˜¯ç›´æ¥åˆ©ç”¨ç¶²çµ¡å„ªåŒ–metricsæŒ‡æ¨™ï¼Œè€Œä¸æ˜¯é–“æ¥å„ªåŒ–ï¼š

> _Conventionally, heat-map based bottom-up approaches use L1 loss to detect keypoints. However, L1 loss may not necessarily be suitable to obtain optimal OKS. Again, L1 loss is naÃ¯ve and doesnâ€™t take into consideration scale of an object or the type of a keypoint._

![](https://miro.medium.com/v2/resize:fit:875/0*u1pW1rvcpBqKJZPp)

æ·»åŠ å›¾ç‰‡æ³¨é‡Šï¼Œä¸è¶…è¿‡ 140 å­—ï¼ˆå¯é€‰ï¼‰

é—œéµé»ç½®ä¿¡åº¦æå¤±ï¼š

![](https://miro.medium.com/v2/resize:fit:875/0*2_ZhNhZfENoD8Bd2)

æ·»åŠ å›¾ç‰‡æ³¨é‡Šï¼Œä¸è¶…è¿‡ 140 å­—ï¼ˆå¯é€‰ï¼‰

yolov8-obbçš„ä¸»è¦æ€è·¯å°±æ˜¯åœ¨æª¢æ¸¬é ­çš„åŸºç¤ä¸ŠåŠ ä¸€å€‹obbæ¨¡å¡Šï¼Œåœ¨é€²è¡Œæ¡†é æ¸¬çš„åŒæ™‚é †ä¾¿é€²è¡Œé—œéµé»æª¢æ¸¬ã€‚å…¶ä¸­éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ­£è² æ¨£æœ¬åˆ†é…ã€NMSéƒ½æ˜¯ä»¥ç›®æ¨™æª¢æ¸¬çˆ²åŸºæº–ï¼›å³å…ˆç›®æ¨™æª¢æ¸¬ï¼Œå†é—œéµé»æª¢æ¸¬ã€‚

é…ç½®æ–‡ä»¶

```
\# Ultralytics YOLO ğŸš€, AGPL-3.0 license  
\# YOLOv8-pose keypoints/pose estimation model. For Usage examples see https://docs.ultralytics.com/tasks/pose
```

```
\# Parameters  
nc: 1  # number of classes  
kpt\_shape: \[17, 3\]  # number of keypoints, number of dims (2 for x,y or 3 for x,y,visible)  
scales: # model compound scaling constants, i.e. 'model=yolov8n-pose.yaml' will call yolov8-pose.yaml with scale 'n'  
  # \[depth, width, max\_channels\]  
  n: \[0.33, 0.25, 1024\]  
  s: \[0.33, 0.50, 1024\]  
  m: \[0.67, 0.75, 768\]  
  l: \[1.00, 1.00, 512\]  
  x: \[1.00, 1.25, 512\]\# YOLOv8.0n backbone  
backbone:  
  # \[from, repeats, module, args\]  
  - \[-1, 1, Conv, \[64, 3, 2\]\]  # 0-P1/2  
  - \[-1, 1, Conv, \[128, 3, 2\]\]  # 1-P2/4  
  - \[-1, 3, C2f, \[128, True\]\]  
  - \[-1, 1, Conv, \[256, 3, 2\]\]  # 3-P3/8  
  - \[-1, 6, C2f, \[256, True\]\]  
  - \[-1, 1, Conv, \[512, 3, 2\]\]  # 5-P4/16  
  - \[-1, 6, C2f, \[512, True\]\]  
  - \[-1, 1, Conv, \[1024, 3, 2\]\]  # 7-P5/32  
  - \[-1, 3, C2f, \[1024, True\]\]  
  - \[-1, 1, SPPF, \[1024, 5\]\]  # 9\# YOLOv8.0n head  
head:  
  - \[-1, 1, nn.Upsample, \[None, 2, 'nearest'\]\]  
  - \[\[-1, 6\], 1, Concat, \[1\]\]  # cat backbone P4  
  - \[-1, 3, C2f, \[512\]\]  # 12 - \[-1, 1, nn.Upsample, \[None, 2, 'nearest'\]\]  
  - \[\[-1, 4\], 1, Concat, \[1\]\]  # cat backbone P3  
  - \[-1, 3, C2f, \[256\]\]  # 15 (P3/8-small) - \[-1, 1, Conv, \[256, 3, 2\]\]  
  - \[\[-1, 12\], 1, Concat, \[1\]\]  # cat head P4  
  - \[-1, 3, C2f, \[512\]\]  # 18 (P4/16-medium) - \[-1, 1, Conv, \[512, 3, 2\]\]  
  - \[\[-1, 9\], 1, Concat, \[1\]\]  # cat head P5  
  - \[-1, 3, C2f, \[1024\]\]  # 21 (P5/32-large)  
#kpt\_shape ä¸€èˆ¬çˆ² (17,3)  
  - \[\[15, 18, 21\], 1, Pose, \[nc, kpt\_shape\]\]  # Pose(P3, P4, P5)
```

æ³¨æ„ï¼Œå¤šäº†å€‹Poseçµæ§‹ã€‚

poseçµæ§‹

```
class Pose(Detect):  
    """YOLOv8 Pose head for keypoints models."""
```

```
 def \_\_init\_\_(self, nc=80, kpt\_shape=(17, 3), ch=()):  
        """Initialize YOLO network with default parameters and Convolutional Layers."""  
        super().\_\_init\_\_(nc, ch)  
        self.kpt\_shape = kpt\_shape  # number of keypoints, number of dims (2 for x,y or 3 for x,y,visible)  
        self.nk = kpt\_shape\[0\] \* kpt\_shape\[1\]  # number of keypoints total  
        self.detect = Detect.forward  
        #ä¸­é–“éæ¸¡å±¤çš„channelæ•¸é‡  
        c4 = max(ch\[0\] // 4, self.nk)  
       #ä¸‰å€‹è¼¸å‡ºé ­  
        self.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nk, 1)) for x in ch) def forward(self, x):  
        """Perform forward pass through YOLO model and return predictions."""  
        bs = x\[0\].shape\[0\]  # batch size  
#åŒæ™‚è¼¸å‡ºæª¢æ¸¬æ¡†å’Œé—œéµé»  
        kpt = torch.cat(\[self.cv4\[i\](x\[i\]).view(bs, self.nk, -1) for i in range(self.nl)\], -1)  # (bs, 17\*3, h\*w)  
        x = self.detect(self, x)  
        if self.training:  
            return x, kpt  
        pred\_kpt = self.kpts\_decode(bs, kpt)  
        return torch.cat(\[x, pred\_kpt\], 1) if self.export else (torch.cat(\[x\[0\], pred\_kpt\], 1), (x\[1\], kpt)) def kpts\_decode(self, bs, kpts):  
        """Decodes keypoints."""  
        ndim = self.kpt\_shape\[1\]  
        if self.export:  # required for TFLite export to avoid 'PLACEHOLDER\_FOR\_GREATER\_OP\_CODES' bug  
            y = kpts.view(bs, \*self.kpt\_shape, -1)  
            a = (y\[:, :, :2\] \* 2.0 + (self.anchors - 0.5)) \* self.strides  
         #å¦‚æœ ndim == 3ï¼Œå°ç¬¬ä¸‰å€‹ç¶­åº¦ï¼ˆç½®ä¿¡åº¦ï¼‰æ‡‰ç”¨ sigmoid å‡½æ•¸ã€‚  
            if ndim == 3:  
                a = torch.cat((a, y\[:, :, 2:3\].sigmoid()), 2)  
            return a.view(bs, self.nk, -1)  
        else:  
            y = kpts.clone()  
            if ndim == 3:  
                y\[:, 2::3\] = y\[:, 2::3\].sigmoid()  # sigmoid (WARNING: inplace .sigmoid\_() Apple MPS bug)  
            y\[:, 0::ndim\] = (y\[:, 0::ndim\] \* 2.0 + (self.anchors\[0\] - 0.5)) \* self.strides  
            y\[:, 1::ndim\] = (y\[:, 1::ndim\] \* 2.0 + (self.anchors\[1\] - 0.5)) \* self.strides  
            return y
```

æ¨¡å‹é æ¸¬é—œéµé»ç›¸å°æ–¼æŸå€‹**anchor point** çš„åç§»é‡ã€‚é€™æ¨£çš„è¨­è¨ˆä½¿æ¨¡å‹æ›´å®¹æ˜“å­¸ç¿’ä¸¦æé«˜æº–ç¢ºæ€§ã€‚æ¨¡å‹è¼¸å‡ºçš„ y\[:, 0::ndim\] å’Œ y\[:, 1::ndim\] åˆ†åˆ¥ä»£è¡¨ x å’Œ y åæ¨™çš„é æ¸¬å€¼ã€‚

*   é€™äº›é æ¸¬å€¼é€šå¸¸æ˜¯ç›¸å°æ–¼ anchor çš„åç§»é‡ï¼Œç¯„åœä¸€èˆ¬åœ¨ \[-1, 1\] ä¹‹é–“ã€‚
*   y\[:, 0::ndim\] \* 2.0 + (self.anchors\[0\] â€” 0.5) ä¸­çš„ \* 2.0 å°‡é æ¸¬çš„åç§»é‡æ”¾å¤§ï¼Œä½¿å…¶ç¯„åœè®Šç‚º \[0, 2\]ï¼Œç„¶å¾ŒåŠ ä¸Š (self.anchors\[0\] â€” 0.5) å°‡å…¶è½‰æ›ç‚ºç›¸å°æ–¼ anchor çš„å¯¦éš›åç§»é‡ã€‚
*   self.anchors\[0\] å’Œ self.anchors\[1\] æ˜¯å°æ‡‰æ–¼ x å’Œ y åæ¨™çš„ anchor é»ï¼Œ-0.5 æ˜¯ç‚ºäº†èª¿æ•´åç§»ï¼Œä½¿å¾—é æ¸¬å€¼å¯ä»¥ç²¾ç¢ºå°æ‡‰åˆ° anchor çš„å‘¨åœã€‚
*   æœ€å¾Œï¼Œçµæœä¹˜ä»¥ self.stridesã€‚strides è¡¨ç¤ºåœ¨åœ–åƒä¸­çš„ç¶²æ ¼æ­¥å¹…ï¼Œç¢ºä¿åç§»é‡è½‰æ›ç‚ºåœ–åƒä¸­çš„å¯¦éš›åƒç´ åæ¨™ã€‚é€™æ„å‘³è‘— strides å†³å®šäº† anchor åœ¨åŸå›¾ä¸­çš„å¤§å°æ¯”ä¾‹ï¼Œæ¯ä¸€ä¸ª stride ç›¸å½“äºåœ¨å›¾åƒä¸­çš„å®é™…åƒç´ æ•°é‡ã€‚

```
from ultralytics.engine.results import Results  
from ultralytics.models.yolo.detect.predict import DetectionPredictor  
from ultralytics.utils import DEFAULT\_CFG, LOGGER, ops  

```

```
class PosePredictor(DetectionPredictor):  
    """  
    A class extending the DetectionPredictor class for prediction based on a pose model. Example:  
        \`\`\`python  
        from ultralytics.utils import ASSETS  
        from ultralytics.models.yolo.pose import PosePredictor args = dict(model='yolov8n-pose.pt', source=ASSETS)  
        predictor = PosePredictor(overrides=args)  
        predictor.predict\_cli()  
        \`\`\`  
    """ def \_\_init\_\_(self, cfg=DEFAULT\_CFG, overrides=None, \_callbacks=None):  
        """Initializes PosePredictor, sets task to 'pose' and logs a warning for using 'mps' as device."""  
        super().\_\_init\_\_(cfg, overrides, \_callbacks)  
        self.args.task = 'pose'  
        if isinstance(self.args.device, str) and self.args.device.lower() == 'mps':  
            LOGGER.warning("WARNING âš ï¸ Apple MPS known Pose bug. Recommend 'device=cpu' for Pose models. "  
                           'See [https://github.com/ultralytics/ultralytics/issues/4031.')](https://github.com/ultralytics/ultralytics/issues/4031.')) def postprocess(self, preds, img, orig\_imgs):  
        """Return detection results for a given input image or list of images."""  
 #å…ˆå°æ¡†ä½œNMS  
        preds = ops.non\_max\_suppression(preds,  
                                        self.args.conf,  
                                        self.args.iou,  
                                        agnostic=self.args.agnostic\_nms,  
                                        max\_det=self.args.max\_det,  
                                        classes=self.args.classes,  
                                        nc=len(self.model.names)) if not isinstance(orig\_imgs, list):  # input images are a torch.Tensor, not a list  
            orig\_imgs = ops.convert\_torch2numpy\_batch(orig\_imgs)  
 #NMSå¾Œçš„çµæœ  
        results = \[\]  
        for i, pred in enumerate(preds):  
            orig\_img = orig\_imgs\[i\]  
           #letterboxçš„é€†éç¨‹ï¼Œä¸»è¦é‡å°æ¡†çš„é æ¸¬  
            pred\[:, :4\] = ops.scale\_boxes(img.shape\[2:\], pred\[:, :4\], orig\_img.shape).round()  
            pred\_kpts = pred\[:, 6:\].view(len(pred), \*self.model.kpt\_shape) if len(pred) else pred\[:, 6:\]  
           #èˆ‡ops.scale\_boxesé¡ä¼¼ï¼Œä¸»è¦é‡å°é»  
            pred\_kpts = ops.scale\_coords(img.shape\[2:\], pred\_kpts, orig\_img.shape)  
            img\_path = self.batch\[0\]\[i\]  
            results.append(  
                Results(orig\_img, path=img\_path, names=self.model.names, boxes=pred\[:, :6\], keypoints=pred\_kpts))  
        return results
```

æœ¬è³ªå°±æ˜¯ä¸Šæ–‡æåŠçš„OKS LOSSï¼š

```
class KeypointLoss(nn.Module):  
    """Criterion class for computing training losses."""
```

```
 def \_\_init\_\_(self, sigmas) -> None:  
        """Initialize the KeypointLoss class."""  
        super().\_\_init\_\_()  
        self.sigmas = sigmas def forward(self, pred\_kpts, gt\_kpts, kpt\_mask, area):  
        """Calculates keypoint loss factor and Euclidean distance loss for predicted and actual keypoints."""  
        d = (pred\_kpts\[..., 0\] - gt\_kpts\[..., 0\]) \*\* 2 + (pred\_kpts\[..., 1\] - gt\_kpts\[..., 1\]) \*\* 2  
        kpt\_loss\_factor = kpt\_mask.shape\[1\] / (torch.sum(kpt\_mask != 0, dim=1) + 1e-9)  
        e = d / (2 \* self.sigmas) \*\* 2 / (area + 1e-9) / 2  # from cocoeval  
        return (kpt\_loss\_factor.view(-1, 1) \* ((1 - torch.exp(-e)) \* kpt\_mask)).mean()
```

å…¨éƒ¨æå¤±ï¼š

ä¸»è¦æ˜¯æ¡†æå¤±ï¼ˆdfl loss+ ciou lossï¼‰ã€é¡åˆ¥æå¤±ã€oks lossï¼ˆé»æå¤±ï¼‰ã€é»çš„ç½®ä¿¡åº¦æå¤±ã€‚

```
class v8PoseLoss(v8DetectionLoss):  
    """Criterion class for computing training losses."""
```

```
 def \_\_init\_\_(self, model):  # model must be de-paralleled  
        """Initializes v8PoseLoss with model, sets keypoint variables and declares a keypoint loss instance."""  
        super().\_\_init\_\_(model)  
        self.kpt\_shape = model.model\[-1\].kpt\_shape  
        #é»çš„ç½®ä¿¡åº¦æå¤±  
        self.bce\_pose = nn.BCEWithLogitsLoss()  
        is\_pose = self.kpt\_shape == \[17, 3\]  
        nkpt = self.kpt\_shape\[0\]  # number of keypoints  
        sigmas = torch.from\_numpy(OKS\_SIGMA).to(self.device) if is\_pose else torch.ones(nkpt, device=self.device) / nkpt  
        self.keypoint\_loss = KeypointLoss(sigmas=sigmas) def \_\_call\_\_(self, preds, batch):  
        """Calculate the total loss and detach it."""  
        loss = torch.zeros(5, device=self.device)  # box, cls, dfl, kpt\_location, kpt\_visibility  
        feats, pred\_kpts = preds if isinstance(preds\[0\], list) else preds\[1\]  
##æŠŠä¸‰å€‹å°ºåº¦çš„æª¢æ¸¬é ­é€²è¡Œæ‹¼æ¥ï¼Œå†æ‹†åˆ†æˆboxé æ¸¬å€¼å’Œé¡åˆ¥æ¦‚ç‡é æ¸¬å€¼  
        pred\_distri, pred\_scores = torch.cat(\[xi.view(feats\[0\].shape\[0\], self.no, -1) for xi in feats\], 2).split(  
            (self.reg\_max \* 4, self.nc), 1) # B, grids, ..  
#(bs,total\_anchor\_num,num\_class)-->(bs,num\_class,total\_anchor\_num)  
        pred\_scores = pred\_scores.permute(0, 2, 1).contiguous()  
 # (bs,total\_anchor\_num,reg\_max\*4)-->(bs,reg\_max\*4,total\_anchor\_num)  
        pred\_distri = pred\_distri.permute(0, 2, 1).contiguous()  
#(bs,total\_anchor\_num,2)-->(bs,2,total\_anchor\_num)  
        pred\_kpts = pred\_kpts.permute(0, 2, 1).contiguous() dtype = pred\_scores.dtype  
        imgsz = torch.tensor(feats\[0\].shape\[2:\], device=self.device, dtype=dtype) \* self.stride\[0\]  # image size (h,w)  
        anchor\_points, stride\_tensor = make\_anchors(feats, self.stride, 0.5) # Targets  
        batch\_size = pred\_scores.shape\[0\]  
        batch\_idx = batch\['batch\_idx'\].view(-1, 1)  
        targets = torch.cat((batch\_idx, batch\['cls'\].view(-1, 1), batch\['bboxes'\]), 1)  
\# å°‡boxä¿¡æ¯ç”±æ­¸ä¸€åŒ–å°ºåº¦è½‰æ›åˆ°è¼¸å…¥åœ–åƒå°ºåº¦ï¼Œä¸¦å°bathå…§æ¯å¼µåœ–åƒçš„gtå€‹æ•¸é€²è¡Œå°é½Š(ç›®æ¨™å€‹æ•¸éƒ½è¨­å®šä¸€å€‹çµ±ä¸€çš„å€¼Mï¼Œ  
#æ–¹ä¾¿é€²è¡ŒçŸ©é™£é‹ç®—)  
    # Må€¼çš„è¨­å®šè¦å‰‡ç‚ºï¼Œé¸å–batchå…§æœ€å¤§çš„gt\_numä½œç‚ºM  
    # targets: (bs,M(n\_max\_boxes) ,5) ,å…¶ä¸­5 = cls,cx,cy,width,height  
        targets = self.preprocess(targets.to(self.device), batch\_size, scale\_tensor=imgsz\[\[1, 0, 1, 0\]\])  
        gt\_labels, gt\_bboxes = targets.split((1, 4), 2)  # cls, xyxy  
\# # é€šéå°å››å€‹åæ¨™å€¼ç›¸åŠ ï¼Œå¦‚æœç‚º0ï¼Œå‰‡èªªæ˜è©²gtä¿¡æ¯ç‚ºå¡«å……ä¿¡æ¯ï¼Œåœ¨maskä¸­ç‚ºFalseï¼Œå¾ŒæœŸè¨ˆç®—éç¨‹ä¸­æœƒé€²è¡Œéæ¿¾  
        mask\_gt = gt\_bboxes.sum(2, keepdim=True).gt\_(0) #  # Pboxes:å¾—åˆ°é æ¸¬çš„æ¡†åº§æ¨™å››å€‹å€¼çš„åç§»é‡  
        pred\_bboxes = self.bbox\_decode(anchor\_points, pred\_distri)  # xyxy, (b, h\*w, 4)  
     #å¾—åˆ°é æ¸¬çš„é»åº§æ¨™åç§»é‡  
        pred\_kpts = self.kpts\_decode(anchor\_points, pred\_kpts.view(batch\_size, -1, \*self.kpt\_shape))  # (b, h\*w, 17, 3)  
æ­£æ¨£æœ¬ç¯©é¸éç¨‹  
    """  
target\_labels(Tensor): shape(bs, num\_total\_anchors)  
target\_bboxes(Tensor): shape(bs, num\_total\_anchors, 4)  
target\_scores(Tensor): shape(bs, num\_total\_anchors, num\_classes)  
fg\_mask(Tensor): shape(bs, num\_total\_anchors):mask of each anchor boxes  
(distinguish the positive and negative samples)  
    """  
        \_, target\_bboxes, target\_scores, fg\_mask, target\_gt\_idx = self.assigner(  
            pred\_scores.detach().sigmoid(), (pred\_bboxes.detach() \* stride\_tensor).type(gt\_bboxes.dtype),  
            anchor\_points \* stride\_tensor, gt\_labels, gt\_bboxes, mask\_gt) target\_scores\_sum = max(target\_scores.sum(), 1) # # Cls loss:æ¯å€‹éƒ½é¡éƒ½ä½¿ç”¨äºŒå…ƒäº¤å‰ç†µæå¤±  
        loss\[3\] = self.bce(pred\_scores, target\_scores.to(dtype)).sum() / target\_scores\_sum  # BCE # Bbox loss  
        if fg\_mask.sum():  
            target\_bboxes /= stride\_tensor  
            loss\[0\], loss\[4\] = self.bbox\_loss(pred\_distri, pred\_bboxes, anchor\_points, target\_bboxes, target\_scores,  
                                              target\_scores\_sum, fg\_mask)  
            keypoints = batch\['keypoints'\].to(self.device).float().clone()  
            keypoints\[..., 0\] \*= imgsz\[1\]  
            keypoints\[..., 1\] \*= imgsz\[0\] loss\[1\], loss\[2\] = self.calculate\_keypoints\_loss(fg\_mask, target\_gt\_idx, keypoints, batch\_idx,  
                                                             stride\_tensor, target\_bboxes, pred\_kpts)  
#å„å€‹æå¤±åŠ æ¬Šæ±‚å’Œ  
        loss\[0\] \*= self.hyp.box  # box gain  
        loss\[1\] \*= self.hyp.pose  # pose gain  
        loss\[2\] \*= self.hyp.kobj  # kobj gain  
        loss\[3\] \*= self.hyp.cls  # cls gain  
        loss\[4\] \*= self.hyp.dfl  # dfl gain return loss.sum() \* batch\_size, loss.detach()  # loss(box, cls, dfl) @staticmethod  
    def kpts\_decode(anchor\_points, pred\_kpts):  
        """Decodes predicted keypoints to image coordinates."""  
        y = pred\_kpts.clone()  
        y\[..., :2\] \*= 2.0  
        y\[..., 0\] += anchor\_points\[:, \[0\]\] - 0.5  
        y\[..., 1\] += anchor\_points\[:, \[1\]\] - 0.5  
        return y
```

é‡é»ä¾†äº†ï¼Œcalculate keypoint loss:

é€™è£¡æœ‰å€‹åƒæ•¸masksï¼Œä»£è¡¨ç”±ç›®æ¨™æª¢æ¸¬é€²è¡Œåˆ†é…çš„æ­£æ¨£æœ¬ã€‚æ‰€ä»¥å¯¦éš›ä¸Šæ˜¯åœ¨ç›®æ¨™æª¢æ¸¬åŸºç¤ä¸Šé€²è¡Œé—œéµé»æª¢æ¸¬ã€‚

```
def calculate\_keypoints\_loss(self, masks, target\_gt\_idx, keypoints, batch\_idx, stride\_tensor, target\_bboxes,  
                                 pred\_kpts):  
        """  
        Calculate the keypoints loss for the model.
```

```
 This function calculates the keypoints loss and keypoints object loss for a given batch. The keypoints loss is  
        based on the difference between the predicted keypoints and ground truth keypoints. The keypoints object loss is  
        a binary classification loss that classifies whether a keypoint is present or not. Args:  
            masks (torch.Tensor): Binary mask tensor indicating object presence, shape (BS, N\_anchors).  
            target\_gt\_idx (torch.Tensor): Index tensor mapping anchors to ground truth objects, shape (BS, N\_anchors).  
            keypoints (torch.Tensor): Ground truth keypoints, shape (N\_kpts\_in\_batch, N\_kpts\_per\_object, kpts\_dim).  
            batch\_idx (torch.Tensor): Batch index tensor for keypoints, shape (N\_kpts\_in\_batch, 1).  
            stride\_tensor (torch.Tensor): Stride tensor for anchors, shape (N\_anchors, 1).  
            target\_bboxes (torch.Tensor): Ground truth boxes in (x1, y1, x2, y2) format, shape (BS, N\_anchors, 4).  
            pred\_kpts (torch.Tensor): Predicted keypoints, shape (BS, N\_anchors, N\_kpts\_per\_object, kpts\_dim). Returns:  
            (tuple): Returns a tuple containing:  
                - kpts\_loss (torch.Tensor): The keypoints loss.  
                - kpts\_obj\_loss (torch.Tensor): The keypoints object loss.  
        """  
        batch\_idx = batch\_idx.flatten()  
        batch\_size = len(masks) # Find the maximum number of keypoints in a single image  
        max\_kpts = torch.unique(batch\_idx, return\_counts=True)\[1\].max() # Create a tensor to hold batched keypoints  
        # ï¼ˆbs, max\_kpts,N\_kpts\_per\_object, kpts\_dim ï¼‰  
        batched\_keypoints = torch.zeros((batch\_size, max\_kpts, keypoints.shape\[1\], keypoints.shape\[2\]),  
                                        device=keypoints.device) # Fill batched\_keypoints with keypoints based on batch\_idx  
        for i in range(batch\_size):  
            keypoints\_i = keypoints\[batch\_idx == i\]  
            batched\_keypoints\[i, :keypoints\_i.shape\[0\]\] = keypoints\_i # Expand dimensions of target\_gt\_idx to match the shape of batched\_keypoints  
       #(BS, N\_anchors,1,1)  
        target\_gt\_idx\_expanded = target\_gt\_idx.unsqueeze(-1).unsqueeze(-1) # Use target\_gt\_idx\_expanded to select keypoints from batched\_keypoints  
        selected\_keypoints = batched\_keypoints.gather(  
            1, target\_gt\_idx\_expanded.expand(-1, -1, keypoints.shape\[1\], keypoints.shape\[2\])) # Divide coordinates by stride  
        selected\_keypoints /= stride\_tensor.view(1, -1, 1, 1) kpts\_loss = 0  
        kpts\_obj\_loss = 0 if masks.any():  
       #åªè¨ˆç®—æ­£æ¨£æœ¬çš„oksæå¤±  
      #æ­£æ¨£æœ¬ç”±ç›®æ¨™æª¢æ¸¬é€²è¡Œåˆ†é…ï¼Œå¯¦éš›ä¸Šæ˜¯åœ¨ç›®æ¨™æª¢æ¸¬åŸºç¤ä¸Šé€²è¡Œé—œéµé»æª¢æ¸¬  
            gt\_kpt = selected\_keypoints\[masks\]  
            area = xyxy2xywh(target\_bboxes\[masks\])\[:, 2:\].prod(1, keepdim=True)  
            pred\_kpt = pred\_kpts\[masks\]  
            #æŠŠvis=0çš„keypointå»æ‰  
            kpt\_mask = gt\_kpt\[..., 2\] != 0 if gt\_kpt.shape\[-1\] == 3 else torch.full\_like(gt\_kpt\[..., 0\], True)  
            kpts\_loss = self.keypoint\_loss(pred\_kpt, gt\_kpt, kpt\_mask, area)  # pose loss if pred\_kpt.shape\[-1\] == 3:  
                kpts\_obj\_loss = self.bce\_pose(pred\_kpt\[..., 2\], kpt\_mask.float())  # keypoint obj loss return kpts\_loss, kpts\_obj\_loss
```