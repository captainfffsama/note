---
title: "Trl SFT: llama2-7b-hf使用QLora 4bit量化后ds zero3加上flash atten v2单机多卡训练(笔记)_unsloth 多卡-CSDN博客"
source: "https://blog.csdn.net/qq_16555103/article/details/137677561"
author:
published:
created: 2025-02-17
description: "文章浏览阅读2.2k次，点赞20次，收藏30次。第三 参考官方命令: https://github.com/Dao-AILab/flash-attention。第一 确保 linux \"外界\"的 cuda版本 与 conda 虚拟环境中cuda版本一致。第二 安装好 c++ g++ ninja。_unsloth 多卡"
tags:
  - "clippings"
---
**目录**

[一、环境](https://blog.csdn.net/qq_16555103/article/details/#t0)

  [1.1、环境安装](https://blog.csdn.net/qq_16555103/article/details/#t1)

  [1.2、安装flash atten](https://blog.csdn.net/qq_16555103/article/details/#t2)

  [1.3、vscode远端可能遇到的一些问题](https://blog.csdn.net/qq_16555103/article/details/#t3)

[二、代码](https://blog.csdn.net/qq_16555103/article/details/#t4)

  [2.1、bash脚本](https://blog.csdn.net/qq_16555103/article/details/#t5) 

  [2.2、utils.py 注释与优化](https://blog.csdn.net/qq_16555103/article/details/#t6)

  [2.3、train.py 注释与优化](https://blog.csdn.net/qq_16555103/article/details/#t7)

  [2.4、模型/参数相关](https://blog.csdn.net/qq_16555103/article/details/#t8)

    [2.4.1、量化后的模型](https://blog.csdn.net/qq_16555103/article/details/#%C2%A0%20%C2%A0%202.3.1%E3%80%81%E9%87%8F%E5%8C%96%E5%90%8E%E7%9A%84%E6%A8%A1%E5%9E%8B)

      [2.4.1.1 量化后模型结构](https://blog.csdn.net/qq_16555103/article/details/#%C2%A0%20%C2%A0%20%C2%A0%20a%29%C2%A0%E9%87%8F%E5%8C%96%E5%90%8E%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84)

      [2.4.1.2 量化后模型layers](https://blog.csdn.net/qq_16555103/article/details/#%C2%A0%20%C2%A0%20%C2%A0%20b%29%C2%A0%E9%87%8F%E5%8C%96%E5%90%8E%E6%A8%A1%E5%9E%8Blayers)

    [2.4.2、参数](https://blog.csdn.net/qq_16555103/article/details/#%C2%A0%20%C2%A0%202.3.2%E3%80%81%E5%8F%82%E6%95%B0)

     [2.4.2.1 training args](https://blog.csdn.net/qq_16555103/article/details/#%C2%A0%20%C2%A0%20%C2%A0%20a%29%20training%20args)

     [2.4.2.2 peft args](https://blog.csdn.net/qq_16555103/article/details/#%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0b%29%20peft%C2%A0args)

     [2.4.2.3 model args](https://blog.csdn.net/qq_16555103/article/details/#%C2%A0%20%C2%A0%20%C2%A0%20c%29%20model%20args)

[三、Trl 库](https://blog.csdn.net/qq_16555103/article/details/#t9)

  [3.1、SFTTrainer](https://blog.csdn.net/qq_16555103/article/details/#t10)

  [3.2、其他的代码](https://blog.csdn.net/qq_16555103/article/details/#t11)

    [3.2.1、datasets.map 使用 load\_from\_cache\_file = False 方便调试​​​​​​​​​​​​​​](https://blog.csdn.net/qq_16555103/article/details/#%C2%A0%20%C2%A0%20%C2%A0%20a%29%20datasets.map%20%E4%BD%BF%E7%94%A8%20load_from_cache_file%20%3D%20False%20%E6%96%B9%E4%BE%BF%E8%B0%83%E8%AF%95%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B)

[四、小结](https://blog.csdn.net/qq_16555103/article/details/#t12)

  [4.1、在SFTTrainer初始化peft模型时，为什么 开启了 QLoRA + FSDP / DS-Zero3 后不使用prepare\_model\_for\_kbit\_training 和 peft\_module\_casting\_to\_bf16 ，prepare\_model\_for\_kbit\_training 和 peft\_module\_casting\_to\_bf16 做了什么？QLoRA + FSDP / DS-Zero3 未开启offload​​​​​​​​​​​​​​模型加载后model为什么在cpu上？](https://blog.csdn.net/qq_16555103/article/details/#t13)

  [4.2、bfloat16和float16的区别](https://blog.csdn.net/qq_16555103/article/details/#t14)

  [4.3、绝对位置编码与相对位置编码的区别，为什么现在的大模型都使用RoPE](https://blog.csdn.net/qq_16555103/article/details/#t15)

[五、Trl 其他Trainer注释笔记](https://blog.csdn.net/qq_16555103/article/details/#t16)

  [5.1、DPOTrainer笔记​​​​​​​​​​​​​​](https://blog.csdn.net/qq_16555103/article/details/#t17)

 [5.2、...](https://blog.csdn.net/qq_16555103/article/details/#t18) 

---

> - 项目地址
> 
> [peft/examples/sft at main · huggingface/peft · GitHub🤗 PEFT: State-of-the-art Parameter-Efficient Fine-Tuning. - peft/examples/sft at main · huggingface/peft![icon-default.png?t=N7T8](https://i-blog.csdnimg.cn/blog_migrate/003a2ce7eb50c2e24a8c624c260c5930.png)https://github.com/huggingface/peft/tree/main/examples/sft](https://github.com/huggingface/peft/tree/main/examples/sft "peft/examples/sft at main · huggingface/peft · GitHub")
> 
> - 文档
> 
> [https://huggingface.co/docs/peft/accelerate/deepspeed![icon-default.png?t=N7T8](https://i-blog.csdnimg.cn/blog_migrate/003a2ce7eb50c2e24a8c624c260c5930.png)https://huggingface.co/docs/peft/accelerate/deepspeed](https://huggingface.co/docs/peft/accelerate/deepspeed "https://huggingface.co/docs/peft/accelerate/deepspeed")

### 一、环境

```python
系统：ubuntu cuda版本：12.1torch版本：2.2.0python版本：3.10conda 虚拟环境中 cuda版本cuda：12.1  
```

####   1.1、环境安装

> ```bash
> pip install -r ...
> ```

    **第一种**

**a) 2024年4月28日更新**

```python
git+https://github.com/huggingface/accelerategit+https://github.com/huggingface/peftgit+https://github.com/huggingface/trlgit+https://github.com/huggingface/datatrove.gitunsloth[conda]@git+https://github.com/unslothai/unsloth.gitgit+https://github.com/huggingface/transformersdeepspeed==0.14.0PyGithubhuggingface-hubevaluatedatasetsbitsandbyteseinopswandbtensorboardtiktokenpandasnumpyscipymatplotlibsentencepiecenltkxformershf_transferlogurutqdmtransformers_stream_generatortorch==2.2.1openpyxlhttpxjoblibscikit_learn
```

**​​​​​​​b)**​​​​​​​ **2024年7月7日更新（增加了vllm）**

```python
git+https://github.com/huggingface/accelerategit+https://github.com/huggingface/peftgit+https://github.com/huggingface/datatrove.gitgit+https://github.com/huggingface/transformersunsloth[conda]@git+https://github.com/unslothai/unsloth.gittrl==0.8.6deepspeed==0.14.0torch==2.3.0vllmraynumpy==1.26.4PyGithubhuggingface-hubevaluatedatasetsbitsandbyteseinopswandbtensorboardtiktokenpandasscipymatplotlibsentencepiecenltkxformershf_transferlogurutqdmtransformers_stream_generatoropenpyxlhttpxjoblibscikit_learn
```

**2024年6月19日更新（增加了vllm、ray）**

```python
unsloth[conda]@git+https://github.com/unslothai/unsloth.gitaccelerate==0.31.0peft==0.11.1datatrove==0.2.0trl==0.8.6transformers==4.41.2deepspeed==0.14.0torch==2.3.0vllm==0.5.0.post1vllm-flash-attn==2.5.9raynumpy==1.26.4PyGithubhuggingface-hubevaluatedatasetsbitsandbyteseinopswandbtensorboardtiktokenpandasscipymatplotlibsentencepiecenltkxformershf_transferlogurutqdmtransformers_stream_generatoropenpyxlhttpxjoblibscikit_learn
```

> **若上述pip安装包更新最新导致版本不匹配，可以参考下面第二种或第三种包版本适当修改** 

     **第二种**

**a) (zero3 peft lora 为bf16),** **2024年4月28日更新**

```python
absl-py==2.1.0accelerate==0.30.0aiohttp==3.9.5aiosignal==1.3.1annotated-types==0.7.0anyio==4.3.0async-timeout==4.0.3attrs==23.2.0bitsandbytes==0.43.1certifi==2024.2.2cffi==1.16.0charset-normalizer==3.3.2click==8.1.7contourpy==1.2.1cryptography==42.0.7cycler==0.12.1datasets==2.19.1datatrove==0.2.0deepspeed==0.14.0Deprecated==1.2.14dill==0.3.8docker-pycreds==0.4.0docstring_parser==0.16einops==0.8.0et-xmlfile==1.1.0evaluate==0.4.2exceptiongroup==1.2.1filelock==3.14.0fonttools==4.51.0frozenlist==1.4.1fsspec==2024.3.1gitdb==4.0.11GitPython==3.1.43grpcio==1.64.0h11==0.14.0hf_transfer==0.1.6hjson==3.1.0httpcore==1.0.5httpx==0.27.0huggingface-hub==0.23.1humanize==4.9.0idna==3.7Jinja2==3.1.4joblib==1.4.2kiwisolver==1.4.5loguru==0.7.2Markdown==3.6markdown-it-py==3.0.0MarkupSafe==2.1.5matplotlib==3.9.0mdurl==0.1.2mpmath==1.3.0multidict==6.0.5multiprocess==0.70.16networkx==3.3ninja==1.11.1.1nltk==3.8.1numpy==1.26.4nvidia-cublas-cu12==12.1.3.1nvidia-cuda-cupti-cu12==12.1.105nvidia-cuda-nvrtc-cu12==12.1.105nvidia-cuda-runtime-cu12==12.1.105nvidia-cudnn-cu12==8.9.2.26nvidia-cufft-cu12==11.0.2.54nvidia-curand-cu12==10.3.2.106nvidia-cusolver-cu12==11.4.5.107nvidia-cusparse-cu12==12.1.0.106nvidia-nccl-cu12==2.19.3nvidia-nvjitlink-cu12==12.5.40nvidia-nvtx-cu12==12.1.105openpyxl==3.1.2packaging==24.0pandas==2.2.2peft==0.10.0pillow==10.3.0pip==24.0platformdirs==4.2.2protobuf==3.20.3psutil==5.9.8py-cpuinfo==9.0.0pyarrow==16.1.0pyarrow-hotfix==0.6pycparser==2.22pydantic==2.7.1pydantic_core==2.18.2PyGithub==2.3.0Pygments==2.18.0PyJWT==2.8.0PyNaCl==1.5.0pynvml==11.5.0pyparsing==3.1.2python-dateutil==2.9.0.post0pytz==2024.1PyYAML==6.0.1regex==2024.5.15requests==2.32.2rich==13.7.1safetensors==0.4.3scikit-learn==1.5.0scipy==1.13.1sentencepiece==0.2.0sentry-sdk==2.3.1setproctitle==1.3.3setuptools==69.5.1shtab==1.7.1six==1.16.0smmap==5.0.1sniffio==1.3.1sympy==1.12tensorboard==2.16.2tensorboard-data-server==0.7.2threadpoolctl==3.5.0tiktoken==0.7.0tokenizers==0.19.1torch==2.2.1tqdm==4.66.4transformers==4.40.1transformers-stream-generator==0.0.5triton==2.2.0trl==0.8.3typing_extensions==4.12.0tyro==0.8.4tzdata==2024.1unsloth==2024.5urllib3==2.2.1wandb==0.17.0Werkzeug==3.0.3wheel==0.43.0wrapt==1.16.0xformers==0.0.25xxhash==3.4.1yarl==1.9.4
```

**b) (zero3 peft lora 为float32)，****2024年6月10日更新**

```python
Package                       Version----------------------------- -----------absl-py                       2.1.0accelerate                    0.31.0.dev0aiohttp                       3.9.5aiosignal                     1.3.1annotated-types               0.7.0anyio                         4.4.0async-timeout                 4.0.3attrs                         23.2.0bitsandbytes                  0.43.1certifi                       2024.6.2cffi                          1.16.0charset-normalizer            3.3.2click                         8.1.7contourpy                     1.2.1cryptography                  42.0.8cycler                        0.12.1datasets                      2.19.2datatrove                     0.2.0deepspeed                     0.14.0Deprecated                    1.2.14dill                          0.3.8docker-pycreds                0.4.0docstring_parser              0.16einops                        0.8.0et-xmlfile                    1.1.0evaluate                      0.4.2exceptiongroup                1.2.1filelock                      3.14.0fonttools                     4.53.0frozenlist                    1.4.1fsspec                        2024.3.1gitdb                         4.0.11GitPython                     3.1.43grpcio                        1.64.1h11                           0.14.0hf_transfer                   0.1.6hjson                         3.1.0httpcore                      1.0.5httpx                         0.27.0huggingface-hub               0.23.3humanize                      4.9.0idna                          3.7Jinja2                        3.1.4joblib                        1.4.2kiwisolver                    1.4.5loguru                        0.7.2Markdown                      3.6markdown-it-py                3.0.0MarkupSafe                    2.1.5matplotlib                    3.9.0mdurl                         0.1.2mpmath                        1.3.0multidict                     6.0.5multiprocess                  0.70.16networkx                      3.3ninja                         1.11.1.1nltk                          3.8.1numpy                         1.26.4nvidia-cublas-cu12            12.1.3.1nvidia-cuda-cupti-cu12        12.1.105nvidia-cuda-nvrtc-cu12        12.1.105nvidia-cuda-runtime-cu12      12.1.105nvidia-cudnn-cu12             8.9.2.26nvidia-cufft-cu12             11.0.2.54nvidia-curand-cu12            10.3.2.106nvidia-cusolver-cu12          11.4.5.107nvidia-cusparse-cu12          12.1.0.106nvidia-nccl-cu12              2.19.3nvidia-nvjitlink-cu12         12.5.40nvidia-nvtx-cu12              12.1.105openpyxl                      3.1.3packaging                     24.0pandas                        2.2.2peft                          0.11.2.dev0pillow                        10.3.0pip                           24.0platformdirs                  4.2.2protobuf                      3.20.3psutil                        5.9.8py-cpuinfo                    9.0.0pyarrow                       16.1.0pyarrow-hotfix                0.6pycparser                     2.22pydantic                      2.7.3pydantic_core                 2.18.4PyGithub                      2.3.0Pygments                      2.18.0PyJWT                         2.8.0PyNaCl                        1.5.0pynvml                        11.5.0pyparsing                     3.1.2python-dateutil               2.9.0.post0pytz                          2024.1PyYAML                        6.0.1regex                         2024.5.15requests                      2.32.3rich                          13.7.1safetensors                   0.4.3scikit-learn                  1.5.0scipy                         1.13.1sentencepiece                 0.2.0sentry-sdk                    2.4.0setproctitle                  1.3.3setuptools                    69.5.1shtab                         1.7.1six                           1.16.0smmap                         5.0.1sniffio                       1.3.1sympy                         1.12.1tensorboard                   2.16.2tensorboard-data-server       0.7.2threadpoolctl                 3.5.0tiktoken                      0.7.0tokenizers                    0.19.1torch                         2.2.1tqdm                          4.66.4transformers                  4.42.0.dev0transformers-stream-generator 0.0.5triton                        2.2.0trl                           0.8.6typing_extensions             4.12.1tyro                          0.8.4tzdata                        2024.1unsloth                       2024.5urllib3                       2.2.1wandb                         0.17.0Werkzeug                      3.0.3wheel                         0.43.0wrapt                         1.16.0xformers                      0.0.25xxhash                        3.4.1yarl                          1.9.4
```

####   1.2、安装flash atten

> **安装 flash atten 和 deepspeed 前，尽量保证：**
> 
> - 第一 确保 linux "外界"的 cuda版本 与 conda 虚拟环境中cuda版本一致(不一致可能导致使用过程中报"不匹配"的错误)
> - 第二 安装好 c++ g++ ninja (c++ g++ Ninjia 安装版本过低后续安装可能会失败)
> - 第三 参考官方命令: [GitHub - Dao-AILab/flash-attention: Fast and memory-efficient exact attentionFast and memory-efficient exact attention. Contribute to Dao-AILab/flash-attention development by creating an account on GitHub.![icon-default.png?t=N7T8](https://i-blog.csdnimg.cn/blog_migrate/003a2ce7eb50c2e24a8c624c260c5930.png)https://github.com/Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention "GitHub - Dao-AILab/flash-attention: Fast and memory-efficient exact attention")

```python
1. 安装 c++ g++sudo apt-get updatesudo apt-get install build-essential2. 安装 Ninjasudo apt-get install ninja-build         ----- 有时候vscode debug deepspeed 需要也可能有其他的依赖： sudo apt-get install -y ninja-build libssl-dev libffi-dev libaio-dev3. 安装flash atten    参考上面官方命令：    pip install packaging  或 conda install packaging    pip install flash-attn==2.6.1 --no-build-isolation          ----- flash atten 编译过程需要一定的时间，需要等待    MAX_JOBS=1 pip install flash-attn --no-build-isolation -i https://pypi.python.org/simple    可以适情况选择上面命令中的一个
```

####   1.3、vscode远端可能遇到的一些问题

- 中文路径识别问题

```bash
在项目目录的 .vscode 下创建 settings.json 文件，加入下面的内容：{"remote.SSH.env": {"LC_ALL": "en_US.UTF-8","LANG": "en_US.UTF-8"    },"remote.SSH.useLocalServer": false,"remote.SSH.connectTimeout": 60}
```

- 端口映射问题

```bash
遇到 Failed to set up socket for dynamic port forward on VSCodevim /etc/ssh/sshd_config ，打开后设置AllowAgentForwarding yes AllowTcpForwarding yes重启sshd服务systemctl restart sshd删除生成的vscode文件rm -rf ~/.vscode-server/重新ssh连接https://github.com/microsoft/vscode-remote-release/issues/8132
```

### 二、代码

[peft/examples/sft at main · huggingface/peft · GitHub🤗 PEFT: State-of-the-art Parameter-Efficient Fine-Tuning. - peft/examples/sft at main · huggingface/peft![icon-default.png?t=N7T8](https://i-blog.csdnimg.cn/blog_migrate/003a2ce7eb50c2e24a8c624c260c5930.png)https://github.com/huggingface/peft/tree/main/examples/sft](https://github.com/huggingface/peft/tree/main/examples/sft "peft/examples/sft at main · huggingface/peft · GitHub")

####   2.1、bash脚本 

```bash
PYTHONPATH=$PWDexport PYTHONPATHecho "当前bash执行目录: $PWD, 已经将PYTHONPATH设置为: $PYTHONPATH"accelerate launch --config_file "examples/sft/configs/deepspeed_config_z3_qlora.yaml"  examples/sft/train.py \    --seed 100 \    --model_name_or_path "/workspace/Llama-2-7b-chat-hf" \    --dataset_name "smangrul/ultrachat-10k-chatml" \    --chat_template_format "chatml" \    --add_special_tokens False \    --append_concat_token False \    --splits "train,test" \    --max_seq_len 2048 \    --num_train_epochs 2 \    --logging_steps 5 \    --log_level "info" \    --logging_strategy "steps" \    --evaluation_strategy "epoch" \    --save_strategy "steps" \    --save_steps 100 \    --save_total_limit 10 \    --bf16 True \    --packing True \    --learning_rate 1e-4 \    --lr_scheduler_type "cosine" \    --weight_decay 1e-4 \    --warmup_ratio 0.0 \    --max_grad_norm 1.0 \    --output_dir "/workspace/output/llama-sft-qlora-dsz3" \    --per_device_train_batch_size 1 \    --per_device_eval_batch_size 2 \    --gradient_accumulation_steps 4 \    --use_flash_attn True \    --gradient_checkpointing True \    --use_reentrant True \    --dataset_text_field "content" \    --use_peft_lora True \    --lora_r 8 \    --lora_alpha 16 \    --lora_dropout 0.1 \    --lora_target_modules "all-linear" \    --use_4bit_quantization True \    --use_nested_quant True \    --bnb_4bit_compute_dtype "bfloat16" \    --bnb_4bit_quant_storage_dtype "bfloat16" \    --resume_from_checkpoint /workspace/output/llama-sft-qlora-dsz3/checkpoint-100 \    2>&1 | tee -a examples/sft/qlora_ds_zero3_log.out
```

####   2.2、utils.py 注释与优化

> ```python
> import osfrom enum import Enumimport torchfrom datasets import DatasetDict, load_dataset, load_from_diskfrom datasets.builder import DatasetGenerationErrorfrom transformers import (    AutoModelForCausalLM,    AutoTokenizer,    BitsAndBytesConfig,)from peft import LoraConfigDEFAULT_CHATML_CHAT_TEMPLATE = "{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% if loop.last and add_generation_prompt %}{{'<|im_start|>assistant\n' }}{% endif %}{% endfor %}"DEFAULT_ZEPHYR_CHAT_TEMPLATE = "{% for message in messages %}\n{% if message['role'] == 'user' %}\n{{ '<|user|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'system' %}\n{{ '<|system|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'assistant' %}\n{{ '<|assistant|>\n'  + message['content'] + eos_token }}\n{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ '<|assistant|>' }}\n{% endif %}\n{% endfor %}"class ZephyrSpecialTokens(str, Enum):    user = "<|user|>"    assistant = "<|assistant|>"    system = "<|system|>"    eos_token = "</s>"          bos_token = "<s>"           pad_token = "<pad>"         @classmethoddef list(cls):return [c.value for c in cls]class ChatmlSpecialTokens(str, Enum):    user = "<|im_start|>user"    assistant = "<|im_start|>assistant"    system = "<|im_start|>system"    eos_token = "<|im_end|>"    bos_token = "<s>"    pad_token = "<pad>"    @classmethoddef list(cls):return [c.value for c in cls]def create_datasets(tokenizer, data_args, training_args, apply_chat_template=False):def preprocess(samples):        batch = []             batch_tokens = []for conversation in samples["messages"]:            chat_tmp = tokenizer.apply_chat_template(conversation, tokenize=False)            batch.append(chat_tmp)            chat_tmp_tokens = tokenizer.tokenize(chat_tmp)            batch_tokens.append(chat_tmp_tokens)return {"content": batch, "content_tokens":batch_tokens}    raw_datasets = DatasetDict()   for split in data_args.splits.split(","):try:            dataset = load_dataset(data_args.dataset_name, split=split)except DatasetGenerationError:            dataset = load_from_disk(os.path.join(data_args.dataset_name, split))if "train" in split:            raw_datasets["train"] = datasetelif "test" in split:            raw_datasets["test"] = datasetelse:raise ValueError(f"Split type {split} not recognized as one of test or train.")if apply_chat_template:        raw_datasets = raw_datasets.map(            preprocess,            batched=True,                     remove_columns=raw_datasets["train"].column_names,            load_from_cache_file = False        )    train_data = raw_datasets["train"]      valid_data = raw_datasets["test"]   if training_args.local_rank == 0 or training_args.local_rank == -1:print(f"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}")  print(f"A sample of train dataset: {train_data[0]}")  return train_data, valid_datadef create_and_prepare_model(args, data_args, training_args):if args.use_unsloth:from unsloth import FastLanguageModel    bnb_config = None        quant_storage_dtype = None   if (        torch.distributed.is_available()and torch.distributed.is_initialized()and torch.distributed.get_world_size() > 1and args.use_unsloth    ):raise NotImplementedError("Unsloth is not supported in distributed training")if args.use_4bit_quantization:        compute_dtype = getattr(torch, args.bnb_4bit_compute_dtype)        quant_storage_dtype = getattr(torch, args.bnb_4bit_quant_storage_dtype)        bnb_config = BitsAndBytesConfig(            load_in_4bit=args.use_4bit_quantization,                      bnb_4bit_quant_type=args.bnb_4bit_quant_type,                 bnb_4bit_compute_dtype=compute_dtype,                         bnb_4bit_use_double_quant=args.use_nested_quant,              bnb_4bit_quant_storage=quant_storage_dtype,               )if compute_dtype == torch.float16 and args.use_4bit_quantization:            major, _ = torch.cuda.get_device_capability()if major >= 8:print("=" * 80)print("Your GPU supports bfloat16, you can accelerate training with the argument --bf16")print("=" * 80)elif args.use_8bit_quantization:            bnb_config = BitsAndBytesConfig(load_in_8bit=args.use_8bit_quantization)if args.use_unsloth:        model, _ = FastLanguageModel.from_pretrained(            model_name=args.model_name_or_path,            max_seq_length=data_args.max_seq_length,            dtype=None,            load_in_4bit=args.use_4bit_quantization,        )else:         torch_dtype = (            quant_storage_dtype if quant_storage_dtype and quant_storage_dtype.is_floating_point else torch.float32        )        model = AutoModelForCausalLM.from_pretrained(            args.model_name_or_path,            quantization_config=bnb_config,            trust_remote_code=True,            attn_implementation="flash_attention_2" if args.use_flash_attn else "eager",            torch_dtype=torch_dtype,        )    peft_config = None          chat_template = None    if args.use_peft_lora and not args.use_unsloth:        peft_config = LoraConfig(            lora_alpha=args.lora_alpha,                     lora_dropout=args.lora_dropout,            r=args.lora_r,            bias="none",                                   task_type="CAUSAL_LM",                         target_modules=args.lora_target_modules.split(",")if args.lora_target_modules != "all-linear"else args.lora_target_modules,        )    special_tokens = None       chat_template = None    if args.chat_template_format == "chatml":        special_tokens = ChatmlSpecialTokens                      chat_template = DEFAULT_CHATML_CHAT_TEMPLATE      elif args.chat_template_format == "zephyr":        special_tokens = ZephyrSpecialTokens                    chat_template = DEFAULT_ZEPHYR_CHAT_TEMPLATE    if special_tokens is not None:        tokenizer = AutoTokenizer.from_pretrained(            args.model_name_or_path,            pad_token=special_tokens.pad_token.value,                 bos_token=special_tokens.bos_token.value,                 eos_token=special_tokens.eos_token.value,                 additional_special_tokens=special_tokens.list(),              trust_remote_code=True,        )        tokenizer.chat_template = chat_template                   model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)else:        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, trust_remote_code=True)        tokenizer.pad_token = tokenizer.eos_token     if args.use_unsloth:        model = FastLanguageModel.get_peft_model(            model,            lora_alpha=args.lora_alpha,            lora_dropout=args.lora_dropout,            r=args.lora_r,            target_modules=args.lora_target_modules.split(",")if args.lora_target_modules != "all-linear"else args.lora_target_modules,            use_gradient_checkpointing=training_args.gradient_checkpointing,            random_state=training_args.seed,            max_seq_length=data_args.max_seq_length,        )return model, peft_config, tokenizer       
> ```

####   2.3、train.py 注释与优化

```python
import osimport sysimport torchfrom dataclasses import dataclass, fieldfrom typing import Optionalimport torch.distributedfrom transformers import HfArgumentParser, TrainingArguments, set_seed, Seq2SeqTrainingArgumentsfrom trl import SFTTrainer    from utils import create_and_prepare_model, create_datasets  os.environ["WANDB_DISABLED"] = "true" @dataclassclass ModelArguments:"""    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.    """    model_name_or_path: str = field(        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}    )    chat_template_format: Optional[str] = field(        default="none",        metadata={"help": "chatml|zephyr|none. Pass \`none\` if the dataset is already formatted with the chat template."        },    )    lora_alpha: Optional[int] = field(default=16)        lora_dropout: Optional[float] = field(default=0.1)      lora_r: Optional[int] = field(default=64)    lora_target_modules: Optional[str] = field(        default="q_proj,k_proj,v_proj,o_proj,down_proj,up_proj,gate_proj",        metadata={"help": "comma separated list of target modules to apply LoRA layers to"},    )    use_nested_quant: Optional[bool] = field(        default=False,        metadata={"help": "Activate nested quantization for 4bit base models"},    )    bnb_4bit_compute_dtype: Optional[str] = field(        default="float16",        metadata={"help": "Compute dtype for 4bit base models"},    )    bnb_4bit_quant_storage_dtype: Optional[str] = field(        default="uint8",        metadata={"help": "Quantization storage dtype for 4bit base models"},    )    bnb_4bit_quant_type: Optional[str] = field(        default="nf4",        metadata={"help": "Quantization type fp4 or nf4"},    )    use_flash_attn: Optional[bool] = field(        default=False,        metadata={"help": "Enables Flash attention for training."},    )    use_peft_lora: Optional[bool] = field(        default=False,        metadata={"help": "Enables PEFT LoRA for training."},    )    use_8bit_quantization: Optional[bool] = field(        default=False,        metadata={"help": "Enables loading model in 8bit."},    )    use_4bit_quantization: Optional[bool] = field(        default=False,        metadata={"help": "Enables loading model in 4bit."},    )    use_reentrant: Optional[bool] = field(        default=False,        metadata={"help": "Gradient Checkpointing param. Refer the related docs"},    )    use_unsloth: Optional[bool] = field(        default=False,        metadata={"help": "Enables UnSloth for training."},    )@dataclassclass DataTrainingArguments:    dataset_name: Optional[str] = field(        default="timdettmers/openassistant-guanaco",        metadata={"help": "The preference dataset to use."},    )    packing: Optional[bool] = field(        default=False,        metadata={"help": "Use packing dataset creating."},    )    dataset_text_field: str = field(default="text", metadata={"help": "Dataset field to use as input text."})    max_seq_length: Optional[int] = field(default=512)    append_concat_token: Optional[bool] = field(        default=False,        metadata={"help": "If True, appends \`eos_token_id\` at the end of each sample being packed."},    )    add_special_tokens: Optional[bool] = field(        default=False,        metadata={"help": "If True, tokenizers adds special tokens to each sample being packed."},    )    splits: Optional[str] = field(        default="train,test",        metadata={"help": "Comma separate list of the splits to use from the dataset."},    )def print_model_allarguments_name_dtype(model):for n,v in model.named_parameters():if v.requires_grad:print(f"trainable model arguments: {n} - {v.dtype} - {v.shape} - {v.device}")else:print(f"not trainable model arguments: {n} - {v.dtype} - {v.shape} - {v.device}")def main(model_args, data_args, training_args):    set_seed(training_args.seed)     model, peft_config, tokenizer = create_and_prepare_model(model_args, data_args, training_args)    model.config.use_cache = not training_args.gradient_checkpointing    training_args.gradient_checkpointing = training_args.gradient_checkpointing and not model_args.use_unslothif training_args.gradient_checkpointing:        training_args.gradient_checkpointing_kwargs = {"use_reentrant": model_args.use_reentrant}    train_dataset, eval_dataset = create_datasets(        tokenizer,        data_args,        training_args,        apply_chat_template=model_args.chat_template_format != "none",    )if (torch.distributed.is_available() and torch.distributed.is_initialized()):        torch.distributed.barrier()      trainer = SFTTrainer(        model=model,        tokenizer=tokenizer,        args=training_args,        train_dataset=train_dataset,        eval_dataset=eval_dataset,        peft_config=peft_config,        packing=data_args.packing,        dataset_kwargs={"append_concat_token": data_args.append_concat_token,"add_special_tokens": data_args.add_special_tokens,        },        dataset_text_field=data_args.dataset_text_field,        max_seq_length=data_args.max_seq_length,        )if training_args.local_rank == 0 or training_args.local_rank == -1:print("---> model layers")        print_model_allarguments_name_dtype(model = trainer.model)     print(f"---> Training/evaluation parameters:\n{training_args}")print(f"---> Model parameters:\n{model_args}")print(f"---> Datas parameters:\n{data_args}")print(f"---> model config:\n{trainer.model.config}")print(f"---> PEFT config:\n{peft_config}")    trainer.accelerator.print(f"{trainer.model}")    trainer.model.print_trainable_parameters()    checkpoint = Noneif training_args.resume_from_checkpoint is not None:        checkpoint = training_args.resume_from_checkpoint    trainer.train(resume_from_checkpoint=checkpoint)if trainer.is_fsdp_enabled:        trainer.accelerator.state.fsdp_plugin.set_state_dict_type("FULL_STATE_DICT")    trainer.save_model()if __name__ == "__main__":    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))else:        model_args, data_args, training_args = parser.parse_args_into_dataclasses()if (torch.distributed.is_available() and torch.distributed.is_initialized()):print(f"---> Torch distributed enable, Torch distributed initialized, This local rank is: {training_args.local_rank}, Word_size: {torch.torch.distributed.get_world_size()}")    main(model_args, data_args, training_args)
```

####   2.4、模型/参数相关

#####     2.4.1、量化后的模型

######       2.4.1.1 量化后模型结构

```python
PeftModelForCausalLM(  (base_model): LoraModel(    (model): LlamaForCausalLM(      (model): LlamaModel(        (embed_tokens): Embedding(32008, 4096)        (layers): ModuleList(          (0-31): 32 x LlamaDecoderLayer(            (self_attn): LlamaFlashAttention2(              (q_proj): lora.Linear4bit(                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)                (lora_dropout): ModuleDict(                  (default): Dropout(p=0.1, inplace=False)                )                (lora_A): ModuleDict(                  (default): Linear(in_features=4096, out_features=8, bias=False)                )                (lora_B): ModuleDict(                  (default): Linear(in_features=8, out_features=4096, bias=False)                )                (lora_embedding_A): ParameterDict()                (lora_embedding_B): ParameterDict()              )              (k_proj): lora.Linear4bit(                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)                (lora_dropout): ModuleDict(                  (default): Dropout(p=0.1, inplace=False)                )                (lora_A): ModuleDict(                  (default): Linear(in_features=4096, out_features=8, bias=False)                )                (lora_B): ModuleDict(                  (default): Linear(in_features=8, out_features=4096, bias=False)                )                (lora_embedding_A): ParameterDict()                (lora_embedding_B): ParameterDict()              )              (v_proj): lora.Linear4bit(                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)                (lora_dropout): ModuleDict(                  (default): Dropout(p=0.1, inplace=False)                )                (lora_A): ModuleDict(                  (default): Linear(in_features=4096, out_features=8, bias=False)                )                (lora_B): ModuleDict(                  (default): Linear(in_features=8, out_features=4096, bias=False)                )                (lora_embedding_A): ParameterDict()                (lora_embedding_B): ParameterDict()              )              (o_proj): lora.Linear4bit(                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)                (lora_dropout): ModuleDict(                  (default): Dropout(p=0.1, inplace=False)                )                (lora_A): ModuleDict(                  (default): Linear(in_features=4096, out_features=8, bias=False)                )                (lora_B): ModuleDict(                  (default): Linear(in_features=8, out_features=4096, bias=False)                )                (lora_embedding_A): ParameterDict()                (lora_embedding_B): ParameterDict()              )              (rotary_emb): LlamaRotaryEmbedding()            )            (mlp): LlamaMLP(              (gate_proj): lora.Linear4bit(                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)                (lora_dropout): ModuleDict(                  (default): Dropout(p=0.1, inplace=False)                )                (lora_A): ModuleDict(                  (default): Linear(in_features=4096, out_features=8, bias=False)                )                (lora_B): ModuleDict(                  (default): Linear(in_features=8, out_features=11008, bias=False)                )                (lora_embedding_A): ParameterDict()                (lora_embedding_B): ParameterDict()              )              (up_proj): lora.Linear4bit(                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)                (lora_dropout): ModuleDict(                  (default): Dropout(p=0.1, inplace=False)                )                (lora_A): ModuleDict(                  (default): Linear(in_features=4096, out_features=8, bias=False)                )                (lora_B): ModuleDict(                  (default): Linear(in_features=8, out_features=11008, bias=False)                )                (lora_embedding_A): ParameterDict()                (lora_embedding_B): ParameterDict()              )              (down_proj): lora.Linear4bit(                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)                (lora_dropout): ModuleDict(                  (default): Dropout(p=0.1, inplace=False)                )                (lora_A): ModuleDict(                  (default): Linear(in_features=11008, out_features=8, bias=False)                )                (lora_B): ModuleDict(                  (default): Linear(in_features=8, out_features=4096, bias=False)                )                (lora_embedding_A): ParameterDict()                (lora_embedding_B): ParameterDict()              )              (act_fn): SiLU()            )            (input_layernorm): LlamaRMSNorm()            (post_attention_layernorm): LlamaRMSNorm()          )        )        (norm): LlamaRMSNorm()      )      (lm_head): Linear(in_features=4096, out_features=32008, bias=False)    )  ))
```

######       2.4.1.2 量化后模型layers

```python
---> model layersnot trainable model arguments: base_model.model.model.embed_tokens.weight - torch.bfloat16 - torch.Size([32008, 4096])not trainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.0.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.0.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.0.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.0.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.1.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.1.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.1.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.1.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.2.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.2.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.2.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.2.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.3.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.3.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.3.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.3.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.4.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.4.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.4.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.4.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.5.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.5.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.5.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.5.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.6.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.6.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.6.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.6.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.7.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.7.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.7.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.7.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.8.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.8.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.8.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.8.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.9.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.9.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.9.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.9.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.10.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.10.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.10.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.10.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.11.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.11.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.11.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.11.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.12.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.12.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.12.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.12.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.13.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.13.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.13.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.13.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.14.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.14.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.14.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.14.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.15.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.15.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.15.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.15.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.16.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.16.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.16.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.16.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.17.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.17.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.17.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.17.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.18.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.18.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.18.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.18.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.19.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.19.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.19.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.19.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.20.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.20.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.20.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.20.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.21.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.21.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.21.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.21.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.22.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.22.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.22.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.22.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.23.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.23.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.23.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.23.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.24.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.24.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.24.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.24.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.25.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.25.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.25.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.25.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.26.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.26.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.26.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.26.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.27.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.27.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.27.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.27.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.28.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.28.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.28.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.28.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.29.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.29.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.29.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.29.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.30.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.30.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.30.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.30.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.31.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.31.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.31.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.31.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.norm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.lm_head.weight - torch.bfloat16 - torch.Size([32008, 4096])
```

#####     2.4.2、参数

######      2.4.2.1 training args

```python
Training/evaluation parameters TrainingArguments(_n_gpu=1,accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},adafactor=False,adam_beta1=0.9,adam_beta2=0.999,adam_epsilon=1e-08,auto_find_batch_size=False,bf16=True,bf16_full_eval=False,data_seed=None,dataloader_drop_last=False,dataloader_num_workers=0,dataloader_persistent_workers=False,dataloader_pin_memory=True,dataloader_prefetch_factor=None,ddp_backend=None,ddp_broadcast_buffers=None,ddp_bucket_cap_mb=None,ddp_find_unused_parameters=None,ddp_timeout=1800,debug=[],deepspeed=None,disable_tqdm=False,dispatch_batches=None,do_eval=True,do_predict=False,do_train=False,eval_accumulation_steps=None,eval_delay=0,eval_steps=None,evaluation_strategy=epoch,fp16=False,fp16_backend=auto,fp16_full_eval=False,fp16_opt_level=O1,fsdp=[],fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},fsdp_min_num_params=0,fsdp_transformer_layer_cls_to_wrap=None,full_determinism=False,gradient_accumulation_steps=4,gradient_checkpointing=True,gradient_checkpointing_kwargs={'use_reentrant': True},greater_is_better=None,group_by_length=False,half_precision_backend=auto,hub_always_push=False,hub_model_id=None,hub_private_repo=False,hub_strategy=every_save,hub_token=<HUB_TOKEN>,ignore_data_skip=False,include_inputs_for_metrics=False,include_num_input_tokens_seen=False,include_tokens_per_second=False,jit_mode_eval=False,label_names=None,label_smoothing_factor=0.0,learning_rate=0.0001,length_column_name=length,load_best_model_at_end=False,local_rank=0,log_level=info,log_level_replica=warning,log_on_each_node=True,logging_dir=/workspace/output/llama-sft-qlora-dsz3/runs/Apr12_05-25-13_afa6d91ea8f6,logging_first_step=False,logging_nan_inf_filter=True,logging_steps=5,logging_strategy=steps,lr_scheduler_kwargs={},lr_scheduler_type=cosine,max_grad_norm=1.0,max_steps=-1,metric_for_best_model=None,mp_parameters=,neftune_noise_alpha=None,no_cuda=False,num_train_epochs=2.0,optim=adamw_torch,optim_args=None,optim_target_modules=None,output_dir=/workspace/output/llama-sft-qlora-dsz3,overwrite_output_dir=False,past_index=-1,per_device_eval_batch_size=2,per_device_train_batch_size=1,prediction_loss_only=False,push_to_hub=False,push_to_hub_model_id=None,push_to_hub_organization=None,push_to_hub_token=<PUSH_TO_HUB_TOKEN>,ray_scope=last,remove_unused_columns=True,report_to=['tensorboard'],resume_from_checkpoint=/workspace/output/llama-sft-qlora-dsz3/checkpoint-100,run_name=/workspace/output/llama-sft-qlora-dsz3,save_on_each_node=False,save_only_model=False,save_safetensors=True,save_steps=100,save_strategy=steps,save_total_limit=10,seed=100,skip_memory_metrics=True,split_batches=None,tf32=None,torch_compile=False,torch_compile_backend=None,torch_compile_mode=None,torchdynamo=None,tpu_metrics_debug=False,tpu_num_cores=None,use_cpu=False,use_ipex=False,use_legacy_prediction_loop=False,use_mps_device=False,warmup_ratio=0.0,warmup_steps=0,weight_decay=0.0001,)
```

######      2.4.2.2 peft args

```python
PEFT parameters LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/workspace/Llama-2-7b-chat-hf', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'q_proj', 'down_proj', 'k_proj', 'v_proj', 'up_proj', 'o_proj', 'gate_proj'}, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
```

######      2.4.2.3 model args

```python
model parameters LlamaConfig {"_name_or_path": "/workspace/Llama-2-7b-chat-hf","architectures": ["LlamaForCausalLM"  ],"attention_bias": false,"attention_dropout": 0.0,"bos_token_id": 1,"eos_token_id": 2,"hidden_act": "silu","hidden_size": 4096,"initializer_range": 0.02,"intermediate_size": 11008,"max_position_embeddings": 4096,"model_type": "llama","num_attention_heads": 32,"num_hidden_layers": 32,"num_key_value_heads": 32,"pretraining_tp": 1,"quantization_config": {"_load_in_4bit": true,"_load_in_8bit": false,"bnb_4bit_compute_dtype": "bfloat16","bnb_4bit_quant_storage": "bfloat16","bnb_4bit_quant_type": "nf4","bnb_4bit_use_double_quant": true,"llm_int8_enable_fp32_cpu_offload": false,"llm_int8_has_fp16_weight": false,"llm_int8_skip_modules": null,"llm_int8_threshold": 6.0,"load_in_4bit": true,"load_in_8bit": false,"quant_method": "bitsandbytes"  },"rms_norm_eps": 1e-05,"rope_scaling": null,"rope_theta": 10000.0,"tie_word_embeddings": false,"torch_dtype": "bfloat16","transformers_version": "4.40.0.dev0","use_cache": false,"vocab_size": 32008}
```

### 三、Trl 库

####   3.1、SFTTrainer

> 上述代码主要包含了以下几个部分:
> 
> 1. **非打包(Non-packed)数据集的准备**:
> 
> 如果`packing=False`,则使用`_prepare_non_packed_dataloader`方法对数据集进行预处理。该方法首先定义了一个`tokenize`函数,用于对每个样本进行tokenize操作,包括添加特殊标记、截断和padding等。然后使用`dataset.map`方法对整个数据集应用`tokenize`函数,得到tokenized的数据集。最后,使用PyTorch的`DataLoader`创建数据加载器,方便在训练时对数据进行批处理。
> 
> 1. **打包(Packed)数据集的准备**:
> 
> 如果`packing=True`,则使用`_prepare_packed_dataloader`方法对数据集进行预处理。该方法使用`ConstantLengthDataset`类对数据集进行打包。`ConstantLengthDataset`是TRL库中的一个自定义数据集类,它可以将不同长度的文本序列打包到同一个张量中,从而提高内存利用率和训练速度。在创建`ConstantLengthDataset`时,需要指定文本字段名称、最大序列长度、每个token的字符数估计值等参数。然后,使用`ConstantLengthBatchSampler`进行批采样,并创建`DataLoader`对象。
> 
> 1. **NEFTune噪声嵌入的激活和移除**:
> 
> `_trl_activate_neftune`方法用于激活NEFTune噪声嵌入。NEFTune是一种在输入嵌入中注入噪声的技术,可以提高模型在指令微调任务中的性能。该方法获取模型的输入嵌入层,然后注册一个钩子函数,在前向传播时将高斯噪声加入到输入嵌入中。`_trl_unwrap_neftune`方法则用于移除NEFTune噪声嵌入,恢复模型原始的前向传播逻辑。
> 
> 需要注意的是,NEFTune噪声嵌入只在模型训练时生效,在推理时不会添加噪声。此外,该技术主要用于指令微调任务,对于其他任务的效果尚无定论。
> 
> 总的来说,上述代码实现了数据集的预处理、加载,以及NEFTune噪声嵌入的激活和移除功能,为监督微调提供了必要的支持。详细的注释有助于理解每个部分的作用和相关技术细节。

```python
class SFTTrainer(Trainer):r"""    监督微调训练器(SFT Trainer)的类定义。    这个类是对transformers.Trainer类的包装,继承了其所有的属性和方法。    当用户传入PeftConfig对象时,该训练器会负责正确地初始化PeftModel。    参数:        model (Union[\`transformers.PreTrainedModel\`, \`nn.Module\`, \`str\`]):            要训练的模型,可以是一个预训练的transformers模型(PreTrainedModel)、一个自定义的PyTorch模块(nn.Module)或一个字符串(表示要从Hugging Face缓存或在线下载的预训练模型名称)。            如果传入了PeftConfig对象,该模型也可以转换为PeftModel(一种用于高效微调的模型结构)。        args (Optional[\`transformers.TrainingArguments\`]):            微调训练的参数配置,包括诸如学习率、批次大小、训练epoches等超参数设置。请参考transformers.TrainingArguments的官方文档以了解更多详细信息。        data_collator (Optional[\`transformers.DataCollator\`]):            用于训练的数据收集器(DataCollator)。DataCollator负责对样本进行padding、batching等操作,以便于输入模型进行训练。如果未指定,将使用默认的DataCollator。        train_dataset (Optional[\`datasets.Dataset\`]):            用于训练的数据集,可以是一个Hugging Face datasets或者PyTorch Dataset。我们建议使用trl.trainer.ConstantLengthDataset创建数据集,这种格式对于序列长度可变的语料来说更加高效。        eval_dataset (Optional[Union[\`datasets.Dataset\`, Dict[\`str\`, \`datasets.Dataset\`]]]):            用于评估的数据集,可以是一个单独的datasets.Dataset,也可以是一个将数据集名称映射到对应数据集对象的字典。我们建议使用trl.trainer.ConstantLengthDataset创建数据集。        tokenizer (Optional[\`transformers.PreTrainedTokenizer\`]):            用于训练的分词器(tokenizer),如果未指定,将使用与模型关联的默认分词器。分词器负责将原始文本转换为模型可以理解的token ID序列。        model_init (\`Callable[[], transformers.PreTrainedModel]\`):            用于训练的模型初始化函数,如果未指定,将使用默认的模型初始化函数。该函数应该返回一个预训练的模型实例。        compute_metrics (\`Callable[[transformers.EvalPrediction], Dict]\`, *optional* defaults to None):            用于计算评估指标的函数,它接收一个transformers.EvalPrediction对象作为输入,并返回一个将指标名称映射到指标值的字典。如果未指定,评估过程中只会计算损失(loss)。        callbacks (\`List[transformers.TrainerCallback]\`):            用于训练的回调函数列表。回调函数可以在训练的不同阶段执行自定义操作,如记录日志、保存模型检查点等。        optimizers (\`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]\`):            用于训练的优化器(Optimizer)和学习率调度器(LRScheduler)对象。如果未指定,将使用默认的优化器和学习率调度器。        preprocess_logits_for_metrics (\`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\`):            用于在计算指标之前预处理模型输出(logits)的函数。该函数接收模型的原始输出(logits)和标签(labels)作为输入,并返回预处理后的logits。        peft_config (\`Optional[PeftConfig]\`):            用于初始化PeftModel的PeftConfig对象。PeftModel是一种高效的微调方法,可以显著减少需要更新的参数数量,从而加快微调速度并节省内存。        dataset_text_field (\`Optional[str]\`):            数据集中文本字段的名称,如果传入,训练器将自动基于该字段创建ConstantLengthDataset。ConstantLengthDataset是一种高效的数据格式,适用于序列长度可变的语料。        formatting_func (\`Optional[Callable]\`):            用于创建ConstantLengthDataset的格式化函数。该函数接收一个样本作为输入,并返回一个经过预处理的字符串列表,用于构建输入序列。如果未指定,将使用默认的格式化函数。        max_seq_length (\`Optional[int]\`):            用于ConstantLengthDataset和自动创建数据集的最大序列长度,默认为512。超过该长度的序列将被截断。        infinite (\`Optional[bool]\`):            是否使用无限数据集,默认为False。如果设置为True,训练将在达到max_steps或max_epochs时停止,而不会因为数据集被遍历完而停止。(此参数已弃用,建议使用TrainingArguments中的max_steps或num_train_epochs参数来控制训练长度)        num_of_sequences (\`Optional[int]\`):            ConstantLengthDataset使用的序列数量,默认为1024。该参数控制了ConstantLengthDataset在内存中缓存的序列数量。        chars_per_token (\`Optional[float]\`):            ConstantLengthDataset使用的每个token的字符数,默认为3.6。该参数用于估计输入序列的长度,以便对序列进行截断和padding操作。您可以在stack-llama示例中查看如何计算该值。        packing (\`Optional[bool]\`):            仅在传入dataset_text_field时使用。如果设置为True,则使用ConstantLengthDataset对数据进行打包,这种格式更加高效,尤其是在处理长序列时。如果设置为False,则使用默认的DataCollatorForLanguageModeling对数据进行处理。        dataset_num_proc (\`Optional[int]\`):            用于标记数据的工作进程数,仅在packing=False时使用,默认为None,即使用主进程进行标记。增加工作进程数量可以加速数据预处理的速度。        dataset_batch_size (\`int\`):            每批标记的示例数量,如果batch_size <= 0或batch_size == None,则将整个数据集标记为单个批次,默认为1000。该参数控制了数据预处理的内存占用和速度,需要根据实际情况进行调整。        neftune_noise_alpha (\`Optional[float]\`):            如果不为None,这将激活NEFTune噪声嵌入。NEFTune是一种噪声注入技术,它通过在输入嵌入中添加噪声,可以提高模型在指令微调任务中的性能。具体细节请参考原论文和代码。        model_init_kwargs: (\`Optional[Dict]\`, *optional*):            实例化模型(从字符串)时传递的可选关键字参数,如指定模型权重文件的本地路径等。        dataset_kwargs: (\`Optional[Dict]\`, *optional*):            创建打包或非打包数据集时传递的可选关键字参数,用于对数据集的构建行为进行更多控制。        eval_packing: (\`Optional[bool]\`, *optional*):            是否也对评估数据集进行打包,如果为None,则默认为packing参数的值。即如果训练数据集使用了打包,评估数据集也会使用打包,反之亦然。    """    _tag_names = ["trl", "sft"]  def __init__(        self,        model: Optional[Union[PreTrainedModel, nn.Module, str]] = None,        args: Optional[TrainingArguments] = None,        data_collator: Optional[DataCollator] = None,          train_dataset: Optional[Dataset] = None,        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,        tokenizer: Optional[PreTrainedTokenizerBase] = None,        model_init: Optional[Callable[[], PreTrainedModel]] = None,        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,        callbacks: Optional[List[TrainerCallback]] = None,        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),        preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,        peft_config: Optional["PeftConfig"] = None,        dataset_text_field: Optional[str] = None,        packing: Optional[bool] = False,        formatting_func: Optional[Callable] = None,        max_seq_length: Optional[int] = None,        infinite: Optional[bool] = None,        num_of_sequences: Optional[int] = 1024,        chars_per_token: Optional[float] = 3.6,        dataset_num_proc: Optional[int] = None,        dataset_batch_size: int = 1000,        neftune_noise_alpha: Optional[float] = None,        model_init_kwargs: Optional[Dict] = None,        dataset_kwargs: Optional[Dict] = None,        eval_packing: Optional[bool] = None,    ):if model_init_kwargs is None:            model_init_kwargs = {}elif not isinstance(model, str):raise ValueError("You passed model_kwargs to the SFTTrainer. But your model is already instantiated.")if infinite is not None:            warnings.warn("The \`infinite\` argument is deprecated and will be removed in a future version of TRL. Use \`TrainingArguments.max_steps\` or \`TrainingArguments.num_train_epochs\` instead to control training length."            )if isinstance(model, str):            warnings.warn("You passed a model_id to the SFTTrainer. This will automatically create an ""\`AutoModelForCausalLM\` or a \`PeftModel\` (if you passed a \`peft_config\`) for you."            )            model = AutoModelForCausalLM.from_pretrained(model, **model_init_kwargs)if packing and data_collator is not None and isinstance(data_collator, DataCollatorForCompletionOnlyLM):raise ValueError("You passed a \`DataCollatorForCompletionOnlyLM\` to the SFTTrainer. This is not compatible with the \`packing\` argument."            )if is_peft_available() and peft_config is not None:if not isinstance(peft_config, PeftConfig):raise ValueError("If you want to use the PeftModel, you need to pass a PeftConfig object to the SFTTrainer."f" and you passed a {type(peft_config)}."                )if not isinstance(model, PeftModel):                _support_gc_kwargs = hasattr(                    args, "gradient_checkpointing_kwargs"                ) and "gradient_checkpointing_kwargs" in list(                    inspect.signature(prepare_model_for_kbit_training).parameters                )                gradient_checkpointing_kwargs = getattr(args, "gradient_checkpointing_kwargs", None) or {}                is_sharded_qlora = Falseif getattr(model, "is_loaded_in_4bit", False):for _, param in model.named_parameters():if param.__class__.__name__ == "Params4bit":                            is_sharded_qlora = param.data.device.type == "cpu"breakif getattr(model, "is_loaded_in_8bit", False) or (getattr(model, "is_loaded_in_4bit", False) and not is_sharded_qlora                ):                    prepare_model_kwargs = {"use_gradient_checkpointing": getattr(args, "gradient_checkpointing", False)                    }if _support_gc_kwargs:                        prepare_model_kwargs["gradient_checkpointing_kwargs"] = gradient_checkpointing_kwargs                    model = prepare_model_for_kbit_training(model, **prepare_model_kwargs)if args is not None:                        args = dataclasses.replace(args, gradient_checkpointing=False)elif getattr(args, "gradient_checkpointing", False) and ("use_reentrant" not in gradient_checkpointing_kwargsor gradient_checkpointing_kwargs["use_reentrant"]                ):if hasattr(model, "enable_input_require_grads"):                        model.enable_input_require_grads()else:def make_inputs_require_grad(module, input, output):                            output.requires_grad_(True)                        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)                model = get_peft_model(model, peft_config)if (                    args is not Noneand args.bf16and getattr(model, "is_loaded_in_4bit", False)and not is_sharded_qlora                ):                    peft_module_casting_to_bf16(model)if tokenizer is None:            tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)if getattr(tokenizer, "pad_token", None) is None:                tokenizer.pad_token = tokenizer.eos_tokenif max_seq_length is None:            max_seq_length = min(tokenizer.model_max_length, 1024)            warnings.warn(f"You didn't pass a \`max_seq_length\` argument to the SFTTrainer, this will default to {max_seq_length}"            )        self.dataset_num_proc = dataset_num_proc        self.dataset_batch_size = dataset_batch_size        self._trainer_supports_neftune = hasattr(args, "neftune_noise_alpha")if neftune_noise_alpha is not None and self._trainer_supports_neftune:            args.neftune_noise_alpha = neftune_noise_alpha            warnings.warn("You passed a \`neftune_noise_alpha\` argument to the SFTTrainer, the value you passed will override the one in the \`TrainingArguments\`."            )elif not self._trainer_supports_neftune:            self.neftune_noise_alpha = neftune_noise_alphaif formatting_func is None and dataset_text_field is None:            formatting_func = get_formatting_func_from_dataset(train_dataset, tokenizer)if not packing:if dataset_text_field is None and formatting_func is None:raise ValueError("You passed \`packing=False\` to the SFTTrainer, but you didn't pass a \`dataset_text_field\` or \`formatting_func\` argument."                )if data_collator is None:                data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)with PartialState().local_main_process_first():if dataset_kwargs is None:                dataset_kwargs = {}if train_dataset is not None:                train_dataset = self._prepare_dataset(                    train_dataset,                    tokenizer,                    packing,                    dataset_text_field,                    max_seq_length,                    formatting_func,                    num_of_sequences,                    chars_per_token,                    remove_unused_columns=args.remove_unused_columns if args is not None else True,                    **dataset_kwargs,                )if eval_dataset is not None:                _multiple = isinstance(eval_dataset, dict)                _eval_datasets = eval_dataset if _multiple else {"singleton": eval_dataset}                eval_packing = packing if eval_packing is None else eval_packingfor _eval_dataset_name, _eval_dataset in _eval_datasets.items():                    _eval_datasets[_eval_dataset_name] = self._prepare_dataset(                        _eval_dataset,                        tokenizer,                        eval_packing,                        dataset_text_field,                        max_seq_length,                        formatting_func,                        num_of_sequences,                        chars_per_token,                        remove_unused_columns=args.remove_unused_columns if args is not None else True,                        **dataset_kwargs,                    )if not _multiple:                    eval_dataset = _eval_datasets["singleton"]if tokenizer.padding_side is not None and tokenizer.padding_side != "right":            warnings.warn("You passed a tokenizer with \`padding_side\` not equal to \`right\` to the SFTTrainer. This might lead to some unexpected behaviour due to ""overflow issues when training a model in half-precision. You might consider adding \`tokenizer.padding_side = 'right'\` to your code."            )super().__init__(            model=model,            args=args,            data_collator=data_collator,            train_dataset=train_dataset,            eval_dataset=eval_dataset,            tokenizer=tokenizer,            model_init=model_init,            compute_metrics=compute_metrics,            callbacks=callbacks,            optimizers=optimizers,            preprocess_logits_for_metrics=preprocess_logits_for_metrics,        )if hasattr(self.model, "add_model_tags"):            self.model.add_model_tags(self._tag_names)if self.args.max_steps > 0 and packing:            warnings.warn("You passed \`packing=True\` to the SFTTrainer, and you are training your model with \`max_steps\` strategy. The dataset will be iterated until the \`max_steps\` are reached."            )            self.train_dataset.infinite = Trueelif self.args.max_steps == -1 and packing:            self.train_dataset.infinite = Falseif any(isinstance(callback, RichProgressCallback) for callback in self.callback_handler.callbacks):for callback in self.callback_handler.callbacks:if callback.__class__.__name__ == "PrinterCallback":                    self.callback_handler.pop_callback(callback)    @wraps(Trainer.train)def train(self, *args, **kwargs):if self.neftune_noise_alpha is not None and not self._trainer_supports_neftune:            self.model = self._trl_activate_neftune(self.model)        output = super().train(*args, **kwargs)if self.neftune_noise_alpha is not None and not self._trainer_supports_neftune:            unwrapped_model = unwrap_model(self.model)if is_peft_available() and isinstance(unwrapped_model, PeftModel):                embeddings = unwrapped_model.base_model.model.get_input_embeddings()else:                embeddings = unwrapped_model.get_input_embeddings()            self.neftune_hook_handle.remove()del embeddings.neftune_noise_alphareturn output    @wraps(Trainer.push_to_hub)def push_to_hub(self, commit_message: Optional[str] = "End of training", blocking: bool = True, **kwargs) -> str:"""        覆写push_to_hub方法,在推送模型到Hub时强制添加"sft"标签。        更多详细信息请参考transformers.Trainer.push_to_hub。        """        kwargs = trl_sanitze_kwargs_for_tagging(model=self.model, tag_names=self._tag_names, kwargs=kwargs)return super().push_to_hub(commit_message=commit_message, blocking=blocking, **kwargs)def _prepare_dataset(        self,        dataset,        tokenizer,        packing,        dataset_text_field,        max_seq_length,        formatting_func,        num_of_sequences,        chars_per_token,        remove_unused_columns=True,        append_concat_token=True,        add_special_tokens=True,        skip_prepare_dataset=False,    ):if dataset is None:raise ValueError("The dataset should not be None")if skip_prepare_dataset:return dataset        column_names = (            dataset.column_names if isinstance(dataset, (datasets.Dataset, datasets.IterableDataset)) else None        )if column_names and "input_ids" in column_names:return datasetif isinstance(            dataset, (torch.utils.data.IterableDataset, torch.utils.data.Dataset, ConstantLengthDataset)        ) and not isinstance(dataset, datasets.IterableDataset):return datasetif not packing:return self._prepare_non_packed_dataloader(                tokenizer,                dataset,                dataset_text_field,                max_seq_length,                formatting_func,                add_special_tokens,                remove_unused_columns,            )else:return self._prepare_packed_dataloader(                tokenizer,                dataset,                dataset_text_field,                max_seq_length,                num_of_sequences,                chars_per_token,                formatting_func,                append_concat_token,                add_special_tokens,            )def _prepare_non_packed_dataloader(        self,        tokenizer,        dataset,        dataset_text_field,        max_seq_length,        formatting_func=None,        add_special_tokens=True,        remove_unused_columns=True,    ):        use_formatting_func = formatting_func is not None and dataset_text_field is None        self._dataset_sanity_checked = Falsedef tokenize(element):            outputs = tokenizer(                element[dataset_text_field] if not use_formatting_func else formatting_func(element),                add_special_tokens=add_special_tokens,                truncation=True,                padding=False,                max_length=max_seq_length,                return_overflowing_tokens=False,                return_length=False,            )if use_formatting_func and not self._dataset_sanity_checked:if not isinstance(formatting_func(element), list):raise ValueError("The \`formatting_func\` should return a list of processed strings since it can lead to silent bugs."                    )else:                    self._dataset_sanity_checked = Truereturn {"input_ids": outputs["input_ids"], "attention_mask": outputs["attention_mask"]}        signature_columns = ["input_ids", "labels", "attention_mask"]        extra_columns = list(set(dataset.column_names) - set(signature_columns))if not remove_unused_columns and len(extra_columns) > 0:            warnings.warn("You passed \`remove_unused_columns=False\` on a non-packed dataset. This might create some issues with the default collator and yield to errors. If you want to "f"inspect dataset other columns (in this case {extra_columns}), you can subclass \`DataCollatorForLanguageModeling\` in case you used the default collator and create your own data collator in order to inspect the unused dataset columns."            )        tokenized_dataset = dataset.map(            tokenize,            batched=True,            remove_columns=dataset.column_names if remove_unused_columns else None,            num_proc=self.dataset_num_proc,            batch_size=self.dataset_batch_size,        )return tokenized_datasetdef _prepare_packed_dataloader(        self,        tokenizer,        dataset,        dataset_text_field,        max_seq_length,        num_of_sequences,        chars_per_token,        formatting_func=None,        append_concat_token=True,        add_special_tokens=True,    ):        use_formatting_func = formatting_func is not None and dataset_text_field is None        constant_length_dataset = ConstantLengthDataset(            dataset,            text_field=dataset_text_field,            formatting_func=formatting_func,            tokenizer=tokenizer,            max_seq_length=max_seq_length,            num_of_sequences=num_of_sequences,            chars_per_token=chars_per_token,            append_concat_token=append_concat_token,            add_special_tokens=add_special_tokens,            use_formatting_func=use_formatting_func,            num_proc=self.dataset_num_proc,            overwrite_cache=self.args.overwrite_cache,        )        data_loader = DataLoader(            constant_length_dataset,            batch_sampler=ConstantLengthBatchSampler(                constant_length_dataset,                batch_size=self.args.train_batch_size,                shuffle=True,                seed=self.args.seed,                epochs=self.args.num_train_epochs if self.args.max_steps <= 0 else int(1e6),            ),            collate_fn=self.data_collator,        )return data_loaderdef _trl_activate_neftune(self, model):"""        激活NEFTune噪声嵌入。NEFTune是一种在输入嵌入中注入噪声的技术,可以提高模型在指令微调任务中的性能。        该函数会修改模型的前向传播逻辑,在输入嵌入中加入高斯噪声。        """from transformers.models.auto import AutoModelif is_peft_available() and isinstance(model, PeftModel):            embeddings = model.base_model.model.get_input_embeddings()else:            embeddings = model.get_input_embeddings()def neftune_forward(embeds, input_ids):            noise = torch.randn(embeds.shape, device=embeds.device) * self.neftune_noise_alpha            embeds = embeds + noisereturn embeds        self.neftune_hook_handle = embeddings.register_forward_hook(neftune_forward)        embeddings.neftune_noise_alpha = self.neftune_noise_alphareturn modeldef _trl_unwrap_neftune(self, model):"""        移除NEFTune噪声嵌入,恢复模型原始的前向传播逻辑。        """from transformers.models.auto import AutoModel        unwrapped_model = unwrap_model(model)if is_peft_available() and isinstance(unwrapped_model, PeftModel):            embeddings = unwrapped_model.base_model.model.get_input_embeddings()else:            embeddings = unwrapped_model.get_input_embeddings()        self.neftune_hook_handle.remove()del embeddings.neftune_noise_alphareturn unwrapped_model
```

####   3.2、其他的代码

#####     3.2.1、datasets.map 使用 load\_from\_cache\_file = False 方便调试​​​​​​​​​​​​​​

[https://huggingface.co/docs/datasets/v2.18.0/en/package\_reference/main\_classes#datasets.Dataset.map![icon-default.png?t=N7T8](https://i-blog.csdnimg.cn/blog_migrate/003a2ce7eb50c2e24a8c624c260c5930.png)https://huggingface.co/docs/datasets/v2.18.0/en/package\_reference/main\_classes#datasets.Dataset.map](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.Dataset.map "https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.Dataset.map")

```python
datasets = load_dataset( ... )dataset.map(            process_fn,               batched=batched,                  remove_columns=remove_columns,              num_proc=self._num_proc,               load_from_cache_file = False         )
```

### 四、小结

####   4.1、在SFTTrainer初始化peft模型时，为什么 开启了 QLoRA + FSDP / DS-Zero3 后不使用prepare\_model\_for\_kbit\_training 和 peft\_module\_casting\_to\_bf16 ，prepare\_model\_for\_kbit\_training 和 peft\_module\_casting\_to\_bf16 做了什么？QLoRA + FSDP / DS-Zero3 未开启offload​​​​​​​​​​​​​​模型加载后model为什么在cpu上？

> 首先,我们需要了解一些基本概念:
> 
> - 量化 (Quantization):将模型权重从高精度浮点数(如32位浮点数)转换为低精度(如8位整数或4位整数)的过程。这可以显著减小模型尺寸,加速推理速度,但可能略微影响模型性能。
> - LoRA (Low-Rank Adaptation):一种参数高效微调技术,通过在模型的权重矩阵中添加低秩分解矩阵,实现以更少的参数对预训练语言模型进行微调。
> - PEFT (Parameter-Efficient Fine-Tuning):一类参数高效微调技术的统称,LoRA就是其中之一。这些技术旨在以更少的参数对大型预训练模型进行微调,以减少计算资源需求。
> - FSDP (Fully Sharded Data Parallelism):一种分布式训练技术,通过将模型权重分片到不同的GPU上,可以支持训练更大的模型。
> - DeepSpeed:由微软开发的深度学习优化库,提供了多种技术来加速和扩展模型训练,如ZeRO(Zero Redundancy Optimizer)。
> - bf16 (Brain Float 16):介于fp32和fp16之间的一种浮点数格式,在保留较大值域的同时,可以进一步节省内存和计算资源。

> 深入分析一下问题。首先,解释一下`prepare_model_for_kbit_training` 和`peft_module_casting_to_bf16` 这两个函数的作用,然后再说明为什么在使用 QLoRA 和 FSDP/DeepSpeed Zero-3 的情况下,不需要再调用这两个函数。最后, 解释为什么在这种情况下,部分模型权重会被加载到 CPU 上？

> - ​​​​​​​`prepare_model_for_kbit_training` 函数，这个函数的主要目的是为了在使用 8 位或 4 位量化**原模型**进行训练时,对模型进行一些必要的准备工作。它主要做了以下几件事:​​​​​​​
> - ​​​​​​​将LayerNorm层的权重转换为float32精度:在低精度(如8位或4位)训练时,LayerNorm层的权重需要保持较高精度,以确保数值稳定性。
> - 确保输出嵌入层(output embedding layer)的参数需要计算梯度:这是为了确保整个模型都能够正确地进行反向传播和更新。
> - 将语言模型头(lm head)的输出转换为float32精度:同样,这是为了在低精度训练时保持数值稳定性。
> - 启用梯度检查点(Gradient Checkpointing):这是一种节省内存的技术,通过在前向传播过程中丢弃一些中间激活值,并在反向传播时重新计算它们,以减少内存消耗。
> 
> 1. 总的来说,`prepare_model_for_kbit_training` 的作用是让低精度量化模型在训练时获得更好的数值稳定性和性能。

```python
def prepare_model_for_kbit_training(model, use_gradient_checkpointing=True, gradient_checkpointing_kwargs=None):r"""    Note this method only works for \`transformers\` models.    This method wraps the entire protocol for preparing a model before running a training. This includes:        1- Cast the layernorm in fp32 2- making output embedding layer require grads 3- Add the upcasting of the lm        head to fp32    Args:        model (\`transformers.PreTrainedModel\`):            The loaded model from \`transformers\`        use_gradient_checkpointing (\`bool\`, *optional*, defaults to \`True\`):            If True, use gradient checkpointing to save memory at the expense of slower backward pass.        gradient_checkpointing_kwargs (\`dict\`, *optional*, defaults to \`None\`):            Keyword arguments to pass to the gradient checkpointing function, please refer to the documentation of            \`torch.utils.checkpoint.checkpoint\` for more details about the arguments that you can pass to that method.            Note this is only available in the latest transformers versions (> 4.34.1).    """    loaded_in_kbit = getattr(model, "is_loaded_in_8bit", False) or getattr(model, "is_loaded_in_4bit", False)    is_gptq_quantized = getattr(model, "quantization_method", None) == "gptq"    is_aqlm_quantized = getattr(model, "quantization_method", None) == "aqlm"if gradient_checkpointing_kwargs is None:        gradient_checkpointing_kwargs = {}for name, param in model.named_parameters():        param.requires_grad = Falseif not is_gptq_quantized and not is_aqlm_quantized:for param in model.parameters():if (param.dtype == torch.float16) or (param.dtype == torch.bfloat16):if param.__class__.__name__ != "Params4bit":                    param.data = param.data.to(torch.float32)if (loaded_in_kbit or is_gptq_quantized or is_aqlm_quantized) and use_gradient_checkpointing:        _supports_gc_kwargs = "gradient_checkpointing_kwargs" in list(            inspect.signature(model.gradient_checkpointing_enable).parameters        )if not _supports_gc_kwargs and len(gradient_checkpointing_kwargs) > 0:            warnings.warn("gradient_checkpointing_kwargs is not supported in this version of transformers. The passed kwargs will be ignored."" if you want to use that feature, please upgrade to the latest version of transformers.",                FutureWarning,            )        gc_enable_kwargs = {} if not _supports_gc_kwargs else {"gradient_checkpointing_kwargs": gradient_checkpointing_kwargs}        model.gradient_checkpointing_enable(**gc_enable_kwargs)return model
```

> - ​​​​​​​`peft_module_casting_to_bf16` 函数:
> - 它遍历模型的所有子模块,找到 PEFT 模块(如 LoRA 的低秩分解矩阵),并将其转换为 bfloat16 精度。
> - 对于LayerNorm层和其他normalization层,它将其权重转换为float32精度,以保持数值稳定性。
> - 对于语言模型头(lm head)、词嵌入(embed tokens)等部分,如果它们的权重是float32精度,也会被转换为bfloat16精度。
> 
> 1. 这个函数的作用是将 **PEFT 模型**(如 LoRA 模型)中**需要训练的参数**转换为 bfloat16 精度。具体来说: 通过将需要训练的参数转换为 bfloat16 精度,可以在保持较高精度的同时,进一步减少内存占用,提高训练效率。

```python
def peft_module_casting_to_bf16(model):from peft.tuners.tuners_utils import BaseTunerLayerfor name, module in model.named_modules():if isinstance(module, BaseTunerLayer):            module = module.to(torch.bfloat16)elif isinstance(module, torch.nn.LayerNorm) or "norm" in name:            module = module.to(torch.float32)elif any(x in name for x in ["lm_head", "embed_tokens", "wte", "wpe"]):if hasattr(module, "weight"):if module.weight.dtype == torch.float32:                    module = module.to(torch.bfloat16)
```

> - 为什么在使用 QLoRA 和 FSDP/DeepSpeed Zero-3 的情况下,不需要再调用这两个函数?
> - 先前模型from\_pretrained 已经指定了未量化层的数据类型，为了节省计算资源，不会将使用`prepare_model_for_kbit_training再将`这些层转化float32。QLoRA和FSDP/DeepSpeed Zero-3是两种互补的技术​​​​​​​, 我们不再需要单独调用 prepare\_model\_for\_kbit\_training 和 peft\_module\_casting\_to\_bf16,因为QLoRA和FSDP/DeepSpeed Zero-3已经自动处理了相关的优化工作。
> - ![](https://i-blog.csdnimg.cn/blog_migrate/d20abf34be46590f946a01d5585a34d2.png)
> 
> ​​​​​​​
> - 为什么在使用 QLoRA 和 FSDP/DeepSpeed Zero-3 的没有开启offload情况下,部分模型权重会被加载到 CPU 上?​​​​​​​
> - 模型在QLoRA 和 FSDP/DeepSpeed Zero-3加载的过程中会自动设置 low\_cpu\_mem\_usage == True​​​​​​​​​​​​​​ 
> 
> ![](https://i-blog.csdnimg.cn/blog_migrate/20d517a440eaeeaa74b16006410a42c9.png)
> - - 虽然在CPU和GPU之间移动数据会引入一些性能开销,但对于超大型模型来说,这种权衡是值得的,因为它允许我们在有限的GPU资源下训练更大的模型。
> - 当QLoRA和FSDP/DeepSpeed Zero-3协同工作时,QLoRA负责将大部分权重量化并冻结,而FSDP/DeepSpeed Zero-3则将这些冻结的量化权重分片存储在CPU上,以节省GPU显存。
> - FSDP和DeepSpeed Zero-3在分布式训练时,会将部分模型权重分片存储在CPU上,以进一步减少GPU显存占用。这样可以支持训练更大的模型。
> - QLoRA会将大部分模型权重量化为低精度(如4位或8位),以减小模型尺寸和提高计算效率。这部分量化后的权重在训练过程中是冻结的,不需要更新。

> - **实验暂定结果：**
> - QLoRA 和 FSDP/DeepSpeed Zero-3 同时启动时，单机多卡训练时，开启offload会报错，不开启offload正常运行，单机单卡未进行测试。

####   4.2、bfloat16和float16的区别

> bfloat16和float16的区别主要体现在以下几个方面:
> 
> 1. **精度**:
> 
> - bfloat16的动态范围更大,但精度略低于float16。
> - bfloat16的指数位有8个bit,尾数位有7个bit,而float16的指数位有5个bit,尾数位有10个bit。
> - 这意味着bfloat16能够表示更大范围的数值,但每个数值的精度略低于float16。
> 2. **计算性能**:
> 
> - bfloat16的计算性能优于float16,因为其指数位更多,计算过程更加高效。
> - 在某些硬件平台(如 Intel 的 Xeon Cascade Lake 和 AMD 的 EPYC 处理器)上,bfloat16 的计算性能甚至可以与float32相媲美。
> 3. **内存占用**:
> 
> - bfloat16和float16都占用16个bit,因此在内存占用上是相同的。

####   4.3、绝对位置编码与相对位置编码的区别，为什么现在的大模型都使用RoPE

> 绝对位置编码(Absolute Positional Encoding)和相对位置编码(Relative Positional Encoding)是两种不同的位置编码方式,各自有其优缺点。RoPE属于相对位置编码的一种,近年来被大型语言模型广泛采用,主要有以下原因:
> 
> - **缓解长序列建模问题**
> 
> 绝对位置编码通过为每个位置分配一个唯一的编码向量来表示位置信息。但是,当序列长度增加时,模型需要学习更多的位置编码向量,这导致计算和内存开销急剧增加。相对位置编码则通过相对距离来编码位置信息,可以有效缓解这个问题,使模型能够更好地处理长序列输入。
> 
> - **捕捉序列中的周期性相对位置的信息**
> 
> 相对位置编码能够更好地捕捉序列中的结构信息和周期性模式。例如,在自然语言处理任务中,相邻词语之间的相对位置对于理解句子结构和语义是非常重要的。RoPE通过对嵌入向量进行旋转操作,可以有效地编码这种相对位置信息。
> 
> - **计算效率高、参数少**
> 
> 与绝对位置编码相比,RoPE只需要很少的参数就可以实现有效的位置编码。这种参数高效性对于大型语言模型来说是非常重要的,因为它们通常具有数十亿个参数,参数效率直接影响模型的计算和存储成本。
> 
> - **实验性能较好性能提升**
> 
> 许多研究表明,在各种自然语言处理任务上,采用RoPE的模型比使用绝对位置编码的模型表现更好。这种性能提升主要归因于RoPE更好地捕捉了序列数据中的结构信息。

### 五、Trl 其他Trainer注释笔记

####   5.1、DPOTrainer笔记​​​​​​​​​​​​​​

[Trl中DPOTrainer注释解析(待完成)![icon-default.png?t=N7T8](https://i-blog.csdnimg.cn/blog_migrate/003a2ce7eb50c2e24a8c624c260c5930.png)https://blog.csdn.net/qq\_16555103/article/details/137743362?csdn\_share\_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22137743362%22%2C%22source%22%3A%22qq\_16555103%22%7D](https://blog.csdn.net/qq_16555103/article/details/137743362?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22137743362%22%2C%22source%22%3A%22qq_16555103%22%7D "Trl中DPOTrainer注释解析(待完成)")

####  5.2、... 

待更新....