---
title: "Trl SFT: llama2-7b-hfä½¿ç”¨QLora 4bité‡åŒ–åŽds zero3åŠ ä¸Šflash atten v2å•æœºå¤šå¡è®­ç»ƒ(ç¬”è®°)_unsloth å¤šå¡-CSDNåšå®¢"
source: "https://blog.csdn.net/qq_16555103/article/details/137677561"
author:
published:
created: 2025-02-17
description: "æ–‡ç« æµè§ˆé˜…è¯»2.2kæ¬¡ï¼Œç‚¹èµž20æ¬¡ï¼Œæ”¶è—30æ¬¡ã€‚ç¬¬ä¸‰ å‚è€ƒå®˜æ–¹å‘½ä»¤: https://github.com/Dao-AILab/flash-attentionã€‚ç¬¬ä¸€ ç¡®ä¿ linux \"å¤–ç•Œ\"çš„ cudaç‰ˆæœ¬ ä¸Ž conda è™šæ‹ŸçŽ¯å¢ƒä¸­cudaç‰ˆæœ¬ä¸€è‡´ã€‚ç¬¬äºŒ å®‰è£…å¥½ c++ g++ ninjaã€‚_unsloth å¤šå¡"
tags:
  - "clippings"
---
**ç›®å½•**

[ä¸€ã€çŽ¯å¢ƒ](https://blog.csdn.net/qq_16555103/article/details/#t0)

Â  [1.1ã€çŽ¯å¢ƒå®‰è£…](https://blog.csdn.net/qq_16555103/article/details/#t1)

Â  [1.2ã€å®‰è£…flash atten](https://blog.csdn.net/qq_16555103/article/details/#t2)

Â  [1.3ã€vscodeè¿œç«¯å¯èƒ½é‡åˆ°çš„ä¸€äº›é—®é¢˜](https://blog.csdn.net/qq_16555103/article/details/#t3)

[äºŒã€ä»£ç ](https://blog.csdn.net/qq_16555103/article/details/#t4)

Â  [2.1ã€bashè„šæœ¬](https://blog.csdn.net/qq_16555103/article/details/#t5)Â 

Â  [2.2ã€utils.py æ³¨é‡Šä¸Žä¼˜åŒ–](https://blog.csdn.net/qq_16555103/article/details/#t6)

Â  [2.3ã€train.py æ³¨é‡Šä¸Žä¼˜åŒ–](https://blog.csdn.net/qq_16555103/article/details/#t7)

Â  [2.4ã€æ¨¡åž‹/å‚æ•°ç›¸å…³](https://blog.csdn.net/qq_16555103/article/details/#t8)

Â  Â  [2.4.1ã€é‡åŒ–åŽçš„æ¨¡åž‹](https://blog.csdn.net/qq_16555103/article/details/#%C2%A0%20%C2%A0%202.3.1%E3%80%81%E9%87%8F%E5%8C%96%E5%90%8E%E7%9A%84%E6%A8%A1%E5%9E%8B)

Â  Â  Â  [2.4.1.1Â é‡åŒ–åŽæ¨¡åž‹ç»“æž„](https://blog.csdn.net/qq_16555103/article/details/#%C2%A0%20%C2%A0%20%C2%A0%20a%29%C2%A0%E9%87%8F%E5%8C%96%E5%90%8E%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84)

Â  Â  Â  [2.4.1.2 é‡åŒ–åŽæ¨¡åž‹layers](https://blog.csdn.net/qq_16555103/article/details/#%C2%A0%20%C2%A0%20%C2%A0%20b%29%C2%A0%E9%87%8F%E5%8C%96%E5%90%8E%E6%A8%A1%E5%9E%8Blayers)

Â  Â  [2.4.2ã€å‚æ•°](https://blog.csdn.net/qq_16555103/article/details/#%C2%A0%20%C2%A0%202.3.2%E3%80%81%E5%8F%82%E6%95%B0)

Â  Â  Â [2.4.2.1Â training args](https://blog.csdn.net/qq_16555103/article/details/#%C2%A0%20%C2%A0%20%C2%A0%20a%29%20training%20args)

Â  Â  Â [2.4.2.2 peftÂ args](https://blog.csdn.net/qq_16555103/article/details/#%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0b%29%20peft%C2%A0args)

Â  Â  Â [2.4.2.3Â model args](https://blog.csdn.net/qq_16555103/article/details/#%C2%A0%20%C2%A0%20%C2%A0%20c%29%20model%20args)

[ä¸‰ã€Trl åº“](https://blog.csdn.net/qq_16555103/article/details/#t9)

Â  [3.1ã€SFTTrainer](https://blog.csdn.net/qq_16555103/article/details/#t10)

Â  [3.2ã€å…¶ä»–çš„ä»£ç ](https://blog.csdn.net/qq_16555103/article/details/#t11)

Â  Â  [3.2.1ã€datasets.map ä½¿ç”¨ load\_from\_cache\_file = False æ–¹ä¾¿è°ƒè¯•â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹](https://blog.csdn.net/qq_16555103/article/details/#%C2%A0%20%C2%A0%20%C2%A0%20a%29%20datasets.map%20%E4%BD%BF%E7%94%A8%20load_from_cache_file%20%3D%20False%20%E6%96%B9%E4%BE%BF%E8%B0%83%E8%AF%95%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B)

[å››ã€å°ç»“](https://blog.csdn.net/qq_16555103/article/details/#t12)

Â  [4.1ã€åœ¨SFTTraineråˆå§‹åŒ–peftæ¨¡åž‹æ—¶ï¼Œä¸ºä»€ä¹ˆ å¼€å¯äº† QLoRA + FSDP / DS-Zero3 åŽä¸ä½¿ç”¨prepare\_model\_for\_kbit\_training å’Œ peft\_module\_casting\_to\_bf16 ï¼Œprepare\_model\_for\_kbit\_training å’Œ peft\_module\_casting\_to\_bf16 åšäº†ä»€ä¹ˆï¼ŸQLoRA + FSDP / DS-Zero3 æœªå¼€å¯offloadâ€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹æ¨¡åž‹åŠ è½½åŽmodelä¸ºä»€ä¹ˆåœ¨cpuä¸Šï¼Ÿ](https://blog.csdn.net/qq_16555103/article/details/#t13)

Â  [4.2ã€bfloat16å’Œfloat16çš„åŒºåˆ«](https://blog.csdn.net/qq_16555103/article/details/#t14)

Â  [4.3ã€ç»å¯¹ä½ç½®ç¼–ç ä¸Žç›¸å¯¹ä½ç½®ç¼–ç çš„åŒºåˆ«ï¼Œä¸ºä»€ä¹ˆçŽ°åœ¨çš„å¤§æ¨¡åž‹éƒ½ä½¿ç”¨RoPE](https://blog.csdn.net/qq_16555103/article/details/#t15)

[äº”ã€Trl å…¶ä»–Traineræ³¨é‡Šç¬”è®°](https://blog.csdn.net/qq_16555103/article/details/#t16)

Â  [5.1ã€DPOTrainerç¬”è®°â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹](https://blog.csdn.net/qq_16555103/article/details/#t17)

Â [5.2ã€...](https://blog.csdn.net/qq_16555103/article/details/#t18)Â 

---

> - é¡¹ç›®åœ°å€
> 
> [peft/examples/sft at main Â· huggingface/peft Â· GitHubðŸ¤— PEFT: State-of-the-art Parameter-Efficient Fine-Tuning. - peft/examples/sft at main Â· huggingface/peft![icon-default.png?t=N7T8](https://i-blog.csdnimg.cn/blog_migrate/003a2ce7eb50c2e24a8c624c260c5930.png)https://github.com/huggingface/peft/tree/main/examples/sft](https://github.com/huggingface/peft/tree/main/examples/sft "peft/examples/sft at main Â· huggingface/peft Â· GitHub")
> 
> - æ–‡æ¡£
> 
> [https://huggingface.co/docs/peft/accelerate/deepspeed![icon-default.png?t=N7T8](https://i-blog.csdnimg.cn/blog_migrate/003a2ce7eb50c2e24a8c624c260c5930.png)https://huggingface.co/docs/peft/accelerate/deepspeed](https://huggingface.co/docs/peft/accelerate/deepspeed "https://huggingface.co/docs/peft/accelerate/deepspeed")

### ä¸€ã€çŽ¯å¢ƒ

```python
ç³»ç»Ÿï¼šubuntu cudaç‰ˆæœ¬ï¼š12.1torchç‰ˆæœ¬ï¼š2.2.0pythonç‰ˆæœ¬ï¼š3.10conda è™šæ‹ŸçŽ¯å¢ƒä¸­ cudaç‰ˆæœ¬cudaï¼š12.1  
```

#### Â  1.1ã€çŽ¯å¢ƒå®‰è£…

> ```bash
> pip install -r ...
> ```

Â  Â  **ç¬¬ä¸€ç§**

**a) 2024å¹´4æœˆ28æ—¥æ›´æ–°**

```python
git+https://github.com/huggingface/accelerategit+https://github.com/huggingface/peftgit+https://github.com/huggingface/trlgit+https://github.com/huggingface/datatrove.gitunsloth[conda]@git+https://github.com/unslothai/unsloth.gitgit+https://github.com/huggingface/transformersdeepspeed==0.14.0PyGithubhuggingface-hubevaluatedatasetsbitsandbyteseinopswandbtensorboardtiktokenpandasnumpyscipymatplotlibsentencepiecenltkxformershf_transferlogurutqdmtransformers_stream_generatortorch==2.2.1openpyxlhttpxjoblibscikit_learn
```

**â€‹â€‹â€‹â€‹â€‹â€‹â€‹b)**â€‹â€‹â€‹â€‹â€‹â€‹â€‹Â **2024å¹´7æœˆ7æ—¥æ›´æ–°ï¼ˆå¢žåŠ äº†vllmï¼‰**

```python
git+https://github.com/huggingface/accelerategit+https://github.com/huggingface/peftgit+https://github.com/huggingface/datatrove.gitgit+https://github.com/huggingface/transformersunsloth[conda]@git+https://github.com/unslothai/unsloth.gittrl==0.8.6deepspeed==0.14.0torch==2.3.0vllmraynumpy==1.26.4PyGithubhuggingface-hubevaluatedatasetsbitsandbyteseinopswandbtensorboardtiktokenpandasscipymatplotlibsentencepiecenltkxformershf_transferlogurutqdmtransformers_stream_generatoropenpyxlhttpxjoblibscikit_learn
```

**2024å¹´6æœˆ19æ—¥æ›´æ–°ï¼ˆå¢žåŠ äº†vllmã€rayï¼‰**

```python
unsloth[conda]@git+https://github.com/unslothai/unsloth.gitaccelerate==0.31.0peft==0.11.1datatrove==0.2.0trl==0.8.6transformers==4.41.2deepspeed==0.14.0torch==2.3.0vllm==0.5.0.post1vllm-flash-attn==2.5.9raynumpy==1.26.4PyGithubhuggingface-hubevaluatedatasetsbitsandbyteseinopswandbtensorboardtiktokenpandasscipymatplotlibsentencepiecenltkxformershf_transferlogurutqdmtransformers_stream_generatoropenpyxlhttpxjoblibscikit_learn
```

> **è‹¥ä¸Šè¿°pipå®‰è£…åŒ…æ›´æ–°æœ€æ–°å¯¼è‡´ç‰ˆæœ¬ä¸åŒ¹é…ï¼Œå¯ä»¥å‚è€ƒä¸‹é¢ç¬¬äºŒç§æˆ–ç¬¬ä¸‰ç§åŒ…ç‰ˆæœ¬é€‚å½“ä¿®æ”¹**Â 

Â Â  Â  **ç¬¬äºŒç§**

**a) (zero3 peft lora ä¸ºbf16),** **2024å¹´4æœˆ28æ—¥æ›´æ–°**

```python
absl-py==2.1.0accelerate==0.30.0aiohttp==3.9.5aiosignal==1.3.1annotated-types==0.7.0anyio==4.3.0async-timeout==4.0.3attrs==23.2.0bitsandbytes==0.43.1certifi==2024.2.2cffi==1.16.0charset-normalizer==3.3.2click==8.1.7contourpy==1.2.1cryptography==42.0.7cycler==0.12.1datasets==2.19.1datatrove==0.2.0deepspeed==0.14.0Deprecated==1.2.14dill==0.3.8docker-pycreds==0.4.0docstring_parser==0.16einops==0.8.0et-xmlfile==1.1.0evaluate==0.4.2exceptiongroup==1.2.1filelock==3.14.0fonttools==4.51.0frozenlist==1.4.1fsspec==2024.3.1gitdb==4.0.11GitPython==3.1.43grpcio==1.64.0h11==0.14.0hf_transfer==0.1.6hjson==3.1.0httpcore==1.0.5httpx==0.27.0huggingface-hub==0.23.1humanize==4.9.0idna==3.7Jinja2==3.1.4joblib==1.4.2kiwisolver==1.4.5loguru==0.7.2Markdown==3.6markdown-it-py==3.0.0MarkupSafe==2.1.5matplotlib==3.9.0mdurl==0.1.2mpmath==1.3.0multidict==6.0.5multiprocess==0.70.16networkx==3.3ninja==1.11.1.1nltk==3.8.1numpy==1.26.4nvidia-cublas-cu12==12.1.3.1nvidia-cuda-cupti-cu12==12.1.105nvidia-cuda-nvrtc-cu12==12.1.105nvidia-cuda-runtime-cu12==12.1.105nvidia-cudnn-cu12==8.9.2.26nvidia-cufft-cu12==11.0.2.54nvidia-curand-cu12==10.3.2.106nvidia-cusolver-cu12==11.4.5.107nvidia-cusparse-cu12==12.1.0.106nvidia-nccl-cu12==2.19.3nvidia-nvjitlink-cu12==12.5.40nvidia-nvtx-cu12==12.1.105openpyxl==3.1.2packaging==24.0pandas==2.2.2peft==0.10.0pillow==10.3.0pip==24.0platformdirs==4.2.2protobuf==3.20.3psutil==5.9.8py-cpuinfo==9.0.0pyarrow==16.1.0pyarrow-hotfix==0.6pycparser==2.22pydantic==2.7.1pydantic_core==2.18.2PyGithub==2.3.0Pygments==2.18.0PyJWT==2.8.0PyNaCl==1.5.0pynvml==11.5.0pyparsing==3.1.2python-dateutil==2.9.0.post0pytz==2024.1PyYAML==6.0.1regex==2024.5.15requests==2.32.2rich==13.7.1safetensors==0.4.3scikit-learn==1.5.0scipy==1.13.1sentencepiece==0.2.0sentry-sdk==2.3.1setproctitle==1.3.3setuptools==69.5.1shtab==1.7.1six==1.16.0smmap==5.0.1sniffio==1.3.1sympy==1.12tensorboard==2.16.2tensorboard-data-server==0.7.2threadpoolctl==3.5.0tiktoken==0.7.0tokenizers==0.19.1torch==2.2.1tqdm==4.66.4transformers==4.40.1transformers-stream-generator==0.0.5triton==2.2.0trl==0.8.3typing_extensions==4.12.0tyro==0.8.4tzdata==2024.1unsloth==2024.5urllib3==2.2.1wandb==0.17.0Werkzeug==3.0.3wheel==0.43.0wrapt==1.16.0xformers==0.0.25xxhash==3.4.1yarl==1.9.4
```

**b)Â (zero3 peft lora ä¸ºfloat32)ï¼Œ****2024å¹´6æœˆ10æ—¥æ›´æ–°**

```python
Package                       Version----------------------------- -----------absl-py                       2.1.0accelerate                    0.31.0.dev0aiohttp                       3.9.5aiosignal                     1.3.1annotated-types               0.7.0anyio                         4.4.0async-timeout                 4.0.3attrs                         23.2.0bitsandbytes                  0.43.1certifi                       2024.6.2cffi                          1.16.0charset-normalizer            3.3.2click                         8.1.7contourpy                     1.2.1cryptography                  42.0.8cycler                        0.12.1datasets                      2.19.2datatrove                     0.2.0deepspeed                     0.14.0Deprecated                    1.2.14dill                          0.3.8docker-pycreds                0.4.0docstring_parser              0.16einops                        0.8.0et-xmlfile                    1.1.0evaluate                      0.4.2exceptiongroup                1.2.1filelock                      3.14.0fonttools                     4.53.0frozenlist                    1.4.1fsspec                        2024.3.1gitdb                         4.0.11GitPython                     3.1.43grpcio                        1.64.1h11                           0.14.0hf_transfer                   0.1.6hjson                         3.1.0httpcore                      1.0.5httpx                         0.27.0huggingface-hub               0.23.3humanize                      4.9.0idna                          3.7Jinja2                        3.1.4joblib                        1.4.2kiwisolver                    1.4.5loguru                        0.7.2Markdown                      3.6markdown-it-py                3.0.0MarkupSafe                    2.1.5matplotlib                    3.9.0mdurl                         0.1.2mpmath                        1.3.0multidict                     6.0.5multiprocess                  0.70.16networkx                      3.3ninja                         1.11.1.1nltk                          3.8.1numpy                         1.26.4nvidia-cublas-cu12            12.1.3.1nvidia-cuda-cupti-cu12        12.1.105nvidia-cuda-nvrtc-cu12        12.1.105nvidia-cuda-runtime-cu12      12.1.105nvidia-cudnn-cu12             8.9.2.26nvidia-cufft-cu12             11.0.2.54nvidia-curand-cu12            10.3.2.106nvidia-cusolver-cu12          11.4.5.107nvidia-cusparse-cu12          12.1.0.106nvidia-nccl-cu12              2.19.3nvidia-nvjitlink-cu12         12.5.40nvidia-nvtx-cu12              12.1.105openpyxl                      3.1.3packaging                     24.0pandas                        2.2.2peft                          0.11.2.dev0pillow                        10.3.0pip                           24.0platformdirs                  4.2.2protobuf                      3.20.3psutil                        5.9.8py-cpuinfo                    9.0.0pyarrow                       16.1.0pyarrow-hotfix                0.6pycparser                     2.22pydantic                      2.7.3pydantic_core                 2.18.4PyGithub                      2.3.0Pygments                      2.18.0PyJWT                         2.8.0PyNaCl                        1.5.0pynvml                        11.5.0pyparsing                     3.1.2python-dateutil               2.9.0.post0pytz                          2024.1PyYAML                        6.0.1regex                         2024.5.15requests                      2.32.3rich                          13.7.1safetensors                   0.4.3scikit-learn                  1.5.0scipy                         1.13.1sentencepiece                 0.2.0sentry-sdk                    2.4.0setproctitle                  1.3.3setuptools                    69.5.1shtab                         1.7.1six                           1.16.0smmap                         5.0.1sniffio                       1.3.1sympy                         1.12.1tensorboard                   2.16.2tensorboard-data-server       0.7.2threadpoolctl                 3.5.0tiktoken                      0.7.0tokenizers                    0.19.1torch                         2.2.1tqdm                          4.66.4transformers                  4.42.0.dev0transformers-stream-generator 0.0.5triton                        2.2.0trl                           0.8.6typing_extensions             4.12.1tyro                          0.8.4tzdata                        2024.1unsloth                       2024.5urllib3                       2.2.1wandb                         0.17.0Werkzeug                      3.0.3wheel                         0.43.0wrapt                         1.16.0xformers                      0.0.25xxhash                        3.4.1yarl                          1.9.4
```

#### Â  1.2ã€å®‰è£…flash atten

> **å®‰è£… flash atten å’Œ deepspeed å‰ï¼Œå°½é‡ä¿è¯ï¼š**
> 
> - ç¬¬ä¸€ ç¡®ä¿ linux "å¤–ç•Œ"çš„ cudaç‰ˆæœ¬ ä¸Ž conda è™šæ‹ŸçŽ¯å¢ƒä¸­cudaç‰ˆæœ¬ä¸€è‡´(ä¸ä¸€è‡´å¯èƒ½å¯¼è‡´ä½¿ç”¨è¿‡ç¨‹ä¸­æŠ¥"ä¸åŒ¹é…"çš„é”™è¯¯)
> - ç¬¬äºŒ å®‰è£…å¥½ c++ g++ ninja (c++ g++ Ninjia å®‰è£…ç‰ˆæœ¬è¿‡ä½ŽåŽç»­å®‰è£…å¯èƒ½ä¼šå¤±è´¥)
> - ç¬¬ä¸‰ å‚è€ƒå®˜æ–¹å‘½ä»¤: [GitHub - Dao-AILab/flash-attention: Fast and memory-efficient exact attentionFast and memory-efficient exact attention. Contribute to Dao-AILab/flash-attention development by creating an account on GitHub.![icon-default.png?t=N7T8](https://i-blog.csdnimg.cn/blog_migrate/003a2ce7eb50c2e24a8c624c260c5930.png)https://github.com/Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention "GitHub - Dao-AILab/flash-attention: Fast and memory-efficient exact attention")

```python
1. å®‰è£… c++ g++sudo apt-get updatesudo apt-get install build-essential2. å®‰è£… Ninjasudo apt-get install ninja-build         ----- æœ‰æ—¶å€™vscode debug deepspeed éœ€è¦ä¹Ÿå¯èƒ½æœ‰å…¶ä»–çš„ä¾èµ–ï¼š sudo apt-get install -y ninja-build libssl-dev libffi-dev libaio-dev3. å®‰è£…flash atten    å‚è€ƒä¸Šé¢å®˜æ–¹å‘½ä»¤ï¼š    pip install packaging  æˆ– conda install packaging    pip install flash-attn==2.6.1 --no-build-isolation          ----- flash atten ç¼–è¯‘è¿‡ç¨‹éœ€è¦ä¸€å®šçš„æ—¶é—´ï¼Œéœ€è¦ç­‰å¾…    MAX_JOBS=1 pip install flash-attn --no-build-isolation -i https://pypi.python.org/simple    å¯ä»¥é€‚æƒ…å†µé€‰æ‹©ä¸Šé¢å‘½ä»¤ä¸­çš„ä¸€ä¸ª
```

#### Â  1.3ã€vscodeè¿œç«¯å¯èƒ½é‡åˆ°çš„ä¸€äº›é—®é¢˜

- ä¸­æ–‡è·¯å¾„è¯†åˆ«é—®é¢˜

```bash
åœ¨é¡¹ç›®ç›®å½•çš„ .vscode ä¸‹åˆ›å»º settings.json æ–‡ä»¶ï¼ŒåŠ å…¥ä¸‹é¢çš„å†…å®¹ï¼š{"remote.SSH.env": {"LC_ALL": "en_US.UTF-8","LANG": "en_US.UTF-8"    },"remote.SSH.useLocalServer": false,"remote.SSH.connectTimeout": 60}
```

- ç«¯å£æ˜ å°„é—®é¢˜

```bash
é‡åˆ° Failed to set up socket for dynamic port forward on VSCodevim /etc/ssh/sshd_config ï¼Œæ‰“å¼€åŽè®¾ç½®AllowAgentForwarding yesÂ AllowTcpForwarding yesé‡å¯sshdæœåŠ¡systemctl restart sshdåˆ é™¤ç”Ÿæˆçš„vscodeæ–‡ä»¶rm -rf ~/.vscode-server/é‡æ–°sshè¿žæŽ¥https://github.com/microsoft/vscode-remote-release/issues/8132
```

### äºŒã€ä»£ç 

[peft/examples/sft at main Â· huggingface/peft Â· GitHubðŸ¤— PEFT: State-of-the-art Parameter-Efficient Fine-Tuning. - peft/examples/sft at main Â· huggingface/peft![icon-default.png?t=N7T8](https://i-blog.csdnimg.cn/blog_migrate/003a2ce7eb50c2e24a8c624c260c5930.png)https://github.com/huggingface/peft/tree/main/examples/sft](https://github.com/huggingface/peft/tree/main/examples/sft "peft/examples/sft at main Â· huggingface/peft Â· GitHub")

#### Â  2.1ã€bashè„šæœ¬Â 

```bash
PYTHONPATH=$PWDexport PYTHONPATHecho "å½“å‰bashæ‰§è¡Œç›®å½•: $PWD, å·²ç»å°†PYTHONPATHè®¾ç½®ä¸º: $PYTHONPATH"accelerate launch --config_file "examples/sft/configs/deepspeed_config_z3_qlora.yaml"  examples/sft/train.py \    --seed 100 \    --model_name_or_path "/workspace/Llama-2-7b-chat-hf" \    --dataset_name "smangrul/ultrachat-10k-chatml" \    --chat_template_format "chatml" \    --add_special_tokens False \    --append_concat_token False \    --splits "train,test" \    --max_seq_len 2048 \    --num_train_epochs 2 \    --logging_steps 5 \    --log_level "info" \    --logging_strategy "steps" \    --evaluation_strategy "epoch" \    --save_strategy "steps" \    --save_steps 100 \    --save_total_limit 10 \    --bf16 True \    --packing True \    --learning_rate 1e-4 \    --lr_scheduler_type "cosine" \    --weight_decay 1e-4 \    --warmup_ratio 0.0 \    --max_grad_norm 1.0 \    --output_dir "/workspace/output/llama-sft-qlora-dsz3" \    --per_device_train_batch_size 1 \    --per_device_eval_batch_size 2 \    --gradient_accumulation_steps 4 \    --use_flash_attn True \    --gradient_checkpointing True \    --use_reentrant True \    --dataset_text_field "content" \    --use_peft_lora True \    --lora_r 8 \    --lora_alpha 16 \    --lora_dropout 0.1 \    --lora_target_modules "all-linear" \    --use_4bit_quantization True \    --use_nested_quant True \    --bnb_4bit_compute_dtype "bfloat16" \    --bnb_4bit_quant_storage_dtype "bfloat16" \    --resume_from_checkpoint /workspace/output/llama-sft-qlora-dsz3/checkpoint-100 \    2>&1 | tee -a examples/sft/qlora_ds_zero3_log.out
```

#### Â  2.2ã€utils.py æ³¨é‡Šä¸Žä¼˜åŒ–

> ```python
> import osfrom enum import Enumimport torchfrom datasets import DatasetDict, load_dataset, load_from_diskfrom datasets.builder import DatasetGenerationErrorfrom transformers import (    AutoModelForCausalLM,    AutoTokenizer,    BitsAndBytesConfig,)from peft import LoraConfigDEFAULT_CHATML_CHAT_TEMPLATE = "{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% if loop.last and add_generation_prompt %}{{'<|im_start|>assistant\n' }}{% endif %}{% endfor %}"DEFAULT_ZEPHYR_CHAT_TEMPLATE = "{% for message in messages %}\n{% if message['role'] == 'user' %}\n{{ '<|user|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'system' %}\n{{ '<|system|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'assistant' %}\n{{ '<|assistant|>\n'  + message['content'] + eos_token }}\n{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ '<|assistant|>' }}\n{% endif %}\n{% endfor %}"class ZephyrSpecialTokens(str, Enum):    user = "<|user|>"    assistant = "<|assistant|>"    system = "<|system|>"    eos_token = "</s>"          bos_token = "<s>"           pad_token = "<pad>"         @classmethoddef list(cls):return [c.value for c in cls]class ChatmlSpecialTokens(str, Enum):    user = "<|im_start|>user"    assistant = "<|im_start|>assistant"    system = "<|im_start|>system"    eos_token = "<|im_end|>"    bos_token = "<s>"    pad_token = "<pad>"    @classmethoddef list(cls):return [c.value for c in cls]def create_datasets(tokenizer, data_args, training_args, apply_chat_template=False):def preprocess(samples):        batch = []             batch_tokens = []for conversation in samples["messages"]:            chat_tmp = tokenizer.apply_chat_template(conversation, tokenize=False)            batch.append(chat_tmp)            chat_tmp_tokens = tokenizer.tokenize(chat_tmp)            batch_tokens.append(chat_tmp_tokens)return {"content": batch, "content_tokens":batch_tokens}    raw_datasets = DatasetDict()   for split in data_args.splits.split(","):try:            dataset = load_dataset(data_args.dataset_name, split=split)except DatasetGenerationError:            dataset = load_from_disk(os.path.join(data_args.dataset_name, split))if "train" in split:            raw_datasets["train"] = datasetelif "test" in split:            raw_datasets["test"] = datasetelse:raise ValueError(f"Split type {split} not recognized as one of test or train.")if apply_chat_template:        raw_datasets = raw_datasets.map(            preprocess,            batched=True,                     remove_columns=raw_datasets["train"].column_names,            load_from_cache_file = False        )    train_data = raw_datasets["train"]      valid_data = raw_datasets["test"]   if training_args.local_rank == 0 or training_args.local_rank == -1:print(f"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}")  print(f"A sample of train dataset: {train_data[0]}")  return train_data, valid_datadef create_and_prepare_model(args, data_args, training_args):if args.use_unsloth:from unsloth import FastLanguageModel    bnb_config = None        quant_storage_dtype = None   if (        torch.distributed.is_available()and torch.distributed.is_initialized()and torch.distributed.get_world_size() > 1and args.use_unsloth    ):raise NotImplementedError("Unsloth is not supported in distributed training")if args.use_4bit_quantization:        compute_dtype = getattr(torch, args.bnb_4bit_compute_dtype)        quant_storage_dtype = getattr(torch, args.bnb_4bit_quant_storage_dtype)        bnb_config = BitsAndBytesConfig(            load_in_4bit=args.use_4bit_quantization,                      bnb_4bit_quant_type=args.bnb_4bit_quant_type,                 bnb_4bit_compute_dtype=compute_dtype,                         bnb_4bit_use_double_quant=args.use_nested_quant,              bnb_4bit_quant_storage=quant_storage_dtype,               )if compute_dtype == torch.float16 and args.use_4bit_quantization:            major, _ = torch.cuda.get_device_capability()if major >= 8:print("=" * 80)print("Your GPU supports bfloat16, you can accelerate training with the argument --bf16")print("=" * 80)elif args.use_8bit_quantization:            bnb_config = BitsAndBytesConfig(load_in_8bit=args.use_8bit_quantization)if args.use_unsloth:        model, _ = FastLanguageModel.from_pretrained(            model_name=args.model_name_or_path,            max_seq_length=data_args.max_seq_length,            dtype=None,            load_in_4bit=args.use_4bit_quantization,        )else:         torch_dtype = (            quant_storage_dtype if quant_storage_dtype and quant_storage_dtype.is_floating_point else torch.float32        )        model = AutoModelForCausalLM.from_pretrained(            args.model_name_or_path,            quantization_config=bnb_config,            trust_remote_code=True,            attn_implementation="flash_attention_2" if args.use_flash_attn else "eager",            torch_dtype=torch_dtype,        )    peft_config = None          chat_template = None    if args.use_peft_lora and not args.use_unsloth:        peft_config = LoraConfig(            lora_alpha=args.lora_alpha,                     lora_dropout=args.lora_dropout,            r=args.lora_r,            bias="none",                                   task_type="CAUSAL_LM",                         target_modules=args.lora_target_modules.split(",")if args.lora_target_modules != "all-linear"else args.lora_target_modules,        )    special_tokens = None       chat_template = None    if args.chat_template_format == "chatml":        special_tokens = ChatmlSpecialTokens                      chat_template = DEFAULT_CHATML_CHAT_TEMPLATE      elif args.chat_template_format == "zephyr":        special_tokens = ZephyrSpecialTokens                    chat_template = DEFAULT_ZEPHYR_CHAT_TEMPLATE    if special_tokens is not None:        tokenizer = AutoTokenizer.from_pretrained(            args.model_name_or_path,            pad_token=special_tokens.pad_token.value,                 bos_token=special_tokens.bos_token.value,                 eos_token=special_tokens.eos_token.value,                 additional_special_tokens=special_tokens.list(),              trust_remote_code=True,        )        tokenizer.chat_template = chat_template                   model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)else:        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, trust_remote_code=True)        tokenizer.pad_token = tokenizer.eos_token     if args.use_unsloth:        model = FastLanguageModel.get_peft_model(            model,            lora_alpha=args.lora_alpha,            lora_dropout=args.lora_dropout,            r=args.lora_r,            target_modules=args.lora_target_modules.split(",")if args.lora_target_modules != "all-linear"else args.lora_target_modules,            use_gradient_checkpointing=training_args.gradient_checkpointing,            random_state=training_args.seed,            max_seq_length=data_args.max_seq_length,        )return model, peft_config, tokenizer       
> ```

#### Â  2.3ã€train.py æ³¨é‡Šä¸Žä¼˜åŒ–

```python
import osimport sysimport torchfrom dataclasses import dataclass, fieldfrom typing import Optionalimport torch.distributedfrom transformers import HfArgumentParser, TrainingArguments, set_seed, Seq2SeqTrainingArgumentsfrom trl import SFTTrainer    from utils import create_and_prepare_model, create_datasets  os.environ["WANDB_DISABLED"] = "true" @dataclassclass ModelArguments:"""    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.    """    model_name_or_path: str = field(        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}    )    chat_template_format: Optional[str] = field(        default="none",        metadata={"help": "chatml|zephyr|none. Pass \`none\` if the dataset is already formatted with the chat template."        },    )    lora_alpha: Optional[int] = field(default=16)        lora_dropout: Optional[float] = field(default=0.1)      lora_r: Optional[int] = field(default=64)    lora_target_modules: Optional[str] = field(        default="q_proj,k_proj,v_proj,o_proj,down_proj,up_proj,gate_proj",        metadata={"help": "comma separated list of target modules to apply LoRA layers to"},    )    use_nested_quant: Optional[bool] = field(        default=False,        metadata={"help": "Activate nested quantization for 4bit base models"},    )    bnb_4bit_compute_dtype: Optional[str] = field(        default="float16",        metadata={"help": "Compute dtype for 4bit base models"},    )    bnb_4bit_quant_storage_dtype: Optional[str] = field(        default="uint8",        metadata={"help": "Quantization storage dtype for 4bit base models"},    )    bnb_4bit_quant_type: Optional[str] = field(        default="nf4",        metadata={"help": "Quantization type fp4 or nf4"},    )    use_flash_attn: Optional[bool] = field(        default=False,        metadata={"help": "Enables Flash attention for training."},    )    use_peft_lora: Optional[bool] = field(        default=False,        metadata={"help": "Enables PEFT LoRA for training."},    )    use_8bit_quantization: Optional[bool] = field(        default=False,        metadata={"help": "Enables loading model in 8bit."},    )    use_4bit_quantization: Optional[bool] = field(        default=False,        metadata={"help": "Enables loading model in 4bit."},    )    use_reentrant: Optional[bool] = field(        default=False,        metadata={"help": "Gradient Checkpointing param. Refer the related docs"},    )    use_unsloth: Optional[bool] = field(        default=False,        metadata={"help": "Enables UnSloth for training."},    )@dataclassclass DataTrainingArguments:    dataset_name: Optional[str] = field(        default="timdettmers/openassistant-guanaco",        metadata={"help": "The preference dataset to use."},    )    packing: Optional[bool] = field(        default=False,        metadata={"help": "Use packing dataset creating."},    )    dataset_text_field: str = field(default="text", metadata={"help": "Dataset field to use as input text."})    max_seq_length: Optional[int] = field(default=512)    append_concat_token: Optional[bool] = field(        default=False,        metadata={"help": "If True, appends \`eos_token_id\` at the end of each sample being packed."},    )    add_special_tokens: Optional[bool] = field(        default=False,        metadata={"help": "If True, tokenizers adds special tokens to each sample being packed."},    )    splits: Optional[str] = field(        default="train,test",        metadata={"help": "Comma separate list of the splits to use from the dataset."},    )def print_model_allarguments_name_dtype(model):for n,v in model.named_parameters():if v.requires_grad:print(f"trainable model arguments: {n} - {v.dtype} - {v.shape} - {v.device}")else:print(f"not trainable model arguments: {n} - {v.dtype} - {v.shape} - {v.device}")def main(model_args, data_args, training_args):    set_seed(training_args.seed)     model, peft_config, tokenizer = create_and_prepare_model(model_args, data_args, training_args)    model.config.use_cache = not training_args.gradient_checkpointing    training_args.gradient_checkpointing = training_args.gradient_checkpointing and not model_args.use_unslothif training_args.gradient_checkpointing:        training_args.gradient_checkpointing_kwargs = {"use_reentrant": model_args.use_reentrant}    train_dataset, eval_dataset = create_datasets(        tokenizer,        data_args,        training_args,        apply_chat_template=model_args.chat_template_format != "none",    )if (torch.distributed.is_available() and torch.distributed.is_initialized()):        torch.distributed.barrier()      trainer = SFTTrainer(        model=model,        tokenizer=tokenizer,        args=training_args,        train_dataset=train_dataset,        eval_dataset=eval_dataset,        peft_config=peft_config,        packing=data_args.packing,        dataset_kwargs={"append_concat_token": data_args.append_concat_token,"add_special_tokens": data_args.add_special_tokens,        },        dataset_text_field=data_args.dataset_text_field,        max_seq_length=data_args.max_seq_length,        )if training_args.local_rank == 0 or training_args.local_rank == -1:print("---> model layers")        print_model_allarguments_name_dtype(model = trainer.model)     print(f"---> Training/evaluation parameters:\n{training_args}")print(f"---> Model parameters:\n{model_args}")print(f"---> Datas parameters:\n{data_args}")print(f"---> model config:\n{trainer.model.config}")print(f"---> PEFT config:\n{peft_config}")    trainer.accelerator.print(f"{trainer.model}")    trainer.model.print_trainable_parameters()    checkpoint = Noneif training_args.resume_from_checkpoint is not None:        checkpoint = training_args.resume_from_checkpoint    trainer.train(resume_from_checkpoint=checkpoint)if trainer.is_fsdp_enabled:        trainer.accelerator.state.fsdp_plugin.set_state_dict_type("FULL_STATE_DICT")    trainer.save_model()if __name__ == "__main__":    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))else:        model_args, data_args, training_args = parser.parse_args_into_dataclasses()if (torch.distributed.is_available() and torch.distributed.is_initialized()):print(f"---> Torch distributed enable, Torch distributed initialized, This local rank is: {training_args.local_rank}, Word_size: {torch.torch.distributed.get_world_size()}")    main(model_args, data_args, training_args)
```

#### Â  2.4ã€æ¨¡åž‹/å‚æ•°ç›¸å…³

##### Â  Â  2.4.1ã€é‡åŒ–åŽçš„æ¨¡åž‹

###### Â  Â  Â  2.4.1.1Â é‡åŒ–åŽæ¨¡åž‹ç»“æž„

```python
PeftModelForCausalLM(  (base_model): LoraModel(    (model): LlamaForCausalLM(      (model): LlamaModel(        (embed_tokens): Embedding(32008, 4096)        (layers): ModuleList(          (0-31): 32 x LlamaDecoderLayer(            (self_attn): LlamaFlashAttention2(              (q_proj): lora.Linear4bit(                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)                (lora_dropout): ModuleDict(                  (default): Dropout(p=0.1, inplace=False)                )                (lora_A): ModuleDict(                  (default): Linear(in_features=4096, out_features=8, bias=False)                )                (lora_B): ModuleDict(                  (default): Linear(in_features=8, out_features=4096, bias=False)                )                (lora_embedding_A): ParameterDict()                (lora_embedding_B): ParameterDict()              )              (k_proj): lora.Linear4bit(                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)                (lora_dropout): ModuleDict(                  (default): Dropout(p=0.1, inplace=False)                )                (lora_A): ModuleDict(                  (default): Linear(in_features=4096, out_features=8, bias=False)                )                (lora_B): ModuleDict(                  (default): Linear(in_features=8, out_features=4096, bias=False)                )                (lora_embedding_A): ParameterDict()                (lora_embedding_B): ParameterDict()              )              (v_proj): lora.Linear4bit(                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)                (lora_dropout): ModuleDict(                  (default): Dropout(p=0.1, inplace=False)                )                (lora_A): ModuleDict(                  (default): Linear(in_features=4096, out_features=8, bias=False)                )                (lora_B): ModuleDict(                  (default): Linear(in_features=8, out_features=4096, bias=False)                )                (lora_embedding_A): ParameterDict()                (lora_embedding_B): ParameterDict()              )              (o_proj): lora.Linear4bit(                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)                (lora_dropout): ModuleDict(                  (default): Dropout(p=0.1, inplace=False)                )                (lora_A): ModuleDict(                  (default): Linear(in_features=4096, out_features=8, bias=False)                )                (lora_B): ModuleDict(                  (default): Linear(in_features=8, out_features=4096, bias=False)                )                (lora_embedding_A): ParameterDict()                (lora_embedding_B): ParameterDict()              )              (rotary_emb): LlamaRotaryEmbedding()            )            (mlp): LlamaMLP(              (gate_proj): lora.Linear4bit(                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)                (lora_dropout): ModuleDict(                  (default): Dropout(p=0.1, inplace=False)                )                (lora_A): ModuleDict(                  (default): Linear(in_features=4096, out_features=8, bias=False)                )                (lora_B): ModuleDict(                  (default): Linear(in_features=8, out_features=11008, bias=False)                )                (lora_embedding_A): ParameterDict()                (lora_embedding_B): ParameterDict()              )              (up_proj): lora.Linear4bit(                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)                (lora_dropout): ModuleDict(                  (default): Dropout(p=0.1, inplace=False)                )                (lora_A): ModuleDict(                  (default): Linear(in_features=4096, out_features=8, bias=False)                )                (lora_B): ModuleDict(                  (default): Linear(in_features=8, out_features=11008, bias=False)                )                (lora_embedding_A): ParameterDict()                (lora_embedding_B): ParameterDict()              )              (down_proj): lora.Linear4bit(                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)                (lora_dropout): ModuleDict(                  (default): Dropout(p=0.1, inplace=False)                )                (lora_A): ModuleDict(                  (default): Linear(in_features=11008, out_features=8, bias=False)                )                (lora_B): ModuleDict(                  (default): Linear(in_features=8, out_features=4096, bias=False)                )                (lora_embedding_A): ParameterDict()                (lora_embedding_B): ParameterDict()              )              (act_fn): SiLU()            )            (input_layernorm): LlamaRMSNorm()            (post_attention_layernorm): LlamaRMSNorm()          )        )        (norm): LlamaRMSNorm()      )      (lm_head): Linear(in_features=4096, out_features=32008, bias=False)    )  ))
```

###### Â  Â  Â  2.4.1.2 é‡åŒ–åŽæ¨¡åž‹layers

```python
---> model layersnot trainable model arguments: base_model.model.model.embed_tokens.weight - torch.bfloat16 - torch.Size([32008, 4096])not trainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.0.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.0.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.0.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.0.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.1.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.1.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.1.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.1.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.2.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.2.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.2.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.2.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.3.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.3.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.3.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.3.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.4.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.4.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.4.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.4.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.5.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.5.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.5.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.5.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.6.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.6.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.6.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.6.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.7.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.7.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.7.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.7.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.8.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.8.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.8.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.8.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.9.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.9.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.9.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.9.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.10.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.10.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.10.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.10.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.11.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.11.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.11.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.11.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.12.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.12.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.12.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.12.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.13.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.13.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.13.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.13.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.14.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.14.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.14.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.14.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.15.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.15.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.15.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.15.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.16.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.16.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.16.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.16.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.17.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.17.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.17.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.17.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.18.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.18.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.18.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.18.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.19.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.19.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.19.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.19.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.20.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.20.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.20.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.20.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.21.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.21.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.21.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.21.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.22.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.22.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.22.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.22.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.23.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.23.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.23.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.23.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.24.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.24.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.24.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.24.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.25.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.25.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.25.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.25.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.26.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.26.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.26.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.26.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.27.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.27.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.27.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.27.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.28.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.28.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.28.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.28.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.29.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.29.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.29.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.29.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.30.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.30.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.30.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.30.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])trainable model arguments: base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.31.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])trainable model arguments: base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])not trainable model arguments: base_model.model.model.layers.31.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])trainable model arguments: base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])trainable model arguments: base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])not trainable model arguments: base_model.model.model.layers.31.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.layers.31.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.model.norm.weight - torch.bfloat16 - torch.Size([4096])not trainable model arguments: base_model.model.lm_head.weight - torch.bfloat16 - torch.Size([32008, 4096])
```

##### Â  Â  2.4.2ã€å‚æ•°

###### Â  Â  Â 2.4.2.1Â training args

```python
Training/evaluation parameters TrainingArguments(_n_gpu=1,accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},adafactor=False,adam_beta1=0.9,adam_beta2=0.999,adam_epsilon=1e-08,auto_find_batch_size=False,bf16=True,bf16_full_eval=False,data_seed=None,dataloader_drop_last=False,dataloader_num_workers=0,dataloader_persistent_workers=False,dataloader_pin_memory=True,dataloader_prefetch_factor=None,ddp_backend=None,ddp_broadcast_buffers=None,ddp_bucket_cap_mb=None,ddp_find_unused_parameters=None,ddp_timeout=1800,debug=[],deepspeed=None,disable_tqdm=False,dispatch_batches=None,do_eval=True,do_predict=False,do_train=False,eval_accumulation_steps=None,eval_delay=0,eval_steps=None,evaluation_strategy=epoch,fp16=False,fp16_backend=auto,fp16_full_eval=False,fp16_opt_level=O1,fsdp=[],fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},fsdp_min_num_params=0,fsdp_transformer_layer_cls_to_wrap=None,full_determinism=False,gradient_accumulation_steps=4,gradient_checkpointing=True,gradient_checkpointing_kwargs={'use_reentrant': True},greater_is_better=None,group_by_length=False,half_precision_backend=auto,hub_always_push=False,hub_model_id=None,hub_private_repo=False,hub_strategy=every_save,hub_token=<HUB_TOKEN>,ignore_data_skip=False,include_inputs_for_metrics=False,include_num_input_tokens_seen=False,include_tokens_per_second=False,jit_mode_eval=False,label_names=None,label_smoothing_factor=0.0,learning_rate=0.0001,length_column_name=length,load_best_model_at_end=False,local_rank=0,log_level=info,log_level_replica=warning,log_on_each_node=True,logging_dir=/workspace/output/llama-sft-qlora-dsz3/runs/Apr12_05-25-13_afa6d91ea8f6,logging_first_step=False,logging_nan_inf_filter=True,logging_steps=5,logging_strategy=steps,lr_scheduler_kwargs={},lr_scheduler_type=cosine,max_grad_norm=1.0,max_steps=-1,metric_for_best_model=None,mp_parameters=,neftune_noise_alpha=None,no_cuda=False,num_train_epochs=2.0,optim=adamw_torch,optim_args=None,optim_target_modules=None,output_dir=/workspace/output/llama-sft-qlora-dsz3,overwrite_output_dir=False,past_index=-1,per_device_eval_batch_size=2,per_device_train_batch_size=1,prediction_loss_only=False,push_to_hub=False,push_to_hub_model_id=None,push_to_hub_organization=None,push_to_hub_token=<PUSH_TO_HUB_TOKEN>,ray_scope=last,remove_unused_columns=True,report_to=['tensorboard'],resume_from_checkpoint=/workspace/output/llama-sft-qlora-dsz3/checkpoint-100,run_name=/workspace/output/llama-sft-qlora-dsz3,save_on_each_node=False,save_only_model=False,save_safetensors=True,save_steps=100,save_strategy=steps,save_total_limit=10,seed=100,skip_memory_metrics=True,split_batches=None,tf32=None,torch_compile=False,torch_compile_backend=None,torch_compile_mode=None,torchdynamo=None,tpu_metrics_debug=False,tpu_num_cores=None,use_cpu=False,use_ipex=False,use_legacy_prediction_loop=False,use_mps_device=False,warmup_ratio=0.0,warmup_steps=0,weight_decay=0.0001,)
```

###### Â  Â  Â 2.4.2.2 peftÂ args

```python
PEFT parameters LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/workspace/Llama-2-7b-chat-hf', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'q_proj', 'down_proj', 'k_proj', 'v_proj', 'up_proj', 'o_proj', 'gate_proj'}, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
```

###### Â  Â  Â 2.4.2.3Â model args

```python
model parameters LlamaConfig {"_name_or_path": "/workspace/Llama-2-7b-chat-hf","architectures": ["LlamaForCausalLM"  ],"attention_bias": false,"attention_dropout": 0.0,"bos_token_id": 1,"eos_token_id": 2,"hidden_act": "silu","hidden_size": 4096,"initializer_range": 0.02,"intermediate_size": 11008,"max_position_embeddings": 4096,"model_type": "llama","num_attention_heads": 32,"num_hidden_layers": 32,"num_key_value_heads": 32,"pretraining_tp": 1,"quantization_config": {"_load_in_4bit": true,"_load_in_8bit": false,"bnb_4bit_compute_dtype": "bfloat16","bnb_4bit_quant_storage": "bfloat16","bnb_4bit_quant_type": "nf4","bnb_4bit_use_double_quant": true,"llm_int8_enable_fp32_cpu_offload": false,"llm_int8_has_fp16_weight": false,"llm_int8_skip_modules": null,"llm_int8_threshold": 6.0,"load_in_4bit": true,"load_in_8bit": false,"quant_method": "bitsandbytes"  },"rms_norm_eps": 1e-05,"rope_scaling": null,"rope_theta": 10000.0,"tie_word_embeddings": false,"torch_dtype": "bfloat16","transformers_version": "4.40.0.dev0","use_cache": false,"vocab_size": 32008}
```

### ä¸‰ã€Trl åº“

#### Â  3.1ã€SFTTrainer

> ä¸Šè¿°ä»£ç ä¸»è¦åŒ…å«äº†ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†:
> 
> 1. **éžæ‰“åŒ…(Non-packed)æ•°æ®é›†çš„å‡†å¤‡**:
> 
> å¦‚æžœ`packing=False`,åˆ™ä½¿ç”¨`_prepare_non_packed_dataloader`æ–¹æ³•å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ã€‚è¯¥æ–¹æ³•é¦–å…ˆå®šä¹‰äº†ä¸€ä¸ª`tokenize`å‡½æ•°,ç”¨äºŽå¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œtokenizeæ“ä½œ,åŒ…æ‹¬æ·»åŠ ç‰¹æ®Šæ ‡è®°ã€æˆªæ–­å’Œpaddingç­‰ã€‚ç„¶åŽä½¿ç”¨`dataset.map`æ–¹æ³•å¯¹æ•´ä¸ªæ•°æ®é›†åº”ç”¨`tokenize`å‡½æ•°,å¾—åˆ°tokenizedçš„æ•°æ®é›†ã€‚æœ€åŽ,ä½¿ç”¨PyTorchçš„`DataLoader`åˆ›å»ºæ•°æ®åŠ è½½å™¨,æ–¹ä¾¿åœ¨è®­ç»ƒæ—¶å¯¹æ•°æ®è¿›è¡Œæ‰¹å¤„ç†ã€‚
> 
> 1. **æ‰“åŒ…(Packed)æ•°æ®é›†çš„å‡†å¤‡**:
> 
> å¦‚æžœ`packing=True`,åˆ™ä½¿ç”¨`_prepare_packed_dataloader`æ–¹æ³•å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ã€‚è¯¥æ–¹æ³•ä½¿ç”¨`ConstantLengthDataset`ç±»å¯¹æ•°æ®é›†è¿›è¡Œæ‰“åŒ…ã€‚`ConstantLengthDataset`æ˜¯TRLåº“ä¸­çš„ä¸€ä¸ªè‡ªå®šä¹‰æ•°æ®é›†ç±»,å®ƒå¯ä»¥å°†ä¸åŒé•¿åº¦çš„æ–‡æœ¬åºåˆ—æ‰“åŒ…åˆ°åŒä¸€ä¸ªå¼ é‡ä¸­,ä»Žè€Œæé«˜å†…å­˜åˆ©ç”¨çŽ‡å’Œè®­ç»ƒé€Ÿåº¦ã€‚åœ¨åˆ›å»º`ConstantLengthDataset`æ—¶,éœ€è¦æŒ‡å®šæ–‡æœ¬å­—æ®µåç§°ã€æœ€å¤§åºåˆ—é•¿åº¦ã€æ¯ä¸ªtokençš„å­—ç¬¦æ•°ä¼°è®¡å€¼ç­‰å‚æ•°ã€‚ç„¶åŽ,ä½¿ç”¨`ConstantLengthBatchSampler`è¿›è¡Œæ‰¹é‡‡æ ·,å¹¶åˆ›å»º`DataLoader`å¯¹è±¡ã€‚
> 
> 1. **NEFTuneå™ªå£°åµŒå…¥çš„æ¿€æ´»å’Œç§»é™¤**:
> 
> `_trl_activate_neftune`æ–¹æ³•ç”¨äºŽæ¿€æ´»NEFTuneå™ªå£°åµŒå…¥ã€‚NEFTuneæ˜¯ä¸€ç§åœ¨è¾“å…¥åµŒå…¥ä¸­æ³¨å…¥å™ªå£°çš„æŠ€æœ¯,å¯ä»¥æé«˜æ¨¡åž‹åœ¨æŒ‡ä»¤å¾®è°ƒä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•èŽ·å–æ¨¡åž‹çš„è¾“å…¥åµŒå…¥å±‚,ç„¶åŽæ³¨å†Œä¸€ä¸ªé’©å­å‡½æ•°,åœ¨å‰å‘ä¼ æ’­æ—¶å°†é«˜æ–¯å™ªå£°åŠ å…¥åˆ°è¾“å…¥åµŒå…¥ä¸­ã€‚`_trl_unwrap_neftune`æ–¹æ³•åˆ™ç”¨äºŽç§»é™¤NEFTuneå™ªå£°åµŒå…¥,æ¢å¤æ¨¡åž‹åŽŸå§‹çš„å‰å‘ä¼ æ’­é€»è¾‘ã€‚
> 
> éœ€è¦æ³¨æ„çš„æ˜¯,NEFTuneå™ªå£°åµŒå…¥åªåœ¨æ¨¡åž‹è®­ç»ƒæ—¶ç”Ÿæ•ˆ,åœ¨æŽ¨ç†æ—¶ä¸ä¼šæ·»åŠ å™ªå£°ã€‚æ­¤å¤–,è¯¥æŠ€æœ¯ä¸»è¦ç”¨äºŽæŒ‡ä»¤å¾®è°ƒä»»åŠ¡,å¯¹äºŽå…¶ä»–ä»»åŠ¡çš„æ•ˆæžœå°šæ— å®šè®ºã€‚
> 
> æ€»çš„æ¥è¯´,ä¸Šè¿°ä»£ç å®žçŽ°äº†æ•°æ®é›†çš„é¢„å¤„ç†ã€åŠ è½½,ä»¥åŠNEFTuneå™ªå£°åµŒå…¥çš„æ¿€æ´»å’Œç§»é™¤åŠŸèƒ½,ä¸ºç›‘ç£å¾®è°ƒæä¾›äº†å¿…è¦çš„æ”¯æŒã€‚è¯¦ç»†çš„æ³¨é‡Šæœ‰åŠ©äºŽç†è§£æ¯ä¸ªéƒ¨åˆ†çš„ä½œç”¨å’Œç›¸å…³æŠ€æœ¯ç»†èŠ‚ã€‚

```python
class SFTTrainer(Trainer):r"""    ç›‘ç£å¾®è°ƒè®­ç»ƒå™¨(SFT Trainer)çš„ç±»å®šä¹‰ã€‚    è¿™ä¸ªç±»æ˜¯å¯¹transformers.Trainerç±»çš„åŒ…è£…,ç»§æ‰¿äº†å…¶æ‰€æœ‰çš„å±žæ€§å’Œæ–¹æ³•ã€‚    å½“ç”¨æˆ·ä¼ å…¥PeftConfigå¯¹è±¡æ—¶,è¯¥è®­ç»ƒå™¨ä¼šè´Ÿè´£æ­£ç¡®åœ°åˆå§‹åŒ–PeftModelã€‚    å‚æ•°:        model (Union[\`transformers.PreTrainedModel\`, \`nn.Module\`, \`str\`]):            è¦è®­ç»ƒçš„æ¨¡åž‹,å¯ä»¥æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„transformersæ¨¡åž‹(PreTrainedModel)ã€ä¸€ä¸ªè‡ªå®šä¹‰çš„PyTorchæ¨¡å—(nn.Module)æˆ–ä¸€ä¸ªå­—ç¬¦ä¸²(è¡¨ç¤ºè¦ä»ŽHugging Faceç¼“å­˜æˆ–åœ¨çº¿ä¸‹è½½çš„é¢„è®­ç»ƒæ¨¡åž‹åç§°)ã€‚            å¦‚æžœä¼ å…¥äº†PeftConfigå¯¹è±¡,è¯¥æ¨¡åž‹ä¹Ÿå¯ä»¥è½¬æ¢ä¸ºPeftModel(ä¸€ç§ç”¨äºŽé«˜æ•ˆå¾®è°ƒçš„æ¨¡åž‹ç»“æž„)ã€‚        args (Optional[\`transformers.TrainingArguments\`]):            å¾®è°ƒè®­ç»ƒçš„å‚æ•°é…ç½®,åŒ…æ‹¬è¯¸å¦‚å­¦ä¹ çŽ‡ã€æ‰¹æ¬¡å¤§å°ã€è®­ç»ƒepochesç­‰è¶…å‚æ•°è®¾ç½®ã€‚è¯·å‚è€ƒtransformers.TrainingArgumentsçš„å®˜æ–¹æ–‡æ¡£ä»¥äº†è§£æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚        data_collator (Optional[\`transformers.DataCollator\`]):            ç”¨äºŽè®­ç»ƒçš„æ•°æ®æ”¶é›†å™¨(DataCollator)ã€‚DataCollatorè´Ÿè´£å¯¹æ ·æœ¬è¿›è¡Œpaddingã€batchingç­‰æ“ä½œ,ä»¥ä¾¿äºŽè¾“å…¥æ¨¡åž‹è¿›è¡Œè®­ç»ƒã€‚å¦‚æžœæœªæŒ‡å®š,å°†ä½¿ç”¨é»˜è®¤çš„DataCollatorã€‚        train_dataset (Optional[\`datasets.Dataset\`]):            ç”¨äºŽè®­ç»ƒçš„æ•°æ®é›†,å¯ä»¥æ˜¯ä¸€ä¸ªHugging Face datasetsæˆ–è€…PyTorch Datasetã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨trl.trainer.ConstantLengthDatasetåˆ›å»ºæ•°æ®é›†,è¿™ç§æ ¼å¼å¯¹äºŽåºåˆ—é•¿åº¦å¯å˜çš„è¯­æ–™æ¥è¯´æ›´åŠ é«˜æ•ˆã€‚        eval_dataset (Optional[Union[\`datasets.Dataset\`, Dict[\`str\`, \`datasets.Dataset\`]]]):            ç”¨äºŽè¯„ä¼°çš„æ•°æ®é›†,å¯ä»¥æ˜¯ä¸€ä¸ªå•ç‹¬çš„datasets.Dataset,ä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªå°†æ•°æ®é›†åç§°æ˜ å°„åˆ°å¯¹åº”æ•°æ®é›†å¯¹è±¡çš„å­—å…¸ã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨trl.trainer.ConstantLengthDatasetåˆ›å»ºæ•°æ®é›†ã€‚        tokenizer (Optional[\`transformers.PreTrainedTokenizer\`]):            ç”¨äºŽè®­ç»ƒçš„åˆ†è¯å™¨(tokenizer),å¦‚æžœæœªæŒ‡å®š,å°†ä½¿ç”¨ä¸Žæ¨¡åž‹å…³è”çš„é»˜è®¤åˆ†è¯å™¨ã€‚åˆ†è¯å™¨è´Ÿè´£å°†åŽŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡åž‹å¯ä»¥ç†è§£çš„token IDåºåˆ—ã€‚        model_init (\`Callable[[], transformers.PreTrainedModel]\`):            ç”¨äºŽè®­ç»ƒçš„æ¨¡åž‹åˆå§‹åŒ–å‡½æ•°,å¦‚æžœæœªæŒ‡å®š,å°†ä½¿ç”¨é»˜è®¤çš„æ¨¡åž‹åˆå§‹åŒ–å‡½æ•°ã€‚è¯¥å‡½æ•°åº”è¯¥è¿”å›žä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡åž‹å®žä¾‹ã€‚        compute_metrics (\`Callable[[transformers.EvalPrediction], Dict]\`, *optional* defaults to None):            ç”¨äºŽè®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°,å®ƒæŽ¥æ”¶ä¸€ä¸ªtransformers.EvalPredictionå¯¹è±¡ä½œä¸ºè¾“å…¥,å¹¶è¿”å›žä¸€ä¸ªå°†æŒ‡æ ‡åç§°æ˜ å°„åˆ°æŒ‡æ ‡å€¼çš„å­—å…¸ã€‚å¦‚æžœæœªæŒ‡å®š,è¯„ä¼°è¿‡ç¨‹ä¸­åªä¼šè®¡ç®—æŸå¤±(loss)ã€‚        callbacks (\`List[transformers.TrainerCallback]\`):            ç”¨äºŽè®­ç»ƒçš„å›žè°ƒå‡½æ•°åˆ—è¡¨ã€‚å›žè°ƒå‡½æ•°å¯ä»¥åœ¨è®­ç»ƒçš„ä¸åŒé˜¶æ®µæ‰§è¡Œè‡ªå®šä¹‰æ“ä½œ,å¦‚è®°å½•æ—¥å¿—ã€ä¿å­˜æ¨¡åž‹æ£€æŸ¥ç‚¹ç­‰ã€‚        optimizers (\`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]\`):            ç”¨äºŽè®­ç»ƒçš„ä¼˜åŒ–å™¨(Optimizer)å’Œå­¦ä¹ çŽ‡è°ƒåº¦å™¨(LRScheduler)å¯¹è±¡ã€‚å¦‚æžœæœªæŒ‡å®š,å°†ä½¿ç”¨é»˜è®¤çš„ä¼˜åŒ–å™¨å’Œå­¦ä¹ çŽ‡è°ƒåº¦å™¨ã€‚        preprocess_logits_for_metrics (\`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\`):            ç”¨äºŽåœ¨è®¡ç®—æŒ‡æ ‡ä¹‹å‰é¢„å¤„ç†æ¨¡åž‹è¾“å‡º(logits)çš„å‡½æ•°ã€‚è¯¥å‡½æ•°æŽ¥æ”¶æ¨¡åž‹çš„åŽŸå§‹è¾“å‡º(logits)å’Œæ ‡ç­¾(labels)ä½œä¸ºè¾“å…¥,å¹¶è¿”å›žé¢„å¤„ç†åŽçš„logitsã€‚        peft_config (\`Optional[PeftConfig]\`):            ç”¨äºŽåˆå§‹åŒ–PeftModelçš„PeftConfigå¯¹è±¡ã€‚PeftModelæ˜¯ä¸€ç§é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•,å¯ä»¥æ˜¾è‘—å‡å°‘éœ€è¦æ›´æ–°çš„å‚æ•°æ•°é‡,ä»Žè€ŒåŠ å¿«å¾®è°ƒé€Ÿåº¦å¹¶èŠ‚çœå†…å­˜ã€‚        dataset_text_field (\`Optional[str]\`):            æ•°æ®é›†ä¸­æ–‡æœ¬å­—æ®µçš„åç§°,å¦‚æžœä¼ å…¥,è®­ç»ƒå™¨å°†è‡ªåŠ¨åŸºäºŽè¯¥å­—æ®µåˆ›å»ºConstantLengthDatasetã€‚ConstantLengthDatasetæ˜¯ä¸€ç§é«˜æ•ˆçš„æ•°æ®æ ¼å¼,é€‚ç”¨äºŽåºåˆ—é•¿åº¦å¯å˜çš„è¯­æ–™ã€‚        formatting_func (\`Optional[Callable]\`):            ç”¨äºŽåˆ›å»ºConstantLengthDatasetçš„æ ¼å¼åŒ–å‡½æ•°ã€‚è¯¥å‡½æ•°æŽ¥æ”¶ä¸€ä¸ªæ ·æœ¬ä½œä¸ºè¾“å…¥,å¹¶è¿”å›žä¸€ä¸ªç»è¿‡é¢„å¤„ç†çš„å­—ç¬¦ä¸²åˆ—è¡¨,ç”¨äºŽæž„å»ºè¾“å…¥åºåˆ—ã€‚å¦‚æžœæœªæŒ‡å®š,å°†ä½¿ç”¨é»˜è®¤çš„æ ¼å¼åŒ–å‡½æ•°ã€‚        max_seq_length (\`Optional[int]\`):            ç”¨äºŽConstantLengthDatasetå’Œè‡ªåŠ¨åˆ›å»ºæ•°æ®é›†çš„æœ€å¤§åºåˆ—é•¿åº¦,é»˜è®¤ä¸º512ã€‚è¶…è¿‡è¯¥é•¿åº¦çš„åºåˆ—å°†è¢«æˆªæ–­ã€‚        infinite (\`Optional[bool]\`):            æ˜¯å¦ä½¿ç”¨æ— é™æ•°æ®é›†,é»˜è®¤ä¸ºFalseã€‚å¦‚æžœè®¾ç½®ä¸ºTrue,è®­ç»ƒå°†åœ¨è¾¾åˆ°max_stepsæˆ–max_epochsæ—¶åœæ­¢,è€Œä¸ä¼šå› ä¸ºæ•°æ®é›†è¢«éåŽ†å®Œè€Œåœæ­¢ã€‚(æ­¤å‚æ•°å·²å¼ƒç”¨,å»ºè®®ä½¿ç”¨TrainingArgumentsä¸­çš„max_stepsæˆ–num_train_epochså‚æ•°æ¥æŽ§åˆ¶è®­ç»ƒé•¿åº¦)        num_of_sequences (\`Optional[int]\`):            ConstantLengthDatasetä½¿ç”¨çš„åºåˆ—æ•°é‡,é»˜è®¤ä¸º1024ã€‚è¯¥å‚æ•°æŽ§åˆ¶äº†ConstantLengthDatasetåœ¨å†…å­˜ä¸­ç¼“å­˜çš„åºåˆ—æ•°é‡ã€‚        chars_per_token (\`Optional[float]\`):            ConstantLengthDatasetä½¿ç”¨çš„æ¯ä¸ªtokençš„å­—ç¬¦æ•°,é»˜è®¤ä¸º3.6ã€‚è¯¥å‚æ•°ç”¨äºŽä¼°è®¡è¾“å…¥åºåˆ—çš„é•¿åº¦,ä»¥ä¾¿å¯¹åºåˆ—è¿›è¡Œæˆªæ–­å’Œpaddingæ“ä½œã€‚æ‚¨å¯ä»¥åœ¨stack-llamaç¤ºä¾‹ä¸­æŸ¥çœ‹å¦‚ä½•è®¡ç®—è¯¥å€¼ã€‚        packing (\`Optional[bool]\`):            ä»…åœ¨ä¼ å…¥dataset_text_fieldæ—¶ä½¿ç”¨ã€‚å¦‚æžœè®¾ç½®ä¸ºTrue,åˆ™ä½¿ç”¨ConstantLengthDatasetå¯¹æ•°æ®è¿›è¡Œæ‰“åŒ…,è¿™ç§æ ¼å¼æ›´åŠ é«˜æ•ˆ,å°¤å…¶æ˜¯åœ¨å¤„ç†é•¿åºåˆ—æ—¶ã€‚å¦‚æžœè®¾ç½®ä¸ºFalse,åˆ™ä½¿ç”¨é»˜è®¤çš„DataCollatorForLanguageModelingå¯¹æ•°æ®è¿›è¡Œå¤„ç†ã€‚        dataset_num_proc (\`Optional[int]\`):            ç”¨äºŽæ ‡è®°æ•°æ®çš„å·¥ä½œè¿›ç¨‹æ•°,ä»…åœ¨packing=Falseæ—¶ä½¿ç”¨,é»˜è®¤ä¸ºNone,å³ä½¿ç”¨ä¸»è¿›ç¨‹è¿›è¡Œæ ‡è®°ã€‚å¢žåŠ å·¥ä½œè¿›ç¨‹æ•°é‡å¯ä»¥åŠ é€Ÿæ•°æ®é¢„å¤„ç†çš„é€Ÿåº¦ã€‚        dataset_batch_size (\`int\`):            æ¯æ‰¹æ ‡è®°çš„ç¤ºä¾‹æ•°é‡,å¦‚æžœbatch_size <= 0æˆ–batch_size == None,åˆ™å°†æ•´ä¸ªæ•°æ®é›†æ ‡è®°ä¸ºå•ä¸ªæ‰¹æ¬¡,é»˜è®¤ä¸º1000ã€‚è¯¥å‚æ•°æŽ§åˆ¶äº†æ•°æ®é¢„å¤„ç†çš„å†…å­˜å ç”¨å’Œé€Ÿåº¦,éœ€è¦æ ¹æ®å®žé™…æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚        neftune_noise_alpha (\`Optional[float]\`):            å¦‚æžœä¸ä¸ºNone,è¿™å°†æ¿€æ´»NEFTuneå™ªå£°åµŒå…¥ã€‚NEFTuneæ˜¯ä¸€ç§å™ªå£°æ³¨å…¥æŠ€æœ¯,å®ƒé€šè¿‡åœ¨è¾“å…¥åµŒå…¥ä¸­æ·»åŠ å™ªå£°,å¯ä»¥æé«˜æ¨¡åž‹åœ¨æŒ‡ä»¤å¾®è°ƒä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚å…·ä½“ç»†èŠ‚è¯·å‚è€ƒåŽŸè®ºæ–‡å’Œä»£ç ã€‚        model_init_kwargs: (\`Optional[Dict]\`, *optional*):            å®žä¾‹åŒ–æ¨¡åž‹(ä»Žå­—ç¬¦ä¸²)æ—¶ä¼ é€’çš„å¯é€‰å…³é”®å­—å‚æ•°,å¦‚æŒ‡å®šæ¨¡åž‹æƒé‡æ–‡ä»¶çš„æœ¬åœ°è·¯å¾„ç­‰ã€‚        dataset_kwargs: (\`Optional[Dict]\`, *optional*):            åˆ›å»ºæ‰“åŒ…æˆ–éžæ‰“åŒ…æ•°æ®é›†æ—¶ä¼ é€’çš„å¯é€‰å…³é”®å­—å‚æ•°,ç”¨äºŽå¯¹æ•°æ®é›†çš„æž„å»ºè¡Œä¸ºè¿›è¡Œæ›´å¤šæŽ§åˆ¶ã€‚        eval_packing: (\`Optional[bool]\`, *optional*):            æ˜¯å¦ä¹Ÿå¯¹è¯„ä¼°æ•°æ®é›†è¿›è¡Œæ‰“åŒ…,å¦‚æžœä¸ºNone,åˆ™é»˜è®¤ä¸ºpackingå‚æ•°çš„å€¼ã€‚å³å¦‚æžœè®­ç»ƒæ•°æ®é›†ä½¿ç”¨äº†æ‰“åŒ…,è¯„ä¼°æ•°æ®é›†ä¹Ÿä¼šä½¿ç”¨æ‰“åŒ…,åä¹‹äº¦ç„¶ã€‚    """    _tag_names = ["trl", "sft"]  def __init__(        self,        model: Optional[Union[PreTrainedModel, nn.Module, str]] = None,        args: Optional[TrainingArguments] = None,        data_collator: Optional[DataCollator] = None,          train_dataset: Optional[Dataset] = None,        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,        tokenizer: Optional[PreTrainedTokenizerBase] = None,        model_init: Optional[Callable[[], PreTrainedModel]] = None,        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,        callbacks: Optional[List[TrainerCallback]] = None,        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),        preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,        peft_config: Optional["PeftConfig"] = None,        dataset_text_field: Optional[str] = None,        packing: Optional[bool] = False,        formatting_func: Optional[Callable] = None,        max_seq_length: Optional[int] = None,        infinite: Optional[bool] = None,        num_of_sequences: Optional[int] = 1024,        chars_per_token: Optional[float] = 3.6,        dataset_num_proc: Optional[int] = None,        dataset_batch_size: int = 1000,        neftune_noise_alpha: Optional[float] = None,        model_init_kwargs: Optional[Dict] = None,        dataset_kwargs: Optional[Dict] = None,        eval_packing: Optional[bool] = None,    ):if model_init_kwargs is None:            model_init_kwargs = {}elif not isinstance(model, str):raise ValueError("You passed model_kwargs to the SFTTrainer. But your model is already instantiated.")if infinite is not None:            warnings.warn("The \`infinite\` argument is deprecated and will be removed in a future version of TRL. Use \`TrainingArguments.max_steps\` or \`TrainingArguments.num_train_epochs\` instead to control training length."            )if isinstance(model, str):            warnings.warn("You passed a model_id to the SFTTrainer. This will automatically create an ""\`AutoModelForCausalLM\` or a \`PeftModel\` (if you passed a \`peft_config\`) for you."            )            model = AutoModelForCausalLM.from_pretrained(model, **model_init_kwargs)if packing and data_collator is not None and isinstance(data_collator, DataCollatorForCompletionOnlyLM):raise ValueError("You passed a \`DataCollatorForCompletionOnlyLM\` to the SFTTrainer. This is not compatible with the \`packing\` argument."            )if is_peft_available() and peft_config is not None:if not isinstance(peft_config, PeftConfig):raise ValueError("If you want to use the PeftModel, you need to pass a PeftConfig object to the SFTTrainer."f" and you passed a {type(peft_config)}."                )if not isinstance(model, PeftModel):                _support_gc_kwargs = hasattr(                    args, "gradient_checkpointing_kwargs"                ) and "gradient_checkpointing_kwargs" in list(                    inspect.signature(prepare_model_for_kbit_training).parameters                )                gradient_checkpointing_kwargs = getattr(args, "gradient_checkpointing_kwargs", None) or {}                is_sharded_qlora = Falseif getattr(model, "is_loaded_in_4bit", False):for _, param in model.named_parameters():if param.__class__.__name__ == "Params4bit":                            is_sharded_qlora = param.data.device.type == "cpu"breakif getattr(model, "is_loaded_in_8bit", False) or (getattr(model, "is_loaded_in_4bit", False) and not is_sharded_qlora                ):                    prepare_model_kwargs = {"use_gradient_checkpointing": getattr(args, "gradient_checkpointing", False)                    }if _support_gc_kwargs:                        prepare_model_kwargs["gradient_checkpointing_kwargs"] = gradient_checkpointing_kwargs                    model = prepare_model_for_kbit_training(model, **prepare_model_kwargs)if args is not None:                        args = dataclasses.replace(args, gradient_checkpointing=False)elif getattr(args, "gradient_checkpointing", False) and ("use_reentrant" not in gradient_checkpointing_kwargsor gradient_checkpointing_kwargs["use_reentrant"]                ):if hasattr(model, "enable_input_require_grads"):                        model.enable_input_require_grads()else:def make_inputs_require_grad(module, input, output):                            output.requires_grad_(True)                        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)                model = get_peft_model(model, peft_config)if (                    args is not Noneand args.bf16and getattr(model, "is_loaded_in_4bit", False)and not is_sharded_qlora                ):                    peft_module_casting_to_bf16(model)if tokenizer is None:            tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)if getattr(tokenizer, "pad_token", None) is None:                tokenizer.pad_token = tokenizer.eos_tokenif max_seq_length is None:            max_seq_length = min(tokenizer.model_max_length, 1024)            warnings.warn(f"You didn't pass a \`max_seq_length\` argument to the SFTTrainer, this will default to {max_seq_length}"            )        self.dataset_num_proc = dataset_num_proc        self.dataset_batch_size = dataset_batch_size        self._trainer_supports_neftune = hasattr(args, "neftune_noise_alpha")if neftune_noise_alpha is not None and self._trainer_supports_neftune:            args.neftune_noise_alpha = neftune_noise_alpha            warnings.warn("You passed a \`neftune_noise_alpha\` argument to the SFTTrainer, the value you passed will override the one in the \`TrainingArguments\`."            )elif not self._trainer_supports_neftune:            self.neftune_noise_alpha = neftune_noise_alphaif formatting_func is None and dataset_text_field is None:            formatting_func = get_formatting_func_from_dataset(train_dataset, tokenizer)if not packing:if dataset_text_field is None and formatting_func is None:raise ValueError("You passed \`packing=False\` to the SFTTrainer, but you didn't pass a \`dataset_text_field\` or \`formatting_func\` argument."                )if data_collator is None:                data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)with PartialState().local_main_process_first():if dataset_kwargs is None:                dataset_kwargs = {}if train_dataset is not None:                train_dataset = self._prepare_dataset(                    train_dataset,                    tokenizer,                    packing,                    dataset_text_field,                    max_seq_length,                    formatting_func,                    num_of_sequences,                    chars_per_token,                    remove_unused_columns=args.remove_unused_columns if args is not None else True,                    **dataset_kwargs,                )if eval_dataset is not None:                _multiple = isinstance(eval_dataset, dict)                _eval_datasets = eval_dataset if _multiple else {"singleton": eval_dataset}                eval_packing = packing if eval_packing is None else eval_packingfor _eval_dataset_name, _eval_dataset in _eval_datasets.items():                    _eval_datasets[_eval_dataset_name] = self._prepare_dataset(                        _eval_dataset,                        tokenizer,                        eval_packing,                        dataset_text_field,                        max_seq_length,                        formatting_func,                        num_of_sequences,                        chars_per_token,                        remove_unused_columns=args.remove_unused_columns if args is not None else True,                        **dataset_kwargs,                    )if not _multiple:                    eval_dataset = _eval_datasets["singleton"]if tokenizer.padding_side is not None and tokenizer.padding_side != "right":            warnings.warn("You passed a tokenizer with \`padding_side\` not equal to \`right\` to the SFTTrainer. This might lead to some unexpected behaviour due to ""overflow issues when training a model in half-precision. You might consider adding \`tokenizer.padding_side = 'right'\` to your code."            )super().__init__(            model=model,            args=args,            data_collator=data_collator,            train_dataset=train_dataset,            eval_dataset=eval_dataset,            tokenizer=tokenizer,            model_init=model_init,            compute_metrics=compute_metrics,            callbacks=callbacks,            optimizers=optimizers,            preprocess_logits_for_metrics=preprocess_logits_for_metrics,        )if hasattr(self.model, "add_model_tags"):            self.model.add_model_tags(self._tag_names)if self.args.max_steps > 0 and packing:            warnings.warn("You passed \`packing=True\` to the SFTTrainer, and you are training your model with \`max_steps\` strategy. The dataset will be iterated until the \`max_steps\` are reached."            )            self.train_dataset.infinite = Trueelif self.args.max_steps == -1 and packing:            self.train_dataset.infinite = Falseif any(isinstance(callback, RichProgressCallback) for callback in self.callback_handler.callbacks):for callback in self.callback_handler.callbacks:if callback.__class__.__name__ == "PrinterCallback":                    self.callback_handler.pop_callback(callback)    @wraps(Trainer.train)def train(self, *args, **kwargs):if self.neftune_noise_alpha is not None and not self._trainer_supports_neftune:            self.model = self._trl_activate_neftune(self.model)        output = super().train(*args, **kwargs)if self.neftune_noise_alpha is not None and not self._trainer_supports_neftune:            unwrapped_model = unwrap_model(self.model)if is_peft_available() and isinstance(unwrapped_model, PeftModel):                embeddings = unwrapped_model.base_model.model.get_input_embeddings()else:                embeddings = unwrapped_model.get_input_embeddings()            self.neftune_hook_handle.remove()del embeddings.neftune_noise_alphareturn output    @wraps(Trainer.push_to_hub)def push_to_hub(self, commit_message: Optional[str] = "End of training", blocking: bool = True, **kwargs) -> str:"""        è¦†å†™push_to_hubæ–¹æ³•,åœ¨æŽ¨é€æ¨¡åž‹åˆ°Hubæ—¶å¼ºåˆ¶æ·»åŠ "sft"æ ‡ç­¾ã€‚        æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒtransformers.Trainer.push_to_hubã€‚        """        kwargs = trl_sanitze_kwargs_for_tagging(model=self.model, tag_names=self._tag_names, kwargs=kwargs)return super().push_to_hub(commit_message=commit_message, blocking=blocking, **kwargs)def _prepare_dataset(        self,        dataset,        tokenizer,        packing,        dataset_text_field,        max_seq_length,        formatting_func,        num_of_sequences,        chars_per_token,        remove_unused_columns=True,        append_concat_token=True,        add_special_tokens=True,        skip_prepare_dataset=False,    ):if dataset is None:raise ValueError("The dataset should not be None")if skip_prepare_dataset:return dataset        column_names = (            dataset.column_names if isinstance(dataset, (datasets.Dataset, datasets.IterableDataset)) else None        )if column_names and "input_ids" in column_names:return datasetif isinstance(            dataset, (torch.utils.data.IterableDataset, torch.utils.data.Dataset, ConstantLengthDataset)        ) and not isinstance(dataset, datasets.IterableDataset):return datasetif not packing:return self._prepare_non_packed_dataloader(                tokenizer,                dataset,                dataset_text_field,                max_seq_length,                formatting_func,                add_special_tokens,                remove_unused_columns,            )else:return self._prepare_packed_dataloader(                tokenizer,                dataset,                dataset_text_field,                max_seq_length,                num_of_sequences,                chars_per_token,                formatting_func,                append_concat_token,                add_special_tokens,            )def _prepare_non_packed_dataloader(        self,        tokenizer,        dataset,        dataset_text_field,        max_seq_length,        formatting_func=None,        add_special_tokens=True,        remove_unused_columns=True,    ):        use_formatting_func = formatting_func is not None and dataset_text_field is None        self._dataset_sanity_checked = Falsedef tokenize(element):            outputs = tokenizer(                element[dataset_text_field] if not use_formatting_func else formatting_func(element),                add_special_tokens=add_special_tokens,                truncation=True,                padding=False,                max_length=max_seq_length,                return_overflowing_tokens=False,                return_length=False,            )if use_formatting_func and not self._dataset_sanity_checked:if not isinstance(formatting_func(element), list):raise ValueError("The \`formatting_func\` should return a list of processed strings since it can lead to silent bugs."                    )else:                    self._dataset_sanity_checked = Truereturn {"input_ids": outputs["input_ids"], "attention_mask": outputs["attention_mask"]}        signature_columns = ["input_ids", "labels", "attention_mask"]        extra_columns = list(set(dataset.column_names) - set(signature_columns))if not remove_unused_columns and len(extra_columns) > 0:            warnings.warn("You passed \`remove_unused_columns=False\` on a non-packed dataset. This might create some issues with the default collator and yield to errors. If you want to "f"inspect dataset other columns (in this case {extra_columns}), you can subclass \`DataCollatorForLanguageModeling\` in case you used the default collator and create your own data collator in order to inspect the unused dataset columns."            )        tokenized_dataset = dataset.map(            tokenize,            batched=True,            remove_columns=dataset.column_names if remove_unused_columns else None,            num_proc=self.dataset_num_proc,            batch_size=self.dataset_batch_size,        )return tokenized_datasetdef _prepare_packed_dataloader(        self,        tokenizer,        dataset,        dataset_text_field,        max_seq_length,        num_of_sequences,        chars_per_token,        formatting_func=None,        append_concat_token=True,        add_special_tokens=True,    ):        use_formatting_func = formatting_func is not None and dataset_text_field is None        constant_length_dataset = ConstantLengthDataset(            dataset,            text_field=dataset_text_field,            formatting_func=formatting_func,            tokenizer=tokenizer,            max_seq_length=max_seq_length,            num_of_sequences=num_of_sequences,            chars_per_token=chars_per_token,            append_concat_token=append_concat_token,            add_special_tokens=add_special_tokens,            use_formatting_func=use_formatting_func,            num_proc=self.dataset_num_proc,            overwrite_cache=self.args.overwrite_cache,        )        data_loader = DataLoader(            constant_length_dataset,            batch_sampler=ConstantLengthBatchSampler(                constant_length_dataset,                batch_size=self.args.train_batch_size,                shuffle=True,                seed=self.args.seed,                epochs=self.args.num_train_epochs if self.args.max_steps <= 0 else int(1e6),            ),            collate_fn=self.data_collator,        )return data_loaderdef _trl_activate_neftune(self, model):"""        æ¿€æ´»NEFTuneå™ªå£°åµŒå…¥ã€‚NEFTuneæ˜¯ä¸€ç§åœ¨è¾“å…¥åµŒå…¥ä¸­æ³¨å…¥å™ªå£°çš„æŠ€æœ¯,å¯ä»¥æé«˜æ¨¡åž‹åœ¨æŒ‡ä»¤å¾®è°ƒä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚        è¯¥å‡½æ•°ä¼šä¿®æ”¹æ¨¡åž‹çš„å‰å‘ä¼ æ’­é€»è¾‘,åœ¨è¾“å…¥åµŒå…¥ä¸­åŠ å…¥é«˜æ–¯å™ªå£°ã€‚        """from transformers.models.auto import AutoModelif is_peft_available() and isinstance(model, PeftModel):            embeddings = model.base_model.model.get_input_embeddings()else:            embeddings = model.get_input_embeddings()def neftune_forward(embeds, input_ids):            noise = torch.randn(embeds.shape, device=embeds.device) * self.neftune_noise_alpha            embeds = embeds + noisereturn embeds        self.neftune_hook_handle = embeddings.register_forward_hook(neftune_forward)        embeddings.neftune_noise_alpha = self.neftune_noise_alphareturn modeldef _trl_unwrap_neftune(self, model):"""        ç§»é™¤NEFTuneå™ªå£°åµŒå…¥,æ¢å¤æ¨¡åž‹åŽŸå§‹çš„å‰å‘ä¼ æ’­é€»è¾‘ã€‚        """from transformers.models.auto import AutoModel        unwrapped_model = unwrap_model(model)if is_peft_available() and isinstance(unwrapped_model, PeftModel):            embeddings = unwrapped_model.base_model.model.get_input_embeddings()else:            embeddings = unwrapped_model.get_input_embeddings()        self.neftune_hook_handle.remove()del embeddings.neftune_noise_alphareturn unwrapped_model
```

#### Â  3.2ã€å…¶ä»–çš„ä»£ç 

##### Â  Â  3.2.1ã€datasets.map ä½¿ç”¨ load\_from\_cache\_file = False æ–¹ä¾¿è°ƒè¯•â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹

[https://huggingface.co/docs/datasets/v2.18.0/en/package\_reference/main\_classes#datasets.Dataset.map![icon-default.png?t=N7T8](https://i-blog.csdnimg.cn/blog_migrate/003a2ce7eb50c2e24a8c624c260c5930.png)https://huggingface.co/docs/datasets/v2.18.0/en/package\_reference/main\_classes#datasets.Dataset.map](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.Dataset.map "https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.Dataset.map")

```python
datasets = load_dataset( ... )dataset.map(            process_fn,               batched=batched,                  remove_columns=remove_columns,              num_proc=self._num_proc,               load_from_cache_file = False         )
```

### å››ã€å°ç»“

#### Â  4.1ã€åœ¨SFTTraineråˆå§‹åŒ–peftæ¨¡åž‹æ—¶ï¼Œä¸ºä»€ä¹ˆ å¼€å¯äº† QLoRA + FSDP / DS-Zero3 åŽä¸ä½¿ç”¨prepare\_model\_for\_kbit\_training å’Œ peft\_module\_casting\_to\_bf16 ï¼Œprepare\_model\_for\_kbit\_training å’Œ peft\_module\_casting\_to\_bf16 åšäº†ä»€ä¹ˆï¼ŸQLoRA + FSDP / DS-Zero3 æœªå¼€å¯offloadâ€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹æ¨¡åž‹åŠ è½½åŽmodelä¸ºä»€ä¹ˆåœ¨cpuä¸Šï¼Ÿ

> é¦–å…ˆ,æˆ‘ä»¬éœ€è¦äº†è§£ä¸€äº›åŸºæœ¬æ¦‚å¿µ:
> 
> - é‡åŒ– (Quantization):å°†æ¨¡åž‹æƒé‡ä»Žé«˜ç²¾åº¦æµ®ç‚¹æ•°(å¦‚32ä½æµ®ç‚¹æ•°)è½¬æ¢ä¸ºä½Žç²¾åº¦(å¦‚8ä½æ•´æ•°æˆ–4ä½æ•´æ•°)çš„è¿‡ç¨‹ã€‚è¿™å¯ä»¥æ˜¾è‘—å‡å°æ¨¡åž‹å°ºå¯¸,åŠ é€ŸæŽ¨ç†é€Ÿåº¦,ä½†å¯èƒ½ç•¥å¾®å½±å“æ¨¡åž‹æ€§èƒ½ã€‚
> - LoRA (Low-Rank Adaptation):ä¸€ç§å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯,é€šè¿‡åœ¨æ¨¡åž‹çš„æƒé‡çŸ©é˜µä¸­æ·»åŠ ä½Žç§©åˆ†è§£çŸ©é˜µ,å®žçŽ°ä»¥æ›´å°‘çš„å‚æ•°å¯¹é¢„è®­ç»ƒè¯­è¨€æ¨¡åž‹è¿›è¡Œå¾®è°ƒã€‚
> - PEFT (Parameter-Efficient Fine-Tuning):ä¸€ç±»å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯çš„ç»Ÿç§°,LoRAå°±æ˜¯å…¶ä¸­ä¹‹ä¸€ã€‚è¿™äº›æŠ€æœ¯æ—¨åœ¨ä»¥æ›´å°‘çš„å‚æ•°å¯¹å¤§åž‹é¢„è®­ç»ƒæ¨¡åž‹è¿›è¡Œå¾®è°ƒ,ä»¥å‡å°‘è®¡ç®—èµ„æºéœ€æ±‚ã€‚
> - FSDP (Fully Sharded Data Parallelism):ä¸€ç§åˆ†å¸ƒå¼è®­ç»ƒæŠ€æœ¯,é€šè¿‡å°†æ¨¡åž‹æƒé‡åˆ†ç‰‡åˆ°ä¸åŒçš„GPUä¸Š,å¯ä»¥æ”¯æŒè®­ç»ƒæ›´å¤§çš„æ¨¡åž‹ã€‚
> - DeepSpeed:ç”±å¾®è½¯å¼€å‘çš„æ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“,æä¾›äº†å¤šç§æŠ€æœ¯æ¥åŠ é€Ÿå’Œæ‰©å±•æ¨¡åž‹è®­ç»ƒ,å¦‚ZeRO(Zero Redundancy Optimizer)ã€‚
> - bf16 (Brain Float 16):ä»‹äºŽfp32å’Œfp16ä¹‹é—´çš„ä¸€ç§æµ®ç‚¹æ•°æ ¼å¼,åœ¨ä¿ç•™è¾ƒå¤§å€¼åŸŸçš„åŒæ—¶,å¯ä»¥è¿›ä¸€æ­¥èŠ‚çœå†…å­˜å’Œè®¡ç®—èµ„æºã€‚

> æ·±å…¥åˆ†æžä¸€ä¸‹é—®é¢˜ã€‚é¦–å…ˆ,è§£é‡Šä¸€ä¸‹`prepare_model_for_kbit_training`Â å’Œ`peft_module_casting_to_bf16`Â è¿™ä¸¤ä¸ªå‡½æ•°çš„ä½œç”¨,ç„¶åŽå†è¯´æ˜Žä¸ºä»€ä¹ˆåœ¨ä½¿ç”¨ QLoRA å’Œ FSDP/DeepSpeed Zero-3 çš„æƒ…å†µä¸‹,ä¸éœ€è¦å†è°ƒç”¨è¿™ä¸¤ä¸ªå‡½æ•°ã€‚æœ€åŽ, è§£é‡Šä¸ºä»€ä¹ˆåœ¨è¿™ç§æƒ…å†µä¸‹,éƒ¨åˆ†æ¨¡åž‹æƒé‡ä¼šè¢«åŠ è½½åˆ° CPU ä¸Šï¼Ÿ

> - â€‹â€‹â€‹â€‹â€‹â€‹â€‹`prepare_model_for_kbit_training`Â å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°çš„ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†åœ¨ä½¿ç”¨ 8 ä½æˆ– 4 ä½é‡åŒ–**åŽŸæ¨¡åž‹**è¿›è¡Œè®­ç»ƒæ—¶,å¯¹æ¨¡åž‹è¿›è¡Œä¸€äº›å¿…è¦çš„å‡†å¤‡å·¥ä½œã€‚å®ƒä¸»è¦åšäº†ä»¥ä¸‹å‡ ä»¶äº‹:â€‹â€‹â€‹â€‹â€‹â€‹â€‹
> - â€‹â€‹â€‹â€‹â€‹â€‹â€‹å°†LayerNormå±‚çš„æƒé‡è½¬æ¢ä¸ºfloat32ç²¾åº¦:åœ¨ä½Žç²¾åº¦(å¦‚8ä½æˆ–4ä½)è®­ç»ƒæ—¶,LayerNormå±‚çš„æƒé‡éœ€è¦ä¿æŒè¾ƒé«˜ç²¾åº¦,ä»¥ç¡®ä¿æ•°å€¼ç¨³å®šæ€§ã€‚
> - ç¡®ä¿è¾“å‡ºåµŒå…¥å±‚(output embedding layer)çš„å‚æ•°éœ€è¦è®¡ç®—æ¢¯åº¦:è¿™æ˜¯ä¸ºäº†ç¡®ä¿æ•´ä¸ªæ¨¡åž‹éƒ½èƒ½å¤Ÿæ­£ç¡®åœ°è¿›è¡Œåå‘ä¼ æ’­å’Œæ›´æ–°ã€‚
> - å°†è¯­è¨€æ¨¡åž‹å¤´(lm head)çš„è¾“å‡ºè½¬æ¢ä¸ºfloat32ç²¾åº¦:åŒæ ·,è¿™æ˜¯ä¸ºäº†åœ¨ä½Žç²¾åº¦è®­ç»ƒæ—¶ä¿æŒæ•°å€¼ç¨³å®šæ€§ã€‚
> - å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹(Gradient Checkpointing):è¿™æ˜¯ä¸€ç§èŠ‚çœå†…å­˜çš„æŠ€æœ¯,é€šè¿‡åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ä¸¢å¼ƒä¸€äº›ä¸­é—´æ¿€æ´»å€¼,å¹¶åœ¨åå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—å®ƒä»¬,ä»¥å‡å°‘å†…å­˜æ¶ˆè€—ã€‚
> 
> 1. æ€»çš„æ¥è¯´,`prepare_model_for_kbit_training`Â çš„ä½œç”¨æ˜¯è®©ä½Žç²¾åº¦é‡åŒ–æ¨¡åž‹åœ¨è®­ç»ƒæ—¶èŽ·å¾—æ›´å¥½çš„æ•°å€¼ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

```python
def prepare_model_for_kbit_training(model, use_gradient_checkpointing=True, gradient_checkpointing_kwargs=None):r"""    Note this method only works for \`transformers\` models.    This method wraps the entire protocol for preparing a model before running a training. This includes:        1- Cast the layernorm in fp32 2- making output embedding layer require grads 3- Add the upcasting of the lm        head to fp32    Args:        model (\`transformers.PreTrainedModel\`):            The loaded model from \`transformers\`        use_gradient_checkpointing (\`bool\`, *optional*, defaults to \`True\`):            If True, use gradient checkpointing to save memory at the expense of slower backward pass.        gradient_checkpointing_kwargs (\`dict\`, *optional*, defaults to \`None\`):            Keyword arguments to pass to the gradient checkpointing function, please refer to the documentation of            \`torch.utils.checkpoint.checkpoint\` for more details about the arguments that you can pass to that method.            Note this is only available in the latest transformers versions (> 4.34.1).    """    loaded_in_kbit = getattr(model, "is_loaded_in_8bit", False) or getattr(model, "is_loaded_in_4bit", False)    is_gptq_quantized = getattr(model, "quantization_method", None) == "gptq"    is_aqlm_quantized = getattr(model, "quantization_method", None) == "aqlm"if gradient_checkpointing_kwargs is None:        gradient_checkpointing_kwargs = {}for name, param in model.named_parameters():        param.requires_grad = Falseif not is_gptq_quantized and not is_aqlm_quantized:for param in model.parameters():if (param.dtype == torch.float16) or (param.dtype == torch.bfloat16):if param.__class__.__name__ != "Params4bit":                    param.data = param.data.to(torch.float32)if (loaded_in_kbit or is_gptq_quantized or is_aqlm_quantized) and use_gradient_checkpointing:        _supports_gc_kwargs = "gradient_checkpointing_kwargs" in list(            inspect.signature(model.gradient_checkpointing_enable).parameters        )if not _supports_gc_kwargs and len(gradient_checkpointing_kwargs) > 0:            warnings.warn("gradient_checkpointing_kwargs is not supported in this version of transformers. The passed kwargs will be ignored."" if you want to use that feature, please upgrade to the latest version of transformers.",                FutureWarning,            )        gc_enable_kwargs = {} if not _supports_gc_kwargs else {"gradient_checkpointing_kwargs": gradient_checkpointing_kwargs}        model.gradient_checkpointing_enable(**gc_enable_kwargs)return model
```

> - â€‹â€‹â€‹â€‹â€‹â€‹â€‹`peft_module_casting_to_bf16`Â å‡½æ•°:
> - å®ƒéåŽ†æ¨¡åž‹çš„æ‰€æœ‰å­æ¨¡å—,æ‰¾åˆ° PEFT æ¨¡å—(å¦‚ LoRA çš„ä½Žç§©åˆ†è§£çŸ©é˜µ),å¹¶å°†å…¶è½¬æ¢ä¸º bfloat16 ç²¾åº¦ã€‚
> - å¯¹äºŽLayerNormå±‚å’Œå…¶ä»–normalizationå±‚,å®ƒå°†å…¶æƒé‡è½¬æ¢ä¸ºfloat32ç²¾åº¦,ä»¥ä¿æŒæ•°å€¼ç¨³å®šæ€§ã€‚
> - å¯¹äºŽè¯­è¨€æ¨¡åž‹å¤´(lm head)ã€è¯åµŒå…¥(embed tokens)ç­‰éƒ¨åˆ†,å¦‚æžœå®ƒä»¬çš„æƒé‡æ˜¯float32ç²¾åº¦,ä¹Ÿä¼šè¢«è½¬æ¢ä¸ºbfloat16ç²¾åº¦ã€‚
> 
> 1. è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯å°† **PEFT æ¨¡åž‹**(å¦‚ LoRA æ¨¡åž‹)ä¸­**éœ€è¦è®­ç»ƒçš„å‚æ•°**è½¬æ¢ä¸º bfloat16 ç²¾åº¦ã€‚å…·ä½“æ¥è¯´: é€šè¿‡å°†éœ€è¦è®­ç»ƒçš„å‚æ•°è½¬æ¢ä¸º bfloat16 ç²¾åº¦,å¯ä»¥åœ¨ä¿æŒè¾ƒé«˜ç²¾åº¦çš„åŒæ—¶,è¿›ä¸€æ­¥å‡å°‘å†…å­˜å ç”¨,æé«˜è®­ç»ƒæ•ˆçŽ‡ã€‚

```python
def peft_module_casting_to_bf16(model):from peft.tuners.tuners_utils import BaseTunerLayerfor name, module in model.named_modules():if isinstance(module, BaseTunerLayer):            module = module.to(torch.bfloat16)elif isinstance(module, torch.nn.LayerNorm) or "norm" in name:            module = module.to(torch.float32)elif any(x in name for x in ["lm_head", "embed_tokens", "wte", "wpe"]):if hasattr(module, "weight"):if module.weight.dtype == torch.float32:                    module = module.to(torch.bfloat16)
```

> - ä¸ºä»€ä¹ˆåœ¨ä½¿ç”¨ QLoRA å’Œ FSDP/DeepSpeed Zero-3 çš„æƒ…å†µä¸‹,ä¸éœ€è¦å†è°ƒç”¨è¿™ä¸¤ä¸ªå‡½æ•°?
> - å…ˆå‰æ¨¡åž‹from\_pretrained å·²ç»æŒ‡å®šäº†æœªé‡åŒ–å±‚çš„æ•°æ®ç±»åž‹ï¼Œä¸ºäº†èŠ‚çœè®¡ç®—èµ„æºï¼Œä¸ä¼šå°†ä½¿ç”¨`prepare_model_for_kbit_trainingå†å°†`è¿™äº›å±‚è½¬åŒ–float32ã€‚QLoRAå’ŒFSDP/DeepSpeed Zero-3æ˜¯ä¸¤ç§äº’è¡¥çš„æŠ€æœ¯â€‹â€‹â€‹â€‹â€‹â€‹â€‹, æˆ‘ä»¬ä¸å†éœ€è¦å•ç‹¬è°ƒç”¨ prepare\_model\_for\_kbit\_training å’Œ peft\_module\_casting\_to\_bf16,å› ä¸ºQLoRAå’ŒFSDP/DeepSpeed Zero-3å·²ç»è‡ªåŠ¨å¤„ç†äº†ç›¸å…³çš„ä¼˜åŒ–å·¥ä½œã€‚
> - ![](https://i-blog.csdnimg.cn/blog_migrate/d20abf34be46590f946a01d5585a34d2.png)
> 
> â€‹â€‹â€‹â€‹â€‹â€‹â€‹
> - ä¸ºä»€ä¹ˆåœ¨ä½¿ç”¨ QLoRA å’Œ FSDP/DeepSpeed Zero-3 çš„æ²¡æœ‰å¼€å¯offloadæƒ…å†µä¸‹,éƒ¨åˆ†æ¨¡åž‹æƒé‡ä¼šè¢«åŠ è½½åˆ° CPU ä¸Š?â€‹â€‹â€‹â€‹â€‹â€‹â€‹
> - æ¨¡åž‹åœ¨QLoRA å’Œ FSDP/DeepSpeed Zero-3åŠ è½½çš„è¿‡ç¨‹ä¸­ä¼šè‡ªåŠ¨è®¾ç½®Â low\_cpu\_mem\_usage == Trueâ€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹Â 
> 
> ![](https://i-blog.csdnimg.cn/blog_migrate/20d517a440eaeeaa74b16006410a42c9.png)
> - - è™½ç„¶åœ¨CPUå’ŒGPUä¹‹é—´ç§»åŠ¨æ•°æ®ä¼šå¼•å…¥ä¸€äº›æ€§èƒ½å¼€é”€,ä½†å¯¹äºŽè¶…å¤§åž‹æ¨¡åž‹æ¥è¯´,è¿™ç§æƒè¡¡æ˜¯å€¼å¾—çš„,å› ä¸ºå®ƒå…è®¸æˆ‘ä»¬åœ¨æœ‰é™çš„GPUèµ„æºä¸‹è®­ç»ƒæ›´å¤§çš„æ¨¡åž‹ã€‚
> - å½“QLoRAå’ŒFSDP/DeepSpeed Zero-3ååŒå·¥ä½œæ—¶,QLoRAè´Ÿè´£å°†å¤§éƒ¨åˆ†æƒé‡é‡åŒ–å¹¶å†»ç»“,è€ŒFSDP/DeepSpeed Zero-3åˆ™å°†è¿™äº›å†»ç»“çš„é‡åŒ–æƒé‡åˆ†ç‰‡å­˜å‚¨åœ¨CPUä¸Š,ä»¥èŠ‚çœGPUæ˜¾å­˜ã€‚
> - FSDPå’ŒDeepSpeed Zero-3åœ¨åˆ†å¸ƒå¼è®­ç»ƒæ—¶,ä¼šå°†éƒ¨åˆ†æ¨¡åž‹æƒé‡åˆ†ç‰‡å­˜å‚¨åœ¨CPUä¸Š,ä»¥è¿›ä¸€æ­¥å‡å°‘GPUæ˜¾å­˜å ç”¨ã€‚è¿™æ ·å¯ä»¥æ”¯æŒè®­ç»ƒæ›´å¤§çš„æ¨¡åž‹ã€‚
> - QLoRAä¼šå°†å¤§éƒ¨åˆ†æ¨¡åž‹æƒé‡é‡åŒ–ä¸ºä½Žç²¾åº¦(å¦‚4ä½æˆ–8ä½),ä»¥å‡å°æ¨¡åž‹å°ºå¯¸å’Œæé«˜è®¡ç®—æ•ˆçŽ‡ã€‚è¿™éƒ¨åˆ†é‡åŒ–åŽçš„æƒé‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯å†»ç»“çš„,ä¸éœ€è¦æ›´æ–°ã€‚

> - **å®žéªŒæš‚å®šç»“æžœï¼š**
> - QLoRA å’Œ FSDP/DeepSpeed Zero-3 åŒæ—¶å¯åŠ¨æ—¶ï¼Œå•æœºå¤šå¡è®­ç»ƒæ—¶ï¼Œå¼€å¯offloadä¼šæŠ¥é”™ï¼Œä¸å¼€å¯offloadæ­£å¸¸è¿è¡Œï¼Œå•æœºå•å¡æœªè¿›è¡Œæµ‹è¯•ã€‚

#### Â  4.2ã€bfloat16å’Œfloat16çš„åŒºåˆ«

> bfloat16å’Œfloat16çš„åŒºåˆ«ä¸»è¦ä½“çŽ°åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢:
> 
> 1. **ç²¾åº¦**:
> 
> - bfloat16çš„åŠ¨æ€èŒƒå›´æ›´å¤§,ä½†ç²¾åº¦ç•¥ä½ŽäºŽfloat16ã€‚
> - bfloat16çš„æŒ‡æ•°ä½æœ‰8ä¸ªbit,å°¾æ•°ä½æœ‰7ä¸ªbit,è€Œfloat16çš„æŒ‡æ•°ä½æœ‰5ä¸ªbit,å°¾æ•°ä½æœ‰10ä¸ªbitã€‚
> - è¿™æ„å‘³ç€bfloat16èƒ½å¤Ÿè¡¨ç¤ºæ›´å¤§èŒƒå›´çš„æ•°å€¼,ä½†æ¯ä¸ªæ•°å€¼çš„ç²¾åº¦ç•¥ä½ŽäºŽfloat16ã€‚
> 2. **è®¡ç®—æ€§èƒ½**:
> 
> - bfloat16çš„è®¡ç®—æ€§èƒ½ä¼˜äºŽfloat16,å› ä¸ºå…¶æŒ‡æ•°ä½æ›´å¤š,è®¡ç®—è¿‡ç¨‹æ›´åŠ é«˜æ•ˆã€‚
> - åœ¨æŸäº›ç¡¬ä»¶å¹³å°(å¦‚ Intel çš„ Xeon Cascade Lake å’Œ AMD çš„ EPYC å¤„ç†å™¨)ä¸Š,bfloat16 çš„è®¡ç®—æ€§èƒ½ç”šè‡³å¯ä»¥ä¸Žfloat32ç›¸åª²ç¾Žã€‚
> 3. **å†…å­˜å ç”¨**:
> 
> - bfloat16å’Œfloat16éƒ½å ç”¨16ä¸ªbit,å› æ­¤åœ¨å†…å­˜å ç”¨ä¸Šæ˜¯ç›¸åŒçš„ã€‚

#### Â  4.3ã€ç»å¯¹ä½ç½®ç¼–ç ä¸Žç›¸å¯¹ä½ç½®ç¼–ç çš„åŒºåˆ«ï¼Œä¸ºä»€ä¹ˆçŽ°åœ¨çš„å¤§æ¨¡åž‹éƒ½ä½¿ç”¨RoPE

> ç»å¯¹ä½ç½®ç¼–ç (Absolute Positional Encoding)å’Œç›¸å¯¹ä½ç½®ç¼–ç (Relative Positional Encoding)æ˜¯ä¸¤ç§ä¸åŒçš„ä½ç½®ç¼–ç æ–¹å¼,å„è‡ªæœ‰å…¶ä¼˜ç¼ºç‚¹ã€‚RoPEå±žäºŽç›¸å¯¹ä½ç½®ç¼–ç çš„ä¸€ç§,è¿‘å¹´æ¥è¢«å¤§åž‹è¯­è¨€æ¨¡åž‹å¹¿æ³›é‡‡ç”¨,ä¸»è¦æœ‰ä»¥ä¸‹åŽŸå› :
> 
> - **ç¼“è§£é•¿åºåˆ—å»ºæ¨¡é—®é¢˜**
> 
> ç»å¯¹ä½ç½®ç¼–ç é€šè¿‡ä¸ºæ¯ä¸ªä½ç½®åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„ç¼–ç å‘é‡æ¥è¡¨ç¤ºä½ç½®ä¿¡æ¯ã€‚ä½†æ˜¯,å½“åºåˆ—é•¿åº¦å¢žåŠ æ—¶,æ¨¡åž‹éœ€è¦å­¦ä¹ æ›´å¤šçš„ä½ç½®ç¼–ç å‘é‡,è¿™å¯¼è‡´è®¡ç®—å’Œå†…å­˜å¼€é”€æ€¥å‰§å¢žåŠ ã€‚ç›¸å¯¹ä½ç½®ç¼–ç åˆ™é€šè¿‡ç›¸å¯¹è·ç¦»æ¥ç¼–ç ä½ç½®ä¿¡æ¯,å¯ä»¥æœ‰æ•ˆç¼“è§£è¿™ä¸ªé—®é¢˜,ä½¿æ¨¡åž‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†é•¿åºåˆ—è¾“å…¥ã€‚
> 
> - **æ•æ‰åºåˆ—ä¸­çš„å‘¨æœŸæ€§ç›¸å¯¹ä½ç½®çš„ä¿¡æ¯**
> 
> ç›¸å¯¹ä½ç½®ç¼–ç èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰åºåˆ—ä¸­çš„ç»“æž„ä¿¡æ¯å’Œå‘¨æœŸæ€§æ¨¡å¼ã€‚ä¾‹å¦‚,åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­,ç›¸é‚»è¯è¯­ä¹‹é—´çš„ç›¸å¯¹ä½ç½®å¯¹äºŽç†è§£å¥å­ç»“æž„å’Œè¯­ä¹‰æ˜¯éžå¸¸é‡è¦çš„ã€‚RoPEé€šè¿‡å¯¹åµŒå…¥å‘é‡è¿›è¡Œæ—‹è½¬æ“ä½œ,å¯ä»¥æœ‰æ•ˆåœ°ç¼–ç è¿™ç§ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚
> 
> - **è®¡ç®—æ•ˆçŽ‡é«˜ã€å‚æ•°å°‘**
> 
> ä¸Žç»å¯¹ä½ç½®ç¼–ç ç›¸æ¯”,RoPEåªéœ€è¦å¾ˆå°‘çš„å‚æ•°å°±å¯ä»¥å®žçŽ°æœ‰æ•ˆçš„ä½ç½®ç¼–ç ã€‚è¿™ç§å‚æ•°é«˜æ•ˆæ€§å¯¹äºŽå¤§åž‹è¯­è¨€æ¨¡åž‹æ¥è¯´æ˜¯éžå¸¸é‡è¦çš„,å› ä¸ºå®ƒä»¬é€šå¸¸å…·æœ‰æ•°åäº¿ä¸ªå‚æ•°,å‚æ•°æ•ˆçŽ‡ç›´æŽ¥å½±å“æ¨¡åž‹çš„è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ã€‚
> 
> - **å®žéªŒæ€§èƒ½è¾ƒå¥½æ€§èƒ½æå‡**
> 
> è®¸å¤šç ”ç©¶è¡¨æ˜Ž,åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Š,é‡‡ç”¨RoPEçš„æ¨¡åž‹æ¯”ä½¿ç”¨ç»å¯¹ä½ç½®ç¼–ç çš„æ¨¡åž‹è¡¨çŽ°æ›´å¥½ã€‚è¿™ç§æ€§èƒ½æå‡ä¸»è¦å½’å› äºŽRoPEæ›´å¥½åœ°æ•æ‰äº†åºåˆ—æ•°æ®ä¸­çš„ç»“æž„ä¿¡æ¯ã€‚

### äº”ã€Trl å…¶ä»–Traineræ³¨é‡Šç¬”è®°

#### Â  5.1ã€DPOTrainerç¬”è®°â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹

[Trlä¸­DPOTraineræ³¨é‡Šè§£æž(å¾…å®Œæˆ)![icon-default.png?t=N7T8](https://i-blog.csdnimg.cn/blog_migrate/003a2ce7eb50c2e24a8c624c260c5930.png)https://blog.csdn.net/qq\_16555103/article/details/137743362?csdn\_share\_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22137743362%22%2C%22source%22%3A%22qq\_16555103%22%7D](https://blog.csdn.net/qq_16555103/article/details/137743362?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22137743362%22%2C%22source%22%3A%22qq_16555103%22%7D "Trlä¸­DPOTraineræ³¨é‡Šè§£æž(å¾…å®Œæˆ)")

#### Â 5.2ã€...Â 

å¾…æ›´æ–°....