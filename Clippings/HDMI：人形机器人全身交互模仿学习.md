---
title: "HDMI：人形机器人全身交互模仿学习"
source: "https://mp.weixin.qq.com/s/UdfdUY8aT7c3ZZdn1-4hZQ"
author:
  - "[[HaoyangWeng]]"
published:
created: 2026-01-20
description:
tags:
  - "clippings"
---
原创 HaoyangWeng *2025年9月23日 12:30*

> https://arxiv.org/abs/2509.16757

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TzZ7x6Qk8P1gicUdcVVSgc6UzgzBJhRNKlyZFDujnHBT6ke78rwVaNQibg6GUxrEeGQA20yAe23ibUPK4ehow3fkw/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=0)

图1：HDMI能使人形机器人直接从人类视频中习得多种全身交互技能。（a）过门行走：机器人连续67次试验（约34分钟）成功穿过门，且在地形变化下仍保持稳健。（b）移动纸板箱：机器人屈膝抓取并重新放置箱子，展现出协调的全身动作。（c）搬运与放置物体：机器人向前行走，拿起并放下一堆泡沫垫。（d）仿真环境中的各类交互任务，包括推倒木板、打开折叠椅、滚球、搬运箱子和推箱子。 网站：https://hdmi-humanoid.github.io/

## 摘要

实现稳健的人形机器人-物体交互（HOI）仍面临挑战，主要源于运动数据稀缺以及交互过程中存在大量接触这一特性。本文提出HDMI（HumanoiD iMitation for Interaction，交互人形模仿学习）框架，该框架简洁通用，能够直接从单目RGB视频中学习人形机器人全身与物体的交互技能。我们的流程包括三个部分：（1）从无约束视频中提取人类和物体轨迹并进行重定向，构建结构化运动数据集；（2）训练强化学习（RL）策略以协同跟踪机器人和物体状态，其中包含三项关键设计：统一的物体表示、残差动作空间和通用的交互奖励；（3）将强化学习策略零样本部署到真实人形机器人上。在宇树G1（Unitree G1）人形机器人上开展的大量仿真到真实环境迁移实验，验证了我们方法的稳健性和通用性：HDMI实现了67次连续过门行走，在真实环境中成功完成6项不同的移动-操作复合任务，在仿真环境中完成14项任务。研究结果表明，HDMI是一个简洁通用的框架，可用于从人类视频中获取人形机器人交互技能。

## 1 引言

人形机器人因其类人形态和多功能性，在各类环境中辅助人类具有巨大潜力。要充分发挥其能力，使人形机器人能够稳健地与物体及周围环境进行交互至关重要。

从人类运动中学习是人形机器人领域的主流方法，该方法利用丰富的人类数据实现了灵活的运动控制\[1-3\]和灵巧的操作控制\[4-6\]。然而，将这些成果扩展到全身、多接触的人形机器人-物体交互（HOI）任务时，存在显著局限性。我们发现两个关键挑战：（1）与自由空间运动相比，包含三维人类和物体运动的人-物交互数据十分稀缺；（2）学习全身交互任务给强化学习训练带来新挑战，例如在运动参考不完善的情况下引导期望的接触行为，以及在复杂姿态下携带物体时保持平衡。

以往的人形机器人-物体交互研究要么依赖特定任务的运动参考生成流程，要么依赖手动设计奖励函数\[7,8\]，这限制了方法的通用性；要么需要高级视觉语言模型（VLM）或基于模型的规划器\[9,10\]。为克服这些局限性，我们提出HDMI（HumanoiD iMitation for Interaction，交互人形模仿学习）框架，这是一个通用框架，能够直接从RGB视频中获取交互技能。该框架支持机器人用不同身体部位（手、脚）与不同类型物体（多关节或刚体、固定基座或浮动基座物体）进行交互。

我们的核心思路是通过端到端强化学习控制策略，联合跟踪视频中的机器人和物体运动，从而避免针对特定任务设计奖励函数。我们的流程分为三个阶段：（1）利用姿态估计技术\[11,12\]从RGB视频中提取并重新定向人类和物体轨迹，构建结构化参考数据集；（2）利用强化学习训练控制策略，结合本体感知信息、相位变量和机器人局部坐标系下的物体状态，协同跟踪机器人和物体状态；（3）将学到的策略直接部署到人形机器人上，执行交互技能。

本文的贡献主要体现在三个方面：

1. 提出HDMI框架，该框架简洁通用，能够直接从RGB视频中学习人形机器人的交互技能。据我们所知，这是首个能够直接从人类视频中学习自主人形机器人全身与物体交互技能的通用框架。
2. 为实现复杂人形机器人-物体交互的稳定高效训练，设计了三个简洁统一的组件：适用于不同物体的统一物体表示、用于稳定探索复杂姿态的残差动作空间，以及即使在参考运动不完善时也能促进稳健且精确接触的统一交互奖励。
3. 在宇树G1（Unitree G1）人形机器人上开展大量仿真到真实环境迁移实验，验证了框架的稳健性和通用性。如图1所示，学到的策略实现了67次连续双向开门与过门行走，同时在真实硬件上成功训练并部署了另外6项不同的移动-操作复合任务，在仿真环境中部署了14项任务。

## 2 相关工作

### 2.1 人形机器人学习

强化学习在人形机器人灵活运动技能方面取得了显著进展。通过大规模训练，人形机器人实现了稳健的行走与奔跑\[13,14\]、跳跃\[15\]、跑酷\[16\]等动态行为，以及富有表现力的全身运动\[3,17\]。与此同时，模仿学习和大规模人类演示数据推动了灵巧操作策略的发展\[4-6\]，这得益于数据采集技术的进步\[18-20\]。尽管运动控制和操作控制各自取得了进展，但针对人形机器人移动-操作复合任务的研究相对较少。在这类任务中，机器人必须在多接触场景下同时完成运动和物体交互\[21-23\]。本文聚焦这一具有挑战性的前沿领域，旨在开发能够将运动与物体交互相结合的策略，使人形机器人在真实环境中具备稳健且富有表现力的技能。

### 2.2 人形机器人移动-操作复合任务

近年来，基于学习的方法已从单独的运动控制和操作控制扩展到完整的人形机器人移动-操作复合任务，但现有方法在通用性和稳健性方面仍存在局限。部分方法依赖奖励函数设计或轨迹优化，在箱子搬运等特定任务中取得了成功\[7,8\]；另一些方法则引入了专门的架构，包括技能融合\[24\]或将低层跟踪与高层任务策略分离\[3,22,23\]。近期有研究通过双智能体强化学习框架\[25,26\]提升稳健性，还有研究针对人-机器人协作设计自适应策略\[27\]。虽然这些进展拓展了人形机器人的能力，但它们的适用范围相对狭窄。与之不同，我们的框架引入了一种通用、密集、基于演示的目标函数，将运动与物体交互相结合，能够实现自适应且接触稳定的行为，并可扩展到各类不同任务。

### 2.3 从人类视频中学习机器人技能

近年来，人形机器人学习领域越来越多地将人类视频作为可扩展的演示数据源。尽管已有方法能够从视频演示中学习运动技能\[1,28-30\]，但这些方法本质上不具备物体交互能力，因为它们在训练过程中没有明确建模物体动力学。另一类研究专注于从视频演示中学习操作技能\[31-33\]，但这些方法通常局限于上半身交互，无法利用全身协调所能实现的更大工作空间。为解决这一局限，HDMI从单目RGB视频中学习全身交互技能，通过建模人-物动力学并协同跟踪轨迹，实现运动与操作的统一。

## 3 方法

HDMI是一个用于获取交互技能的通用框架。首先，我们对人类与物体交互的单目RGB视频进行重定向和处理，构建参考轨迹的结构化数据集（见第3.1节）；然后，利用这些参考轨迹，通过机器人-物体协同跟踪训练以交互为核心的策略（见第3.2节）。

![图片](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

### 3.1 从人类视频中实现可扩展的技能描述

教授机器人复杂交互技能的一个主要挑战是难以描述期望的行为。为解决这一问题，我们将人类与物体交互的单目RGB视频作为丰富且可扩展的数据源。我们使用GVHMR\[11\]和LocoMujoco\[12\]工具进行SMPL（人体参数化模型）姿态估计和重定向。接下来，我们对物体轨迹和接触信号进行后处理和标注，生成参考运动的结构化数据集。在数据集中，每一帧都提供一个参考状态 ，以及在物体局部坐标系下定义的期望接触点 。

第t帧的参考状态为 ，其中：

- 包含 、 和 ，分别表示机器人参考根节点位置、根节点姿态和参考关节位置；
- 包含 和 ，分别表示物体的位置和姿态；对于门、折叠椅等多关节物体， 还包含其关节状态 ；
- 是二进制信号，表示第t帧是否需要产生接触。

### 3.2 通过机器人-物体协同跟踪学习交互技能

我们将交互技能学习转化为机器人-物体协同跟踪问题。具体而言，给定处理后的参考运动 ，我们的目标是通过强化学习训练一个全身控制策略，使得在每个时间步，机器人能同时跟踪机器人和物体的参考轨迹。

借鉴已有的跟踪研究工作\[1,2,34\]，我们采用DeepMimic风格\[34\]的训练方法，具体步骤如下：

1. 参考状态初始化：在每个训练回合（episode）中，机器人和物体均从参考运动的随机帧 进行初始化，并加入较小的随机扰动以提高稳健性；
2. 相位变量观测：策略会接收一个相位变量 ，其中 表示运动开始， 表示运动结束。已有研究证明，仅通过这个时间相位变量就足以实现单运动跟踪\[1,34\]；
3. 基于跟踪误差的终止条件：当机器人或物体状态与参考运动偏差过大时，终止当前回合。

我们使用跟踪奖励和正则化奖励训练策略，并采用PPO（近端策略优化）算法进行优化。详细的奖励函数列表和终止条件分别见表1和表2。

![图片](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

![图片](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

然而，学习稳健的全身物体交互面临新的挑战。针对这些关键挑战，我们提出以下三项针对性解决方案：

#### 3.2.1 统一的物体表示

为使框架能够泛化到具有不同几何形状和类型的物体，我们为物体观测设计了统一的表示方式。在每个时间步，策略会接收物体在机器人局部根节点坐标系下的位姿。这种空间不变的表示方式有利于泛化，并且可以自然地扩展到机器人自身的传感输入（如RGB图像或深度图像）。

为进一步引导交互过程，我们还向策略提供参考接触点 ，用于指定机器人与物体期望的接触位置（如图3所示）。这些接触点同样被转换到机器人根节点坐标系下。结合机器人的本体感知状态 和相位变量 ，这种统一的观测（如图2顶部所示）使得我们的框架无需修改架构，即可应用于不同类型的物体。

![图片](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

图3：三项不同任务中的参考接触位置（黄点）。在训练和部署过程中，策略都会观测到这些在根节点坐标系下的接触点位置。

#### 3.2.2 残差动作空间

学习复杂运动往往涉及一些非默认姿态（如屈膝），这些姿态与机器人默认的站立姿态差异较大。在训练初期，标准策略的探索行为以默认姿态为中心。当从屈膝参考姿态初始化一个训练回合时，策略的第一个动作往往会使机器人突然弹起至站立姿态，导致瞬间失去稳定性，产生无效的训练样本。

为解决这一问题，我们采用残差动作空间。策略不直接学习绝对关节目标值 ，而是学习输出一个校正偏移量 ，将其与参考运动学轨迹中的关节位置 相加（如图2中部所示），得到： 。这种方式使得初始探索行为以当前参考姿态为中心。例如，若参考运动是屈膝接地，借助残差动作空间的探索机制，策略能够学习如何保持平衡；而在直接动作空间中，策略需要学习输出相对于零姿态（默认姿态）的较大偏移量才能实现屈膝。这种有针对性的探索提高了样本效率，显著加快了收敛到期望行为的速度，尤其对于那些与标准站立姿态差异较大的复杂运动效果更为明显。

#### 3.2.3 统一的交互奖励

通过视频重定向得到的参考轨迹仅包含运动学信息，通常缺乏精确的接触信息，甚至可能存在穿透伪影。因此，仅依靠运动跟踪奖励是不够的。为解决这一问题，我们引入统一的接触促进奖励 ，当参考信号指示需要进行交互时（即 ），该奖励鼓励策略建立并维持稳定的接触。

对于每个活跃的末端执行器i，奖励由两部分组成：（1）位置项，鼓励末端执行器与目标接触点对齐；（2）力项，促进产生稳定且有界的接触力。具体公式如下：

其中， 和 分别表示末端执行器i的位置及其在物体上的目标接触点位置， 是末端执行器i与物体之间的接触力。

位置项奖励末端执行器与物体接触点的接近程度；力项奖励足够但不过大的接触力，通过阈值 限制接触力大小，以确保部署时的安全性。整体交互奖励是所有活跃末端执行器奖励的平均值，并由接触信号控制其有效性，公式如下：

其中， 表示期望发生接触的末端执行器数量。

#### 3.2.4 领域随机化

为提高训练后策略的稳健性，在训练过程中，我们对机器人和物体的惯性属性及摩擦属性进行随机化处理。

![图片](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

图4：真实环境中复杂任务的演示。（a）开门与过门行走：机器人能根据不同的初始姿态和地形变化（有无木板）调整脚步，成功完成67次连续往返。（b）箱子移动-操作复合任务：该策略使人形机器人能够灵活协调全身动作，实现对不同形状、大小和重量物体的抓取、提升和搬运。

## 4 真实环境实验

我们在五项真实环境交互任务中对HDMI进行评估。这些任务综合测试了机器人处理多接触交互、融合运动与操作、以及完成长时程全身协调动作的能力。

### 4.1 实验设置

我们在IsaacSim仿真环境中训练所有策略，并使用相同的超参数，然后直接将这些策略部署到宇树G1（Unitree G1）人形机器人上。为获取策略所需的输入，我们在机器人骨盆和被操作物体上粘贴运动捕捉标记点，以获取每个根连杆的全局位姿。随后，将物体位姿转换到机器人根节点坐标系下，作为策略的观测信息。

所有参考运动均来自RGB视频，除了行李箱操作任务，其参考运动从重定向Omomo数据集\[35\]得到。尽管Omomo数据集提供了高质量的人-物交互数据，但其中许多运动涉及需灵巧手部动作的预抓取操作（如移动显示器或台灯），这超出了本研究的范围。

### 4.2 案例1：开门与过门行走

该任务测试策略处理连续不同接触交互的能力。机器人需用手推开大门并穿过，转身，再用脚踢开大门并再次穿过，最后返回起始位置。

该策略在失败前连续完成了67次开门-过门行走（时长34分钟）。在地形变化情况下仍保持稳健性，移除木地板后，仍成功完成约7次动作。如图4（a）所示，机器人每次开始动作时，初始位置会有10-30厘米的随机偏移。尽管存在这种位置变化，机器人仍能调整脚步，并精确抬起手臂，在正确位置与门产生接触。

这些结果表明，该策略对环境变化具有稳健性，且能适应位置变化。

### 4.3 案例2：箱子移动-操作复合任务

这些任务评估机器人全身协调能力，以操作不同高度和重量的箱型物体（如图4（b）所示）。机器人需整合全身动作，如屈膝、抓取、搬运、转身和放置物体。

1. 行李箱操作：机器人连续7次成功完成动作，在屈膝、提箱和负重行走之间实现平滑过渡；
2. 面包箱搬运：该策略完成了2次完整试验。主要挑战在于180°快速转身，这偶尔会导致腿部碰撞；
3. 泡沫垫重新放置：机器人向前行走、抓取泡沫垫、侧身移动，最后成功将其放下。

这些结果表明，HDMI能够使人形机器人在处理不同属性和高度的物体时，实现抓取、运动和操作之间无缝的全身协调。

### 4.4 案例3：杜鲁门鞠躬（Truman’s Bow）

这一复杂的多阶段任务需要执行一系列连续动作：爬楼梯、完成杜鲁门风格的鞠躬、坐在楼梯上、挥手、跳下楼梯，最后走回起始位置。

由于爬楼梯存在风险，操作人员偶尔会轻轻扶住机器人背部提供辅助。学到的策略成功连续3次完成了整个动作序列。这一结果表明，该框架具备稳健执行多样复杂动作的能力，既能处理动态且多接触的物体-场景交互（如爬楼梯和坐在楼梯上），也能实现用于表达性姿态的精确全身姿态控制（如鞠躬、挥手）。

这一案例凸显了HDMI在真实环境中实现高度依赖接触、长时程全身动作的能力。

![图片](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

图5：杜鲁门鞠躬（Truman’s Bow）。该演示展示了一系列多样且高度依赖接触的连续动作。

## 5 仿真消融实验

我们通过一系列实验分析框架中关键组件和设计选择的作用。所有仿真实验均在IsaacSim中进行。每次评估均从参考运动的起始处初始化，若策略能完成任务且未触发任何终止条件，则判定为成功。指标的均值和标准差在4096个并行仿真环境中计算得出。

### 5.1 交互奖励

我们研究了两项引导学习实现期望接触行为的设计选择的有效性：交互奖励和基于接触的终止条件（见表2中的“丢失接触”）。我们对比了以下三种训练变体：

- 含交互奖励、含接触终止条件（w/ interaction rew, w/ contact term）：同时包含交互奖励和基于接触的终止条件；
- 不含交互奖励、含接触终止条件（w/o interaction rew, w/ contact term）：仅移除交互奖励；
- 不含交互奖励、不含接触终止条件（w/o interaction rew, w/o contact term）：同时移除交互奖励和基于接触的终止条件。除非另有说明，本实验评估时均移除基于接触的终止条件。

对于大多数任务，移除这两个组件对最终性能无显著影响。然而，我们发现有两类特定任务，这两个组件对其至关重要，下文将详细分析。

#### 5.1.1 交互奖励处理不完善的参考运动

由于重定向过程不完善，参考运动有时无法精确实现抓取动作。交互奖励能使策略偏离不完善的参考运动，从而实现正确的抓取；相反，若没有交互奖励，智能体将严格遵循有缺陷的参考运动，导致交互失败（如图7（b）上半部分所示）。

为进一步验证该奖励的作用在于处理参考运动的不完善性，我们利用一个成功的策略采集完美的参考运动，并基于此进行训练。如图7（a）和图7（b）所示，即使没有交互奖励，训练也能成功。这一结果证实，交互奖励专门用于解决参考运动不完善带来的挑战。

![图片](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

图6：8项任务的最终成功率。对于大多数任务，移除接触奖励和基于接触的终止条件实际上并不会影响最终成功率。

![图片](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

图7（a）：（左图）对于行李箱移动任务，当参考运动不完善时，交互奖励对任务成功至关重要；（右图）对于推箱子任务，在评估时不使用基于接触的终止条件（eval w/o term），所有策略的成功率相近；但启用该终止条件后（eval w/ term），未使用交互奖励训练的策略成功率显著下降。这表明，我们提出的奖励对于学习维持稳定接触至关重要。

#### 5.1.2 交互奖励引导精确的接触位置

推箱子任务要求策略将其L形末端执行器精确放置在箱子边缘。若没有交互奖励（如图7（c）右图所示），策略无法实现这种精度，末端执行器常放置在箱子的垂直面上，导致接触极不稳定。这一结果表明，交互奖励对于学习能够维持稳定且精确接触的策略至关重要。

图7（b）：交互奖励引导在参考运动不完善时建立接触。当参考运动不完善时（a），交互奖励促使策略偏离有缺陷的参考运动，建立抓取动作（b）；若没有交互奖励，策略会严格模仿参考运动，导致失败（c）。通过对（b）中的运动轨迹进行采样，我们得到完美的参考运动（d），基于该完美参考运动训练时，交互奖励对任务成功并非必需。

图7（c）：交互奖励引导精确的接触位置。（左图）交互奖励引导策略将L形末端执行器精确放置在箱子边缘；（右图）若没有交互奖励，策略无法实现精确接触，末端执行器常放置在箱子的垂直面上，导致交互不稳定。

图7：交互奖励和基于接触的终止条件消融实验。

### 5.2 残差动作空间

我们研究了两项设计选择对探索过程和收敛速度的影响：残差动作空间和基于身体跟踪误差的终止条件（见表2中的“身体局部位姿误差”）。我们对比了以下三种训练变体：

- 含残差动作、含跟踪终止条件（w/ residual action, w/ track term）：完整方法，同时包含残差动作空间和基于身体跟踪误差的终止条件；
- 不含残差动作、含跟踪终止条件（w/o residual action, w/ track term）：移除残差动作空间，策略输出相对于默认关节位置（而非参考关节位置）的偏移；
- 不含残差动作、不含跟踪终止条件（w/o residual action, w/o track term）：进一步禁用基于身体跟踪误差的终止条件。训练中保留基于根节点误差的终止条件，以避免机器人倒地产生无效样本。

本实验评估时，移除基于身体跟踪误差的终止条件。

如图8所示，完整方法（含残差动作、含跟踪终止条件）在8项任务中始终实现最低的关节和身体跟踪误差。残差动作空间加快了收敛速度并提高了性能：若没有残差动作空间（不含残差动作、含跟踪终止条件，以及不含残差动作、不含跟踪终止条件），训练收敛速度大幅减慢，且无法达到相同的性能水平（如图9（a）所示）。

![图片](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

图8：8项任务的关节和身体跟踪误差。采用残差动作空间（含残差动作、含跟踪终止条件）的方法始终实现最低的跟踪误差；移除残差动作空间（不含残差动作、含跟踪终止条件，以及不含残差动作、不含跟踪终止条件）总体上会增大跟踪误差；基于身体跟踪误差的提前终止（不含残差动作、含跟踪终止条件）对降低跟踪误差的作用较小。

对于行李箱移动任务，若同时移除残差动作空间和基于身体跟踪误差的终止条件（不含残差动作、不含跟踪终止条件），策略无法学习到期望的屈膝动作（如图9（b）所示），反而会收敛到一种次优策略——保持双脚平放，仅通过腰部弯曲来完成动作。

产生这一差距的原因在于，残差动作空间使探索行为以参考运动为中心。若没有残差动作空间，动作定义以默认站立姿态为基准，因此初始探索行为围绕站立姿态展开；当从屈膝姿态初始化训练回合时，会导致机器人突然“弹起”（如图9（c）下图所示），产生低质量样本。相反，基于残差动作的策略围绕参考姿态进行局部探索（上图），能够实现稳定训练，加快收敛速度，并成功习得复杂技能。

![图片](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

图9（a）：残差动作实现稳定高效训练。采用残差动作空间时，策略从训练初期就实现较低且稳定的跟踪误差，并快速收敛到高成功率；没有残差动作空间的策略初始误差较高，且收敛速度大幅减慢。

图9（b）：残差动作使人形机器人能够学习复杂姿态。采用残差动作时（上图），策略成功执行屈膝动作以抓取行李箱；没有残差动作时（下图），策略收敛到次优策略——双脚保持平放，仅通过更大幅度的腰部弯曲来完成动作。

图9：残差动作空间和基于身体跟踪误差的提前终止条件消融实验。

图9（c）：残差动作将探索行为锚定在参考运动上。当从屈膝姿态初始化时，基于残差动作的策略（上图）围绕参考姿态进行探索；相反，标准策略的探索以默认姿态为中心，导致机器人突然“弹起”并立即失去平衡（下图），产生无效的训练数据。

## 6 局限性与未来方向

我们提出的HDMI框架使人形机器人能够从人类视频中获取多种物体交互技能。尽管我们已在14项仿真任务中验证了其有效性，但该框架仍存在两项关键局限性：

1. 依赖运动捕捉：当前系统依赖运动捕捉真值数据（如物体位姿）。未来的关键研究方向是开发能够直接基于机器人自身传感模态（如摄像头）工作的策略，以实现在无外部设备环境中的部署。
2. 单任务单策略：目前，每项任务都需要训练一个单独的专用策略。未来的重要研究方向是利用多项技能的数据训练统一的通用模型，使其能够完成各类交互任务。

## 参考文献

参照原文

---

  

欢迎“点赞”、“收藏”、“在看” 三连

可添加微信交流

  

![图片](https://mp.weixin.qq.com/s/www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg%20stroke='none'%20stroke-width='1'%20fill='none'%20fill-rule='evenodd'%20fill-opacity='0'%3E%3Cg%20transform='translate(-249.000000,%20-126.000000)'%20fill='%23FFFFFF'%3E%3Crect%20x='249'%20y='126'%20width='1'%20height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

[阅读原文](https://mp.weixin.qq.com/s/)

继续滑动看下一个

human five

向上滑动看下一个