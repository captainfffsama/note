---
title: "2024年全面的多模态大模型5000字分析总结（122个MLLMs）"
source: "https://developer.volcengine.com/articles/7389112087835836466"
author:
  - "[[用户4912874724279]]"
published: 2024-07-08
created: 2024-12-09
description: "综述了多模态大型语言模型（MLLMs）的发展，分析了它们的架构、训练方法和执行的任务。MLLMs结合视觉编码器、语言模型和适配器模块，以实现视觉和文本的无缝整合。探讨视觉基础、图像生成和编辑等应用，和在减少幻觉、防止偏见生成和降低计算的挑战"
tags:
  - "clippings"
---
自2022底ChatGPT发布以来，大语言模型领域取得飞速发展，年初对大模型2024年的发展方向也做了一些分析，其中一点就是多模态，未来结合文本、图片、视频等模态，想象空间更大。

[2024：ToB、Agent、多模态](http://mp.weixin.qq.com/s?__biz=Mzk0MTYzMzMxMA==&mid=2247483838&idx=1&sn=547104d60f0e4620417adf503eaaab9d&chksm=c2ce3bcff5b9b2d97bb4835bac4d00217f07603e64db9546c611dad53305dac4b32c6923f071&scene=21#wechat_redirect)

多模态大模型（MLLMs）是一类结合了大型语言模型（LLMs）的自然语言处理能力与对其他模态（如视觉、音频等）数据的理解与生成能力的模型。 旨在通过整合文本、图像、声音等多种类型的输入和输出，提供更加丰富和自然的交互体验。

**多模态效果示例** ：“找出图中东方明珠和上海环球金融中心”（来自Qwen-VL）

![picture.image](https://p6-volc-community-sign.byteimg.com/tos-cn-i-tlddhu82om/03aecf977b124bf5af1713f5abd18b5a~tplv-tlddhu82om-image.image?=&rk3s=8031ce6d&x-expires=1733819671&x-signature=PTGNNstKSbOjtbOmQ52ALCGYs1g%3D)

文章对多模态大型语言模型（MLLMs）的全面、深入的综述，为未来的MLLMs发展奠定了基础，接下来将讨论如下内容：

- 多模态大型语言模型（MLLMs）的发展
- M LLMs的分类
- M LLM s的架构与组件
- MLLMs的训练流程
- MLLMs在不同任务上的 表现
- MLLMs的挑战与未来研究方向

**一、多模态LLMs的发展**

在现实世界中，信息往往是以多种模态存在的，例如，社交媒体帖子可能包含文本、图像和视频。

近年来，像GPT-3和BERT这样的大型语言模型在自然语言处理（NLP）领域取得了显著的成功，它们能够执行多种语言任务，如文本生成、翻译、摘要等。 这些模型的成功激发了研究者探索如何将这些模型的能力扩展到其他模态，如视觉和听觉。

**Flamingo** 是第一个在视觉-语言领域探索上下文学习的模型。

*MLLMs的发展路线经历了从单一模态到多模态融合的转变，以及从静态模型到动态、交互式系统的演进。*

多模态大型语言模型（MLLMs）发展时间线（ **2022.4-2024.2** ）

![picture.image](https://p6-volc-community-sign.byteimg.com/tos-cn-i-tlddhu82om/e6ad4583543f4f95af3f6913d71c9d93~tplv-tlddhu82om-image.image?=&rk3s=8031ce6d&x-expires=1733819671&x-signature=iydebdpkknxKC%2B3zY%2Fhv9sT%2BCow%3D)

MLLMs（多模态大型语言模型）的发展路线可以概括为以下几个关键阶段：

1. 基础模型的建立： 早期的MLLMs主要基于单一模态的大型语言模型（如GPT系列）和视觉处理模型（如ViT系列）；

这些模型通常独立训练，然后在特定任务上进行微调或融合。

5. 多模态融合技术的探索： 开始探索如何将视觉编码器与语言模型相结合，以实现跨模态的信息融合；

出现了多种融合策略，包括直接连接、交叉注意力机制和联合训练等。

9. 预训练与微调的结合： MLLMs开始采用预训练加微调（Pre-training and Fine-tuning）的策略，以提高模型在特定任务上的性能；

预训练阶段通常在大规模多模态数据集上进行，以学习通用的多模态表示；

微调阶段则针对特定任务进行，以适应特定的应用场景。

15. 交互式和对话式应用的发展： MLLMs开始被应用于交互式和对话式系统，如虚拟助手和聊天机器人；

这些应用要求模型能够理解用户的多模态输入，并提供合适的多模态输出。

19. 计算效率和可扩展性的优化： 随着模型规模的增长，研究者开始关注如何提高MLLMs的计算效率和可扩展性；

出现了参数高效的微调（PEFT）技术和模型压缩方法，以减少训练和推理的资源需求。

**二、多模态LLMs的分类**

多模态大模型（MLLMs）的分类是基于它们的功能性和设计原则。以下是对这些分类的总结：

1. 功能性分类： 理解（Understanding）：这类MLLMs主要关注于理解和处理多模态输入，例如图像、视频、音频和文本。

生成（Generation）：这类模型不仅理解输入，还能生成特定模态的输出，如图像、视频、音频或文本。

5. 设计分类： 工具使用（Tool-using）：这类模型将LLM视为黑盒，并提供对特定多模态专家系统的访问，通过推理来执行特定的多模态任务。

端到端（End-to-end）：这类模型是整体联合训练的，意味着整个模型在训练过程中是一起优化的。

9. 模态转换： I+T→T：图像和文本输入，文本输出。

V+T→T：视频和文本输入，文本输出。

A+T→T：音频和文本输入，文本输出。

3D+T→T：3D点云和文本输入，文本输出。

I+V+A+T→T：图像、视频、音频和文本输入，文本输出。

19. 特定功能： 文档理解（ID）：处理文档内容的理解任务。

输出边界框（IB）：在图像中识别并输出对象的边界框。

输出分割掩模（IM）：生成图像中对象的分割掩模。

输出检索图像（IR）：从数据库中检索与输入相关的图像。

**122个MLLMs的分类体系** ，I：图像，V：视频，A/S：音频/语音，T：文本。ID：文档理解，IB：输出边界框，IM：输出分割掩模，IR：输出检索图像。

![picture.image](https://p6-volc-community-sign.byteimg.com/tos-cn-i-tlddhu82om/bde3133f4b014ef9b1da2b4b12f26838~tplv-tlddhu82om-image.image?=&rk3s=8031ce6d&x-expires=1733819671&x-signature=9Cr4d7oExtBUwDYbKwAayQW6R3M%3D) ![picture.image](https://p6-volc-community-sign.byteimg.com/tos-cn-i-tlddhu82om/49471eb5130746b1a45bfca43a337639~tplv-tlddhu82om-image.image?=&rk3s=8031ce6d&x-expires=1733819671&x-signature=ploByzocuCVTXDD%2BEu%2B1L2UbHMc%3D)

**三、多模态LLMs的通用架构**

多模态大型语言模型（MLLMs）的通用架构通常包括以下几个关键组件：

1. 视觉编码器（Visual Encoder）：这部分负责处理和理解输入的视觉信息，如图像。它通常使用预训练的视觉模型，如Vision Transformer（ViT）或其他卷积神经网络（CNN）架构，来提取图像特征。
2. 语言模型（Language Model）：这是MLLM的核心部分，通常基于Transformer架构，如BERT或GPT系列模型。语言模型处理文本输入，理解和生成自然语言。
3. 适配器模块（Adapter Module）：这个模块是MLLM中的关键部分，它负责在视觉和语言模态之间建立联系。适配器可以是一个简单的线性层，也可以是更复杂的结构，如多层感知器（MLP）或Transformer层，它们通过自注意力机制促进视觉和文本特征之间的对齐。

多模态大型语言模型（MLLMs）的 **通用架构** ，由视觉编码器、语言模型和一个适配器模块组成，该适配器模块将视觉输入连接到文本空间。

![picture.image](https://p6-volc-community-sign.byteimg.com/tos-cn-i-tlddhu82om/e083acc167bc43b3a1b61e4540a78d8b~tplv-tlddhu82om-image.image?=&rk3s=8031ce6d&x-expires=1733819671&x-signature=dGvRBqN6tJ4Bf79SNgJGrv7Qu1o%3D)

3.1 视觉编码器

视觉编码器是MLLMs中的核心组件之一，负责为LLM提供从图像中提取的视觉特征。它通常采用预训练的视觉模型，以便在训练过程中只更新一个可学习的接口，该接口连接视觉特征和底层的LLM。

1. 常用的视觉编码器：最常用的视觉编码器基于预训练的Vision Transformer（ViT）模型，这些模型使用CLIP（Contrastive Language-Image Pre-training）目标来利用CLIP嵌入的内在对齐。
2. 视觉编码器的选择：流行的选择包括CLIP模型中的ViT-L、OpenCLIP中的ViT-H骨干，以及EVA-CLIP中的ViT-g。这些模型在训练时保持冻结状态，以减少训练参数的数量，从而提高与语言模态的对齐。
3. 视觉编码器的限制：使用冻结的视觉编码器有一些限制，主要是由于参数数量有限，导致视觉和语言模态之间的对齐不足。为了解决这个问题，一些方法采用了两阶段训练范式，其中在第一阶段中，视觉编码器是可训练的，而在第二阶段中，它被冻结。
4. 视觉编码器的训练：在某些情况下，为了提高模型在视觉问答或视觉描述等任务上的性能，研究者们会训练视觉编码器。然而，这可能会导致在其他任务上性能下降，表明存在一定程度的遗忘和对通用视觉表示的损害。

3.2 语言模型

大语言模型是MLLMs架构的核心部分，它负责处理来自不同模态的表示，并执行语义理解、推理和决策。

1. 核心角色：LLM作为MLLMs的中心，继承了大型语言模型（LLMs）的关键特性，如零样本泛化、少样本学习、上下文学习（In-Context Learning, ICL）和指令遵循。
2. 处理多模态输入：LLM接收来自视觉编码器的编码特征，这些特征可能来自图像、视频、音频等不同模态。它将这些特征与文本特征结合，进行进一步的处理。
3. 生成输出：LLM能够直接生成文本输出，以及从其他模态（如果有的话）生成信号标记。这些信号标记作为指令，判断是否以及如何生成多模态内容。

**一些流行大语言模型的高级概述**

![picture.image](https://p6-volc-community-sign.byteimg.com/tos-cn-i-tlddhu82om/69de1de131d74129ae8ae2e57b833b69~tplv-tlddhu82om-image.image?=&rk3s=8031ce6d&x-expires=1733819671&x-signature=PzAdIBHh7YV%2FYkxKImfAKMuCbzw%3D)

![picture.image](https://p6-volc-community-sign.byteimg.com/tos-cn-i-tlddhu82om/f75135f87c7147f08fa342f3d6f7623a~tplv-tlddhu82om-image.image?=&rk3s=8031ce6d&x-expires=1733819671&x-signature=JXlDgxQRAo9bvpVktAlJig7xnoo%3D)

3.3 适配器：用于连接视觉和语言

适配器模块旨在促进视觉和文本域之间的互操作性，通过建立潜在的对应关系，使得模型能够理解和处理多模态输入。

1. 适配器的类型：常见的适配器类型包括线性层或多层感知器（MLP），以及基于Transformer的解决方案，如Q-Former模型，以及添加到LLM中的条件交叉注意力层。
2. 线性和MLP投影：最简单的方法之一是使用线性映射将视觉输入投影到与文本嵌入相同的维度。一些方法，如LLaMA-Adapter和FROMAGe，仅使用单个线性层来执行多模态连接，而LLaVA-1.5采用两层MLP，显示出改进的多模态能力。
3. Q-Former：Q-Former是一个基于Transformer的模型，它由两个共享相互自注意力层的Transformer块组成，有助于视觉和文本表示之间的对齐过程。它涉及一组可学习的查询，这些查询在自注意力层内相互作用，并通过交叉注意力机制与视觉特征接口。
4. 额外的交叉注意力层：这种方法在Flamingo中被提出，通过在LLM的现有预训练层之间集成密集的交叉注意力块。新添加的层通常与零初始化的tanh门控机制结合使用，以确保在初始化时，条件模型表现为其原始版本。
5. 适配器的训练：在训练过程中，适配器模块会根据模型的具体设计进行训练。在某些情况下，适配器模块是可训练的，而在其他情况下，它们可能与视觉编码器一起保持冻结状态。

**四、多模态LLMs训练过程**

MLLMs的训练通常涉及单阶段或双阶段过程。在这两种情况下，都使用标准的交叉熵损失来预测下一个标记，作为自回归目标。

1. 单阶段训练：在单阶段训练中，模型同时学习视觉知识和文本指令。例如，LLaMA-Adapter引入额外的可训练参数来封装视觉知识，并同时处理文本指令学习。在这个阶段，模型使用图像-文本对和指令进行联合训练。
2. 双阶段训练：在双阶段训练中，首先对图像特征与文本嵌入空间进行对齐。在这个阶段，输出通常是片段化的，不够连贯。因此，第二阶段旨在提高多模态对话能力。LLaVA是最早引入视觉指令跟随训练方案的模型之一，它在第二阶段更新多模态适配器和LLM的参数。

4.1 第一阶段：文本与视觉对齐

在训练的第一阶段（或单阶段），通常使用来自不同来源的

*<图像/音频/视频,文本>*

对，目的是将视觉特征与 LLM word embedding 对齐。例如COCO Caption、LAION-2B、LAION400M、Conceptual Captions等数据集。

COCO Caption示例， **格式<图像，文本>对**

![picture.image](https://p6-volc-community-sign.byteimg.com/tos-cn-i-tlddhu82om/f267f55e9469403a8eb25f1545f217ed~tplv-tlddhu82om-image.image?=&rk3s=8031ce6d&x-expires=1733819671&x-signature=fduywWYH0yJTFFpUgSyA9eQzu%2F4%3D)

文本与视觉对齐训练数据统计信息 ，#.X 表示X的数量，#.T 表示文本的数量，#.X-T 表示X-文本对的数量，其中X可以是图像、视频或音频。

![picture.image](https://p6-volc-community-sign.byteimg.com/tos-cn-i-tlddhu82om/754db3374cb4468dab05f044d34a7059~tplv-tlddhu82om-image.image?=&rk3s=8031ce6d&x-expires=1733819671&x-signature=%2BUkdGI39e0Y6%2BuHlJZyBy0kjUO0%3D) ![picture.image](https://p6-volc-community-sign.byteimg.com/tos-cn-i-tlddhu82om/7d84b22fcde94d02adf864573f4dfdb3~tplv-tlddhu82om-image.image?=&rk3s=8031ce6d&x-expires=1733819671&x-signature=1bh4rXir2mHbFvYTNP6Ynn0NKqc%3D)

4.2 第二阶段：指令微调，提升多模态对话能力

使用带有指令的数据集对模型进行微调，通常包含监督微调（SFT）或基于人类反馈的强化学习(RLHF)，以提高其在特定任务上的性能。这些指令数据集通常包含任务相关的模板或提示。

数据示例参考下图：

指令微调数据示例， **格式<上下文，问题，答案>**

![picture.image](https://p6-volc-community-sign.byteimg.com/tos-cn-i-tlddhu82om/d4d1bb4d610d4230a0ff94dec4681338~tplv-tlddhu82om-image.image?=&rk3s=8031ce6d&x-expires=1733819671&x-signature=MPVgwd1Vve%2F7m2tUglfF5ddSwLQ%3D)

指令微调数据集统计信息， I→O：输入到输出模态，T：文本，I：图像，V：视频，A：音频，B：边界框，3D：点云，Tab：表格，以及Web：网页

![picture.image](https://p6-volc-community-sign.byteimg.com/tos-cn-i-tlddhu82om/3365325ca9574acb8c0cdafa34711643~tplv-tlddhu82om-image.image?=&rk3s=8031ce6d&x-expires=1733819671&x-signature=os8LUz%2BhmuHTzNWnAaGX2KyklJU%3D)

**五、MLLMs处理视觉任务**

多模态大型语言模型（MLLMs）处理视觉理解任务，包括视觉问答（VQA）、图像生成和编辑等任务：

1. 视觉基础（Visual Grounding）：MLLMs的视觉基础能力指的是在对话中定位内容的能力，也称为指称性对话。这包括理解输入区域的内容（称为指称）和定位给定文本描述的区域（称为基础）。为了实现这些能力，MLLMs需要具备处理输入区域的序列到序列方法和序列到区域的方法。
2. 图像生成和编辑：现代MLLMs不仅能够从视觉数据中提取信息，还能够生成视觉输出。这通常是通过将MLLM框架与图像生成机制（如Stable Diffusion模型）整合来实现的。这些模型通过交叉注意力层将文本或视觉嵌入条件化，生成图像。
3. 其他模态和应用：除了处理图像，一些研究还提出了专门设计用于处理视频序列的MLLMs。这些模型独立处理视频帧，并使用基于CLIP的骨干网络提取帧级特征，然后通过池化机制或基于Q-Former的解决方案进行组合。此外，还有研究关注于设计能够处理多种模态的模型，这些模型通过Transformer块（如Q-Former和Perceiver）对齐多模态特征。

23个MLLMs用于视觉基础和区域级理解任务的总结 。对于每个模型，指出了其最佳配置中使用的LLM，在某些情况下，这些LLM是用预训练MLLM的权重初始化的，以及用于执行任务的Supporting Model（♦：微调；▲：使用PEFT技术微调；⋆：冻结）灰色表示模型未公开可用。

![picture.image](https://p6-volc-community-sign.byteimg.com/tos-cn-i-tlddhu82om/d917afbc7eee4139848e62ec9a8d5169~tplv-tlddhu82om-image.image?=&rk3s=8031ce6d&x-expires=1733819671&x-signature=htaz%2BsBLQ4h%2Bhn3vZPFCu01C0c0%3D)

17个MLLMs用于图像生成和编辑任务的总结 。对于每个模型，指出了其最佳配置中使用的LLM（✻：LLM变体），在某些情况下，这些LLM是用预训练MLLM的权重初始化的，以及用于执行任务的Supporting Model（♢：从头开始训练；♦：微调；▲：使用PEFT技术微调；⋆：冻结）灰色表示模型未公开可用。

![picture.image](https://p6-volc-community-sign.byteimg.com/tos-cn-i-tlddhu82om/10649c6dcb894181bc854563934e25ef~tplv-tlddhu82om-image.image?=&rk3s=8031ce6d&x-expires=1733819671&x-signature=jzmhk%2BbQKAQK8e7I5DbdOCmn9eI%3D)

**六、 **MLLMs的挑战与未来研究方向****

MLLMs（多模态大型语言模型）在处理和理解多种模态的数据方面取得了显著进展，但仍面临一系列挑战，这些挑战也是未来研究的重要方向。以下是一些关键的挑战和研究方向：

1. 模态融合与对齐:

如何有效地整合来自不同模态的信息，确保模型能够理解和利用这些信息进行推理和决策。 开发新的架构和技术，以改善模态间的对齐和交互。

2. 计算效率和可扩展性:

降低MLLMs的训练和推理成本，使其能够在资源受限的环境中部署。 研究如何使模型更轻量化，同时保持高性能。

3. 安全性和伦理性:

确保MLLMs不会生成有害、偏见或不道德的内容。 开发技术来检测和减轻模型的偏见。

4. 可解释性和透明度:

提高模型决策过程的可解释性，帮助用户理解模型的输出。 开发可视化工具和解释性方法，以增强模型的透明度。

5. 多模态生成和编辑:

提升模型在多模态内容生成和编辑方面的能力，如图像和文本的联合生成。 研究如何使模型在创作过程中更好地遵循用户的指导和意图。

这些挑战和研究方向不仅需要计算机视觉、自然语言处理和机器学习等领域的技术进步，还需要跨学科的合作，包括认知科学、心理学和伦理学等，以确保MLLMs的发展能够造福社会。

参考文献：

```sql
          
1、The (R)Evolution of Multimodal Large Language Models: A Survey
          
   https://arxiv.org/abs/2402.12451
          
2、MM-LLMs: Recent Advances in MultiModal Large Language Models
          
   https://arxiv.org/pdf/2401.13601.pdf
          
3、Large Language Models: A Survey
          
   https://arxiv.org/pdf/2402.06196.pdf 
      
```

推荐阅读

- • [对齐LLM偏好的直接偏好优化方法：DPO、IPO、KTO](http://mp.weixin.qq.com/s?__biz=Mzk0MTYzMzMxMA==&mid=2247484447&idx=1&sn=f01188d29e2c5133addbd67229db4ee7&chksm=c2ce3e6ef5b9b77874aa250e55522bbbaf214df817ad5f5f1ff98135255863522daeebdf2d3b&scene=21#wechat_redirect)
- • [2024：ToB、Agent、多模态](http://mp.weixin.qq.com/s?__biz=Mzk0MTYzMzMxMA==&mid=2247483838&idx=1&sn=547104d60f0e4620417adf503eaaab9d&chksm=c2ce3bcff5b9b2d97bb4835bac4d00217f07603e64db9546c611dad53305dac4b32c6923f071&scene=21#wechat_redirect)
- • [TA们的RAG真正投产了吗？（上）](http://mp.weixin.qq.com/s?__biz=Mzk0MTYzMzMxMA==&mid=2247483849&idx=1&sn=b919616f81192c434481ec8424cc5e3b&chksm=c2ce3bb8f5b9b2ae90c90df7c9ac27404396b66176d16af0886745a379dc3c5eb9ee70098869&scene=21#wechat_redirect)
- • [2023年最新LLM发展时间线一览（ChatGPT、LLaMA等）](http://mp.weixin.qq.com/s?__biz=Mzk0MTYzMzMxMA==&mid=2247483720&idx=1&sn=2e22d8f542d6db0cc94c8a66c560528b&chksm=c2ce3b39f5b9b22fdb12b48bee6e1f8cbabfd34e7607c45287ab8f1623343f47050d7052e659&scene=21#wechat_redirect)

---

欢迎关注我的公众号“ **PaperAgent** ”， 每天一篇大模型（LLM）文章来锻炼我们的思维，简单的例子，不简单的方法，提升自己。